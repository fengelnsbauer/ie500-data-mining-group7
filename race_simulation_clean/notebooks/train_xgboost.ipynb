{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append('../')\n",
    "from models.xgboost.xgboost import F1XGBoostPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your processed data\n",
    "with open('processed_race_data.pkl', 'rb') as f:\n",
    "    processed_data = pickle.load(f)\n",
    "\n",
    "# Initialize and optimize\n",
    "predictor = F1XGBoostPredictor(processed_data)\n",
    "study = predictor.optimize(n_trials=500)\n",
    "\n",
    "# Train the model with best parameters\n",
    "predictor.train(study)\n",
    "\n",
    "# Evaluate performance\n",
    "metrics = predictor.evaluate()\n",
    "print(f\"Test RMSE: {metrics['rmse']:.2f} ms\")\n",
    "print(f\"Test MAE: {metrics['mae']:.2f} ms\")\n",
    "\n",
    "# Save the model\n",
    "predictor.save_model('f1_xgboost_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def analyze_feature_importance(predictor, processed_data: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive feature importance analysis using multiple methods.\n",
    "    \n",
    "    Args:\n",
    "        predictor: Trained XGBoost predictor\n",
    "        processed_data: Dictionary containing processed feature data\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing various feature importance metrics\n",
    "    \"\"\"\n",
    "    # Get feature names\n",
    "    feature_names = predictor.static_features + predictor.dynamic_features\n",
    "    \n",
    "    # Get feature importance from XGBoost\n",
    "    xgb_importance = predictor.model.feature_importances_\n",
    "    \n",
    "    # Calculate correlation with target\n",
    "    features = processed_data['train']['features']\n",
    "    targets = processed_data['train']['targets']\n",
    "    \n",
    "    # Calculate Spearman correlation for each feature\n",
    "    correlations = []\n",
    "    for i in range(features.shape[1]):\n",
    "        correlation, _ = spearmanr(features[:, i], targets)\n",
    "        correlations.append(abs(correlation))  # Use absolute correlation\n",
    "    \n",
    "    # Calculate feature stability (variance across different subsets)\n",
    "    n_splits = 5\n",
    "    split_size = len(features) // n_splits\n",
    "    stability_scores = []\n",
    "    \n",
    "    for i in range(features.shape[1]):\n",
    "        importances = []\n",
    "        for j in range(n_splits):\n",
    "            start_idx = j * split_size\n",
    "            end_idx = (j + 1) * split_size\n",
    "            subset_features = features[start_idx:end_idx]\n",
    "            subset_targets = targets[start_idx:end_idx]\n",
    "            \n",
    "            # Calculate correlation for this subset\n",
    "            corr, _ = spearmanr(subset_features[:, i], subset_targets)\n",
    "            importances.append(abs(corr))\n",
    "        \n",
    "        stability_scores.append(1 - np.std(importances))\n",
    "    \n",
    "    # Combine all metrics\n",
    "    feature_metrics = []\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_metrics.append({\n",
    "            'feature': feature_name,\n",
    "            'xgb_importance': xgb_importance[i],\n",
    "            'correlation': correlations[i],\n",
    "            'stability': stability_scores[i],\n",
    "            # Combined score giving equal weight to all metrics\n",
    "            'combined_score': (\n",
    "                0.4 * xgb_importance[i] + \n",
    "                0.4 * correlations[i] + \n",
    "                0.2 * stability_scores[i]\n",
    "            )\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(feature_metrics).sort_values('combined_score', ascending=False)\n",
    "\n",
    "def plot_feature_importance(importance_df: pd.DataFrame, top_n: int = 20):\n",
    "    \"\"\"\n",
    "    Create visualizations for feature importance analysis.\n",
    "    \n",
    "    Args:\n",
    "        importance_df: DataFrame containing feature importance metrics\n",
    "        top_n: Number of top features to display\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot top features by combined score\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.barplot(data=top_features, x='combined_score', y='feature')\n",
    "    plt.title(f'Top {top_n} Features by Combined Importance Score')\n",
    "    plt.xlabel('Combined Importance Score')\n",
    "    \n",
    "    # Plot correlation between different importance metrics\n",
    "    plt.subplot(2, 1, 2)\n",
    "    importance_metrics = ['xgb_importance', 'correlation', 'stability']\n",
    "    correlation_matrix = importance_df[importance_metrics].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation between Importance Metrics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_feature_recommendations(importance_df: pd.DataFrame, \n",
    "                              threshold: float = 0.01) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Provide recommendations for feature selection.\n",
    "    \n",
    "    Args:\n",
    "        importance_df: DataFrame containing feature importance metrics\n",
    "        threshold: Minimum importance threshold for keeping features\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing (features to keep, features to consider removing)\n",
    "    \"\"\"\n",
    "    # Features to definitely keep (high importance or correlation)\n",
    "    keep_features = importance_df[\n",
    "        (importance_df['combined_score'] > threshold) |\n",
    "        (importance_df['correlation'] > threshold * 2)\n",
    "    ]['feature'].tolist()\n",
    "    \n",
    "    # Features to consider removing\n",
    "    remove_features = importance_df[\n",
    "        (importance_df['combined_score'] <= threshold) &\n",
    "        (importance_df['correlation'] <= threshold * 2)\n",
    "    ]['feature'].tolist()\n",
    "    \n",
    "    return keep_features, remove_features\n",
    "\n",
    "# Example usage after optimization:\n",
    "\n",
    "# Run analysis\n",
    "importance_df = analyze_feature_importance(predictor, processed_data)\n",
    "\n",
    "# Plot results\n",
    "plot_feature_importance(importance_df)\n",
    "\n",
    "# Get recommendations\n",
    "keep_features, remove_features = get_feature_recommendations(importance_df)\n",
    "\n",
    "print(\"\\nRecommended features to keep:\")\n",
    "print(\"\\n\".join(f\"- {feature}\" for feature in keep_features))\n",
    "\n",
    "print(\"\\nFeatures to consider removing:\")\n",
    "print(\"\\n\".join(f\"- {feature}\" for feature in remove_features))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ie500-data-mining-group7-LKR-OXJO-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
