{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from common.data_preparation import load_and_preprocess_data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pickle\n",
    "\n",
    "def organize_features(df):\n",
    "    \"\"\"Organize features into static and dynamic groups based on simulation requirements.\"\"\"\n",
    "    static_features = [\n",
    "        # Driver characteristics\n",
    "        'driver_overall_skill',\n",
    "        'driver_circuit_skill',\n",
    "        'driver_consistency',\n",
    "        'driver_aggression',\n",
    "        'driver_reliability',\n",
    "        'driver_risk_taking',\n",
    "        'driver_adaptability',\n",
    "        \n",
    "        # Constructor information\n",
    "        'constructorId',\n",
    "        'constructor_performance',\n",
    "        'constructor_nationality',\n",
    "        'constructor_position',\n",
    "        \n",
    "        # Circuit characteristics\n",
    "        'circuit_type_encoded',\n",
    "        'alt',\n",
    "        \n",
    "        # Historical performance\n",
    "        'fp1_median_time',\n",
    "        'fp2_median_time',\n",
    "        'fp3_median_time',\n",
    "        'quali_time',\n",
    "        \n",
    "        # Race start configuration\n",
    "        'grid'\n",
    "    ]\n",
    "    \n",
    "    dynamic_features = [\n",
    "        # Timing and position\n",
    "        'lap',\n",
    "        'position',\n",
    "        'positionOrder',\n",
    "        'track_position',\n",
    "        'cumulative_milliseconds',\n",
    "        'seconds_from_start',\n",
    "        'GapToLeader_ms',\n",
    "        'IntervalToPositionAhead_ms',\n",
    "        \n",
    "        # Car state\n",
    "        'tire_compound',\n",
    "        'tire_age',\n",
    "        'fuel_load',\n",
    "        'is_pit_lap',\n",
    "        \n",
    "        # Environmental conditions\n",
    "        'TrackTemp',\n",
    "        'AirTemp',\n",
    "        'Humidity',\n",
    "        'TrackStatus'\n",
    "    ]\n",
    "    \n",
    "    target = 'milliseconds'\n",
    "    \n",
    "    return static_features, dynamic_features, target\n",
    "\n",
    "def encode_categorical_features(train_df, test_df, static_features, dynamic_features):\n",
    "    \"\"\"Encode categorical features using one-hot encoding.\"\"\"\n",
    "    encoders = {'onehot_encoders': {}}\n",
    "    onehot_encode_features = ['nationality', 'country', 'status', 'constructor_nationality']\n",
    "    \n",
    "    for feature in onehot_encode_features:\n",
    "        if feature not in train_df.columns:\n",
    "            continue\n",
    "            \n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        train_encoded = encoder.fit_transform(train_df[[feature]])\n",
    "        feature_names = encoder.get_feature_names_out([feature])\n",
    "        test_encoded = encoder.transform(test_df[[feature]])\n",
    "        \n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=feature_names, index=train_df.index)\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=feature_names, index=test_df.index)\n",
    "        \n",
    "        train_df = pd.concat([train_df.drop(feature, axis=1), train_encoded_df], axis=1)\n",
    "        test_df = pd.concat([test_df.drop(feature, axis=1), test_encoded_df], axis=1)\n",
    "        \n",
    "        if feature in static_features:\n",
    "            static_features.remove(feature)\n",
    "            static_features.extend(feature_names)\n",
    "        elif feature in dynamic_features:\n",
    "            dynamic_features.remove(feature)\n",
    "            dynamic_features.extend(feature_names)\n",
    "            \n",
    "        encoders['onehot_encoders'][feature] = encoder\n",
    "    \n",
    "    return train_df, test_df, static_features, dynamic_features, encoders\n",
    "\n",
    "def split_data(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"Split data while keeping races together.\"\"\"\n",
    "    splitter = GroupShuffleSplit(test_size=test_size, n_splits=1, random_state=random_state)\n",
    "    split = splitter.split(df, groups=df['raceId'])\n",
    "    train_idx, test_idx = next(split)\n",
    "    return df.iloc[train_idx], df.iloc[test_idx]\n",
    "\n",
    "def scale_features(train_data, test_data, scaler_type='standard'):\n",
    "    \"\"\"Scale features using specified scaler.\"\"\"\n",
    "    scaler = StandardScaler() if scaler_type == 'standard' else MinMaxScaler()\n",
    "    scaled_train = scaler.fit_transform(train_data)\n",
    "    scaled_test = scaler.transform(test_data)\n",
    "    return scaled_train, scaled_test, scaler\n",
    "\n",
    "def prepare_race_data(df, static_features, dynamic_features):\n",
    "    \"\"\"Prepare data while maintaining race and temporal structure.\"\"\"\n",
    "    features = static_features + dynamic_features\n",
    "    X = df[features]\n",
    "    y = df['milliseconds']\n",
    "    \n",
    "    # Keep race and driver information for later sequence creation\n",
    "    metadata = df[['raceId', 'driverId', 'lap']]\n",
    "    \n",
    "    return X, y, metadata\n",
    "\n",
    "def save_processed_data(data, filename):\n",
    "    \"\"\"Save processed data to file.\"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data()\n",
    "    \n",
    "    # Get feature groups\n",
    "    static_features, dynamic_features, target = organize_features(df)\n",
    "    \n",
    "    # Split data (keeping races together)\n",
    "    train_df, test_df = split_data(df)\n",
    "    \n",
    "    # Reset indices to ensure alignment\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    train_df, test_df, static_features, dynamic_features, encoders = encode_categorical_features(\n",
    "        train_df, test_df, static_features, dynamic_features\n",
    "    )\n",
    "    \n",
    "    # Prepare data while maintaining temporal structure\n",
    "    train_X, train_y, train_metadata = prepare_race_data(train_df, static_features, dynamic_features)\n",
    "    test_X, test_y, test_metadata = prepare_race_data(test_df, static_features, dynamic_features)\n",
    "\n",
    "    # Scale features\n",
    "    scaled_train_X, scaled_test_X, feature_scaler = scale_features(train_X, test_X)\n",
    "    \n",
    "    # Create final processed dataset dictionary\n",
    "    processed_data = {\n",
    "        'train': {\n",
    "            'features': scaled_train_X,\n",
    "            'targets': train_y,\n",
    "            'metadata': train_metadata\n",
    "        },\n",
    "        'test': {\n",
    "            'features': scaled_test_X,\n",
    "            'targets': test_y,\n",
    "            'metadata': test_metadata\n",
    "        },\n",
    "        'feature_info': {\n",
    "            'static_features': static_features,\n",
    "            'dynamic_features': dynamic_features\n",
    "        },\n",
    "        'scalers': feature_scaler,\n",
    "        'encoders': encoders\n",
    "    }\n",
    "    \n",
    "    # Save processed data\n",
    "    save_processed_data(processed_data, 'processed_race_data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "def load_processed_data(filepath):\n",
    "    \"\"\"Load the processed race data from pickle file.\"\"\"\n",
    "    logging.info(f\"Attempting to load data from {filepath}\")\n",
    "    try:\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        logging.info(\"Data loaded successfully\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def prepare_data_for_profiling(features, targets, metadata, feature_info):\n",
    "    \"\"\"Combine features, targets, and metadata into a single DataFrame for profiling.\"\"\"\n",
    "    logging.info(\"Starting data preparation for profiling\")\n",
    "    \n",
    "    try:\n",
    "        # Log shapes before processing\n",
    "        logging.info(f\"Features shape: {features.shape}\")\n",
    "        logging.info(f\"Targets shape: {targets.shape}\")\n",
    "        logging.info(f\"Metadata shape: {metadata.shape}\")\n",
    "        \n",
    "        static_features = feature_info['static_features']\n",
    "        dynamic_features = feature_info['dynamic_features']\n",
    "        all_features = static_features + dynamic_features\n",
    "        \n",
    "        logging.info(f\"Number of features: Static={len(static_features)}, Dynamic={len(dynamic_features)}\")\n",
    "        \n",
    "        # Create feature DataFrame with reset index\n",
    "        feature_df = pd.DataFrame(features, columns=all_features).reset_index(drop=True)\n",
    "        logging.info(f\"Feature DataFrame created with shape: {feature_df.shape}\")\n",
    "        \n",
    "        # Reset metadata index and remove duplicate lap column\n",
    "        metadata_df = metadata.reset_index(drop=True).drop('lap', axis=1)\n",
    "        \n",
    "        # Create targets series with matching index\n",
    "        targets_series = pd.Series(targets, name='target_milliseconds').reset_index(drop=True)\n",
    "        \n",
    "        # Combine data with aligned indices\n",
    "        df = pd.concat([\n",
    "            feature_df,\n",
    "            metadata_df,\n",
    "            targets_series\n",
    "        ], axis=1)\n",
    "        \n",
    "        logging.info(f\"Final combined DataFrame shape: {df.shape}\")\n",
    "        logging.info(f\"Final columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in data preparation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def generate_profile_reports(data):\n",
    "    \"\"\"Generate profile reports for both training and test datasets.\"\"\"\n",
    "    logging.info(\"Starting profile report generation\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare training data\n",
    "        logging.info(\"Preparing training dataset\")\n",
    "        train_df = prepare_data_for_profiling(\n",
    "            data['train']['features'],\n",
    "            data['train']['targets'],\n",
    "            data['train']['metadata'],\n",
    "            data['feature_info']\n",
    "        )\n",
    "        \n",
    "        # Prepare test data\n",
    "        logging.info(\"Preparing test dataset\")\n",
    "        test_df = prepare_data_for_profiling(\n",
    "            data['test']['features'],\n",
    "            data['test']['targets'],\n",
    "            data['test']['metadata'],\n",
    "            data['feature_info']\n",
    "        )\n",
    "        \n",
    "        # Generate reports\n",
    "        logging.info(\"Generating training dataset profile\")\n",
    "        train_profile = ProfileReport(train_df, title=\"Training Dataset Profile\")\n",
    "        \n",
    "        logging.info(\"Generating test dataset profile\")\n",
    "        test_profile = ProfileReport(test_df, title=\"Test Dataset Profile\")\n",
    "        \n",
    "        # Save reports\n",
    "        logging.info(\"Saving profile reports to files\")\n",
    "        train_profile.to_file(\"train_profile.html\")\n",
    "        test_profile.to_file(\"test_profile.html\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in profile generation: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Starting main execution\")\n",
    "    \n",
    "    try:\n",
    "        data = load_processed_data('processed_race_data.pkl')\n",
    "        \n",
    "        generate_profile_reports(data)\n",
    "        logging.info(\"Profile reports generated successfully\")\n",
    "        \n",
    "        # Print basic dataset information\n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(f\"Training Features Shape: {data['train']['features'].shape}\")\n",
    "        print(f\"Test Features Shape: {data['test']['features'].shape}\")\n",
    "        print(f\"\\nNumber of static features: {len(data['feature_info']['static_features'])}\")\n",
    "        print(f\"Number of dynamic features: {len(data['feature_info']['dynamic_features'])}\")\n",
    "        print(f\"\\nMetadata columns: {', '.join(data['train']['metadata'].columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Main execution failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "race-simulation_copy-3-DsZshj-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
