{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Define the RaceFeatures dataclass\n",
    "@dataclass\n",
    "class RaceFeatures:\n",
    "    \"\"\"Data structure for race features\"\"\"\n",
    "    static_features: List[str] = field(default_factory=lambda: [\n",
    "        'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "        'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "        'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time'\n",
    "    ])\n",
    "    \n",
    "    dynamic_features: List[str] = field(default_factory=lambda: [\n",
    "        'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "        'air_temp', 'humidity', 'tire_compound', 'TrackStatus', 'is_pit_lap'\n",
    "    ])\n",
    "    \n",
    "    target: str = 'milliseconds'\n",
    "\n",
    "# Define the F1Dataset class\n",
    "class F1Dataset(Dataset):\n",
    "    def __init__(self, sequences, static_features, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.static_features = torch.FloatTensor(static_features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.sequences[idx],\n",
    "            'static': self.static_features[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "\n",
    "# Define the F1DataPreprocessor class\n",
    "class F1DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.static_scaler = StandardScaler()\n",
    "        self.dynamic_scaler = StandardScaler()\n",
    "        self.lap_time_scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_sequence_data(self, df: pd.DataFrame, window_size: int = 3) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare sequential data with sliding window and apply scaling\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        static_features = []\n",
    "        targets = []\n",
    "        \n",
    "        # Instantiate RaceFeatures\n",
    "        race_features = RaceFeatures()\n",
    "        \n",
    "        # Sort the dataframe to ensure consistent ordering\n",
    "        df = df.sort_values(['raceId', 'driverId', 'lap'])\n",
    "        \n",
    "        # Group by race and driver\n",
    "        for (race_id, driver_id), group in df.groupby(['raceId', 'driverId']):\n",
    "            group = group.sort_values('lap')\n",
    "            \n",
    "            # Extract static features (assumed to be constant per driver per race)\n",
    "            static = group[race_features.static_features].iloc[0].values\n",
    "            static_features.append(static)\n",
    "            \n",
    "            # Extract dynamic features and target\n",
    "            lap_times = group[race_features.target].values.reshape(-1, 1)  # Shape: (num_laps, 1)\n",
    "            dynamic = group[race_features.dynamic_features].values  # Shape: (num_laps, num_dynamic_features)\n",
    "            \n",
    "            # Apply scaling\n",
    "            # Note: Scalers should be fitted on the training data to prevent data leakage.\n",
    "            # Here, for simplicity, we're fitting on the entire dataset. For a real-world scenario,\n",
    "            # consider splitting the data first before fitting the scalers.\n",
    "            dynamic_features_to_scale = [col for col in race_features.dynamic_features if col != 'tire_compound']\n",
    "            tire_compounds = dynamic[:, race_features.dynamic_features.index('tire_compound')].reshape(-1, 1)\n",
    "            other_dynamic = dynamic[:, [race_features.dynamic_features.index(col) for col in dynamic_features_to_scale]]\n",
    "            \n",
    "            lap_times_scaled = self.lap_time_scaler.fit_transform(lap_times).flatten()\n",
    "            other_dynamic_scaled = self.dynamic_scaler.fit_transform(other_dynamic)\n",
    "            static_scaled = self.static_scaler.fit_transform(static.reshape(1, -1)).flatten()\n",
    "            \n",
    "            dynamic_scaled = np.hstack((tire_compounds, other_dynamic_scaled))\n",
    "            \n",
    "            # Create sequences\n",
    "            # Create sequences\n",
    "        for i in range(len(lap_times_scaled) - window_size):\n",
    "            sequence_lap_times = lap_times_scaled[i:i+window_size].reshape(-1, 1)  # Shape: (window_size, 1)\n",
    "            sequence_dynamic = dynamic_scaled[i:i+window_size]  # Shape: (window_size, num_dynamic_features)\n",
    "            sequence = np.hstack((sequence_lap_times, sequence_dynamic))  # Shape: (window_size, 1 + num_dynamic_features)\n",
    "            sequences.append(sequence)\n",
    "            static_features.append(static_scaled)\n",
    "            targets.append(lap_times_scaled[i + window_size])\n",
    "        \n",
    "        return (np.array(sequences), \n",
    "                np.array(static_features), \n",
    "                np.array(targets))\n",
    "\n",
    "    \n",
    "    def create_train_val_loaders(\n",
    "        self, \n",
    "        sequences: np.ndarray, \n",
    "        static_features: np.ndarray, \n",
    "        targets: np.ndarray,\n",
    "        batch_size: int = 32,\n",
    "        val_split: float = 0.2\n",
    "    ) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create train and validation dataloaders with given split ratio\n",
    "        \"\"\"\n",
    "        dataset = F1Dataset(sequences, static_features, targets)\n",
    "        \n",
    "        # Calculate lengths for split\n",
    "        val_size = int(len(dataset) * val_split)\n",
    "        train_size = len(dataset) - val_size\n",
    "        \n",
    "        # Split dataset\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "class F1PredictionModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 sequence_dim: int,\n",
    "                 static_dim: int,\n",
    "                 hidden_dim: int = 64,\n",
    "                 num_layers: int = 2,\n",
    "                 dropout_prob: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTM for sequential features with dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=sequence_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Static features processing with dropout\n",
    "        self.static_network = nn.Sequential(\n",
    "            nn.Linear(static_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Combine everything\n",
    "        self.final_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence, static):\n",
    "        # Process sequence through LSTM\n",
    "        lstm_out, _ = self.lstm(sequence)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Output of the last time step\n",
    "        \n",
    "        # Process static features\n",
    "        static_out = self.static_network(static)\n",
    "        \n",
    "        # Combine LSTM output and static features\n",
    "        combined = torch.cat([lstm_out, static_out], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.final_network(combined)\n",
    "        \n",
    "        return prediction.squeeze()\n",
    "\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                epochs: int = 10,\n",
    "                learning_rate: float = 0.001,\n",
    "                lap_time_scaler: StandardScaler = None,  # Pass the lap time scaler\n",
    "                device: Optional[str] = None) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train the model and return training history including MAE in milliseconds\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_maes = []\n",
    "        for batch in train_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            static = batch['static'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(sequences, static)\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Calculate MAE in normalized scale\n",
    "            mae = torch.mean(torch.abs(predictions - targets)).item()\n",
    "            train_maes.append(mae)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_maes = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch['sequence'].to(device)\n",
    "                static = batch['static'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                predictions = model(sequences, static)\n",
    "                loss = criterion(predictions, targets)\n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Calculate MAE in normalized scale\n",
    "                mae_normalized = torch.mean(torch.abs(predictions - targets)).item()\n",
    "                val_maes.append(mae_normalized)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        train_mae_normalized = np.mean(train_maes)\n",
    "        val_mae_normalized = np.mean(val_maes)\n",
    "        \n",
    "        # Convert MAE back to milliseconds using the inverse scaler\n",
    "        if lap_time_scaler:\n",
    "            train_mae_ms = lap_time_scaler.inverse_transform([[train_mae_normalized]])[0][0]\n",
    "            val_mae_ms = lap_time_scaler.inverse_transform([[val_mae_normalized]])[0][0]\n",
    "        else:\n",
    "            train_mae_ms, val_mae_ms = train_mae_normalized, val_mae_normalized\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mae'].append(train_mae_ms)\n",
    "        history['val_mae'].append(val_mae_ms)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train MAE: {train_mae_ms:.2f} ms, Val MAE: {val_mae_ms:.2f} ms')\n",
    "    \n",
    "    return history\n",
    "\n",
    "def predict_with_uncertainty(model, inputs, n_samples=100):\n",
    "    model.train()  # Enable dropout layers\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            prediction = model(**inputs).cpu().numpy()\n",
    "            predictions.append(prediction)\n",
    "    predictions = np.array(predictions)\n",
    "    mean_prediction = predictions.mean(axis=0)\n",
    "    std_prediction = predictions.std(axis=0)\n",
    "    return mean_prediction, std_prediction\n",
    "\n",
    "# Define a function to save the model\n",
    "def save_model_with_preprocessor(model, preprocessor, sequence_dim, static_dim, path: str):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'lap_time_scaler': preprocessor.lap_time_scaler,\n",
    "        'dynamic_scaler': preprocessor.dynamic_scaler, \n",
    "        'static_scaler': preprocessor.static_scaler,\n",
    "        'sequence_dim': sequence_dim,\n",
    "        'static_dim': static_dim\n",
    "    }, path)\n",
    "    print(f\"Model and preprocessor saved to {path}\")\n",
    "\n",
    "\n",
    "# Now, integrate your code snippets into data preprocessing\n",
    "def load_and_preprocess_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from CSV files and preprocess it to create the enhanced_laps DataFrame.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    na_values = ['\\\\N', 'NaN', '']\n",
    "    lap_times = pd.read_csv('../../data/raw_data/lap_times.csv', na_values=na_values)\n",
    "    drivers = pd.read_csv('../../data/raw_data/drivers.csv', na_values=na_values)\n",
    "    races = pd.read_csv('../../data/raw_data/races.csv', na_values=na_values)\n",
    "    circuits = pd.read_csv('../../data/raw_data/circuits.csv', na_values=na_values)\n",
    "    pit_stops = pd.read_csv('../../data/raw_data/pit_stops.csv', na_values=na_values)\n",
    "    pit_stops.rename(columns={'milliseconds' : 'pitstop_milliseconds'}, inplace=True)\n",
    "    results = pd.read_csv('../../data/raw_data/results.csv', na_values=na_values)\n",
    "    results.rename(columns={'milliseconds' : 'racetime_milliseconds'}, inplace=True)\n",
    "\n",
    "    qualifying = pd.read_csv('../../data/raw_data/qualifying.csv', na_values=na_values)\n",
    "    status = pd.read_csv('../../data/raw_data/status.csv', na_values=na_values)\n",
    "    weather_data = pd.read_csv('../../data/raw_data/ff1_weather.csv', na_values=na_values)\n",
    "    practice_sessions = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "    # Load the tire data\n",
    "    tire_data = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "\n",
    "    # Convert date columns to datetime\n",
    "    races['date'] = pd.to_datetime(races['date'])\n",
    "    results['date'] = results['raceId'].map(races.set_index('raceId')['date'])\n",
    "    lap_times['date'] = lap_times['raceId'].map(races.set_index('raceId')['date'])\n",
    "    \n",
    "    # Merge dataframes\n",
    "    laps = lap_times.merge(drivers, on='driverId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(races, on='raceId', how='left', suffixes=('', '_race'))\n",
    "    laps.rename(columns={'quali_time' : 'quali_date_time'}, inplace=True)\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(circuits, on='circuitId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(results[['raceId', 'driverId', 'positionOrder', 'grid', 'racetime_milliseconds', 'fastestLap', 'statusId']], on=['raceId', 'driverId'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(status, on='statusId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(pit_stops[['raceId', 'driverId', 'lap', 'pitstop_milliseconds']], on=['raceId', 'driverId', 'lap'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps['pitstop_milliseconds'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Add weather information\n",
    "    # Filter weather data to include only the Race session\n",
    "    weather_data = weather_data[weather_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge weather data with races to get raceId\n",
    "    weather_data = weather_data.merge(\n",
    "        races[['raceId', 'year', 'name']], \n",
    "        left_on=['EventName', 'Year'],\n",
    "        right_on=['name', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Compute cumulative time from the start of the race for each driver\n",
    "    laps.sort_values(['raceId', 'driverId', 'lap'], inplace=True)\n",
    "    laps['cumulative_milliseconds'] = laps.groupby(['raceId', 'driverId'])['milliseconds'].cumsum()\n",
    "    laps['seconds_from_start'] = laps['cumulative_milliseconds'] / 1000\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Use 'Time' in weather_data as 'seconds_from_start'\n",
    "    weather_data['seconds_from_start'] = weather_data['Time']\n",
    "    \n",
    "    # Standardize text data\n",
    "    tire_data['Compound'] = tire_data['Compound'].str.upper()\n",
    "    tire_data['EventName'] = tire_data['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Filter for race sessions only\n",
    "    tire_data = tire_data[tire_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge with races to get raceId\n",
    "    tire_data = tire_data.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    tire_data['Driver'] = tire_data['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    tire_data['driverId'] = tire_data['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Rename 'LapNumber' to 'lap' and ensure integer type\n",
    "    tire_data.rename(columns={'LapNumber': 'lap'}, inplace=True)\n",
    "    tire_data['lap'] = tire_data['lap'].astype(int)\n",
    "    laps['lap'] = laps['lap'].astype(int)\n",
    "    \n",
    "    # Create compound mapping (ordered from hardest to softest)\n",
    "    compound_mapping = {\n",
    "        'UNKNOWN': 0,\n",
    "        'HARD': 1,\n",
    "        'MEDIUM': 2,\n",
    "        'SOFT': 3,\n",
    "        'INTERMEDIATE': 4,\n",
    "        'WET': 5\n",
    "    }\n",
    "    \n",
    "    # Merge tire_data with laps\n",
    "    laps = laps.merge(\n",
    "        tire_data[['raceId', 'driverId', 'lap', 'Compound', 'TrackStatus']],\n",
    "        on=['raceId', 'driverId', 'lap'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Handle missing compounds and apply numeric encoding\n",
    "    laps['Compound'].fillna('UNKNOWN', inplace=True)\n",
    "    laps['tire_compound'] = laps['Compound'].map(compound_mapping)\n",
    "    \n",
    "    # Drop the original Compound column if desired\n",
    "    laps.drop('Compound', axis=1, inplace=True)\n",
    "    \n",
    "    # Standardize names\n",
    "    practice_sessions['EventName'] = practice_sessions['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Merge practice_sessions with races to get raceId\n",
    "    practice_sessions = practice_sessions.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    practice_sessions['Driver'] = practice_sessions['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    practice_sessions['driverId'] = practice_sessions['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Convert LapTime to milliseconds\n",
    "    practice_sessions['LapTime_ms'] = practice_sessions['LapTime'].apply(lambda x: pd.to_timedelta(x).total_seconds() * 1000)\n",
    "    \n",
    "    # Calculate median lap times for each driver in each session\n",
    "    session_medians = practice_sessions.groupby(['raceId', 'driverId', 'SessionName'])['LapTime_ms'].median().reset_index()\n",
    "    \n",
    "    # Pivot the data to have sessions as columns\n",
    "    session_medians_pivot = session_medians.pivot_table(\n",
    "        index=['raceId', 'driverId'],\n",
    "        columns='SessionName',\n",
    "        values='LapTime_ms'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    session_medians_pivot.rename(columns={\n",
    "        'FP1': 'fp1_median_time',\n",
    "        'FP2': 'fp2_median_time',\n",
    "        'FP3': 'fp3_median_time',\n",
    "        'Q': 'quali_time'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    laps = laps.merge(\n",
    "    session_medians_pivot,\n",
    "    on=['raceId', 'driverId'],\n",
    "    how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing practice times with global median or a placeholder value\n",
    "    global_median_fp1 = laps['fp1_median_time'].median()\n",
    "    laps['fp1_median_time'].fillna(global_median_fp1, inplace=True)\n",
    "    \n",
    "    # Repeat for other sessions\n",
    "    global_median_fp2 = laps['fp2_median_time'].median()\n",
    "    laps['fp2_median_time'].fillna(global_median_fp2, inplace=True)\n",
    "    \n",
    "    global_median_fp3 = laps['fp3_median_time'].median()\n",
    "    laps['fp3_median_time'].fillna(global_median_fp3, inplace=True)\n",
    "    \n",
    "    global_median_quali = laps['quali_time'].median()\n",
    "    laps['quali_time'].fillna(global_median_quali, inplace=True)\n",
    "\n",
    "    \n",
    "    # Create a binary indicator for pit stops\n",
    "    laps['is_pit_lap'] = laps['pitstop_milliseconds'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    \n",
    "    # Define a function to match weather data to laps\n",
    "    def match_weather_to_lap(race_laps, race_weather):\n",
    "        \"\"\"\n",
    "        For each lap, find the closest weather measurement in time\n",
    "        \"\"\"\n",
    "        race_laps = race_laps.sort_values('seconds_from_start')\n",
    "        race_weather = race_weather.sort_values('seconds_from_start')\n",
    "        merged = pd.merge_asof(\n",
    "            race_laps,\n",
    "            race_weather,\n",
    "            on='seconds_from_start',\n",
    "            direction='nearest'\n",
    "        )\n",
    "        return merged\n",
    "\n",
    "    # Apply matching per race\n",
    "    matched_laps_list = []\n",
    "    for race_id in laps['raceId'].unique():\n",
    "        print(f'Matching for {race_id}')\n",
    "        race_laps = laps[laps['raceId'] == race_id]\n",
    "        race_weather = weather_data[weather_data['raceId'] == race_id]\n",
    "        \n",
    "        if not race_weather.empty:\n",
    "            matched = match_weather_to_lap(race_laps, race_weather)\n",
    "            print(f\"Matched DataFrame shape: {matched.shape}\")\n",
    "            matched_laps_list.append(matched)\n",
    "        else:\n",
    "            matched_laps_list.append(race_laps)  # No weather data for this race\n",
    "\n",
    "    # Concatenate all matched laps\n",
    "    laps = pd.concat(matched_laps_list, ignore_index=True)\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Fill missing weather data with default values\n",
    "    laps['track_temp'] = laps['TrackTemp'].fillna(25.0)\n",
    "    laps['air_temp'] = laps['AirTemp'].fillna(20.0)\n",
    "    laps['humidity'] = laps['Humidity'].fillna(50.0)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    # Create driver names\n",
    "    drivers['driver_name'] = drivers['forename'] + ' ' + drivers['surname']\n",
    "    driver_mapping = drivers[['driverId', 'driver_name']].copy()\n",
    "    driver_mapping.set_index('driverId', inplace=True)\n",
    "    driver_names = driver_mapping['driver_name'].to_dict()\n",
    "    \n",
    "    # Map statusId to status descriptions\n",
    "    status_dict = status.set_index('statusId')['status'].to_dict()\n",
    "    results['status'] = results['statusId'].map(status_dict)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    def calculate_aggression(driver_results):\n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default aggression for new drivers\n",
    "        \n",
    "        # Only consider recent races for more current behavior\n",
    "        recent_results = driver_results.sort_values('date', ascending=False).head(20)\n",
    "        \n",
    "        # Calculate overtaking metrics\n",
    "        positions_gained = recent_results['grid'] - recent_results['positionOrder']\n",
    "        \n",
    "        # Calculate risk metrics\n",
    "        dnf_rate = (recent_results['status'] != 'Finished').mean()\n",
    "        incidents = (recent_results['statusId'].isin([\n",
    "            4,  # Collision\n",
    "            5,  # Spun off\n",
    "            6,  # Accident\n",
    "            20, # Collision damage\n",
    "            82, # Collision with another driver\n",
    "        ])).mean()\n",
    "        \n",
    "        # Calculate overtaking success rate (normalized between 0-1)\n",
    "        positive_overtakes = (positions_gained > 0).sum()\n",
    "        negative_overtakes = (positions_gained < 0).sum()\n",
    "        total_overtake_attempts = positive_overtakes + negative_overtakes\n",
    "        overtake_success_rate = positive_overtakes / total_overtake_attempts if total_overtake_attempts > 0 else 0.5\n",
    "        \n",
    "        # Normalize average positions gained (0-1)\n",
    "        avg_positions_gained = positions_gained[positions_gained > 0].mean() if len(positions_gained[positions_gained > 0]) > 0 else 0\n",
    "        max_possible_gain = 20  # Maximum grid positions that could be gained\n",
    "        normalized_gains = np.clip(avg_positions_gained / max_possible_gain, 0, 1)\n",
    "        \n",
    "        # Normalize risk factors (0-1)\n",
    "        normalized_dnf = np.clip(dnf_rate, 0, 1)\n",
    "        normalized_incidents = np.clip(incidents, 0, 1)\n",
    "        \n",
    "        # Calculate component scores (each between 0-1)\n",
    "        overtaking_component = (normalized_gains * 0.6 + overtake_success_rate * 0.4)\n",
    "        risk_component = (normalized_dnf * 0.5 + normalized_incidents * 0.5)\n",
    "        \n",
    "        # Combine components with weights (ensuring sum of weights = 1)\n",
    "        weights = {\n",
    "            'overtaking': 0.4,  # Aggressive overtaking\n",
    "            'risk': 0.5,       # Risk-taking behavior\n",
    "            'baseline': 0.1    # Baseline aggression\n",
    "        }\n",
    "        \n",
    "        aggression = (\n",
    "            overtaking_component * weights['overtaking'] +\n",
    "            risk_component * weights['risk'] +\n",
    "            0.5 * weights['baseline']  # Baseline aggression factor\n",
    "        )\n",
    "        \n",
    "        # Add small random variation while maintaining 0-1 bounds\n",
    "        variation = np.random.normal(0, 0.02)\n",
    "        aggression = np.clip(aggression + variation, 0, 1)\n",
    "        \n",
    "        return aggression\n",
    "    \n",
    "    def calculate_skill(driver_data, results_data, circuit_id):\n",
    "        driver_results = results_data[\n",
    "            (results_data['driverId'] == driver_data['driverId']) & \n",
    "            (results_data['circuitId'] == circuit_id)\n",
    "        ].sort_values('date', ascending=False).head(10)  # Use last 10 races at circuit\n",
    "        \n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default skill\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        avg_finish_pos = driver_results['positionOrder'].mean()\n",
    "        avg_quali_pos = driver_results['grid'].mean()\n",
    "        points_per_race = driver_results['points'].mean()\n",
    "        fastest_laps = (driver_results['rank'] == 1).mean()  # Add fastest lap consideration\n",
    "        \n",
    "        # Improved normalization (exponential decay for positions)\n",
    "        normalized_finish_pos = np.exp(-avg_finish_pos/5) # Better spread of values\n",
    "        normalized_quali_pos = np.exp(-avg_quali_pos/5)\n",
    "        \n",
    "        # Points normalization with improved scaling\n",
    "        max_points_per_race = 26  # Maximum possible points (25 + 1 fastest lap)\n",
    "        normalized_points = points_per_race / max_points_per_race\n",
    "        \n",
    "        # Weighted combination with more factors\n",
    "        weights = {\n",
    "            'finish': 0.35,\n",
    "            'quali': 0.25,\n",
    "            'points': 0.25,\n",
    "            'fastest_laps': 0.15\n",
    "        }\n",
    "        \n",
    "        skill = (\n",
    "            weights['finish'] * normalized_finish_pos +\n",
    "            weights['quali'] * normalized_quali_pos +\n",
    "            weights['points'] * normalized_points +\n",
    "            weights['fastest_laps'] * fastest_laps\n",
    "        )\n",
    "        \n",
    "        # Add random variation to prevent identical skills\n",
    "        skill = np.clip(skill + np.random.normal(0, 0.05), 0.1, 1.0)\n",
    "        \n",
    "        return skill\n",
    "    \n",
    "    # First merge results with races to get circuitId\n",
    "    results = results.merge(\n",
    "        races[['raceId', 'circuitId']], \n",
    "        on='raceId',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Now calculate driver aggression and skill\n",
    "    driver_aggression = {}\n",
    "    driver_skill = {}\n",
    "    for driver_id in drivers['driverId'].unique():\n",
    "        driver_results = results[results['driverId'] == driver_id]\n",
    "        aggression = calculate_aggression(driver_results)\n",
    "        driver_aggression[driver_id] = aggression\n",
    "        \n",
    "        # Now we have circuit_id from the merge\n",
    "        recent_race = driver_results.sort_values('date', ascending=False).head(1)\n",
    "        if not recent_race.empty:\n",
    "            circuit_id = recent_race['circuitId'].iloc[0]\n",
    "            skill = calculate_skill({'driverId': driver_id}, results, circuit_id)\n",
    "            driver_skill[driver_id] = skill\n",
    "        else:\n",
    "            driver_skill[driver_id] = 0.5  # Default skill for new drivers\n",
    "    \n",
    "    # Map calculated aggression and skill back to laps DataFrame\n",
    "    laps['driver_aggression'] = laps['driverId'].map(driver_aggression)\n",
    "    laps['driver_overall_skill'] = laps['driverId'].map(driver_skill)\n",
    "    laps['driver_circuit_skill'] = laps['driver_overall_skill']  # For simplicity, using overall skill\n",
    "    laps['driver_consistency'] = 0.5  # Placeholder\n",
    "    laps['driver_reliability'] = 0.5  # Placeholder\n",
    "    laps['driver_risk_taking'] = laps['driver_aggression']  # Assuming similar to aggression\n",
    "    \n",
    "    # Dynamic features\n",
    "    laps['tire_age'] = laps.groupby(['raceId', 'driverId'])['lap'].cumcount()\n",
    "    laps['fuel_load'] = laps.groupby(['raceId', 'driverId'])['lap'].transform(lambda x: x.max() - x + 1)\n",
    "    laps['track_position'] = laps['position']  # Assuming 'position' is available in laps data\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    # Create an instance of RaceFeatures\n",
    "    race_features = RaceFeatures()\n",
    "\n",
    "    \n",
    "    laps['TrackStatus'].fillna(1, inplace=True)  # 1 = regular racing status\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    required_columns = race_features.static_features + race_features.dynamic_features\n",
    "    # Before dropping NaN values\n",
    "    print(\"\\nNaN counts in required columns:\")\n",
    "    for col in required_columns:\n",
    "        nan_count = laps[col].isna().sum()\n",
    "        total_rows = len(laps)\n",
    "        if nan_count > 0:\n",
    "            print(f\"{col}: {nan_count} NaN values ({(nan_count/total_rows*100):.2f}% of rows)\")\n",
    "    missing_columns = set(required_columns) - set(laps.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Drop rows with missing values in required columns\n",
    "    laps.to_csv('laps_withNan.csv', index=False)\n",
    "    laps = laps.dropna(subset=required_columns)\n",
    "    \n",
    "    print(laps.shape)\n",
    "    \n",
    "    return laps\n",
    "\n",
    "# Update the main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    enhanced_laps = load_and_preprocess_data()\n",
    "    \n",
    "    enhanced_laps.drop(columns=['position', 'time', 'driverRef', 'number', 'R', 'S', 'code', 'forename', 'surname', 'url_x', 'url_race', 'name_x', 'circuitRef', 'name_y', 'location', 'country', 'url_y', 'positionOrder', 'fastestLap', 'cumulative_milliseconds', 'seconds_from_start', 'raceId_x', 'year_x', 'Time', 'TrackTemp', 'AirTemp', 'Humidity', 'name', 'year_y', 'raceId_y'], inplace=True)\n",
    "    \n",
    "    # Save the preprocessed laps DataFrame for inspection\n",
    "    enhanced_laps.to_csv('enhanced_laps_before_training.csv', index=False)\n",
    "    print(enhanced_laps.shape)\n",
    "    \n",
    "    print(\"Enhanced laps DataFrame saved to 'enhanced_laps_before_training.csv'\")\n",
    "    \n",
    "    preprocessor = F1DataPreprocessor()\n",
    "    sequences, static, targets = preprocessor.prepare_sequence_data(enhanced_laps, window_size=3)\n",
    "    \n",
    "    # Create train and validation loaders\n",
    "    train_loader, val_loader = preprocessor.create_train_val_loaders(\n",
    "        sequences, \n",
    "        static, \n",
    "        targets,\n",
    "        batch_size=32,\n",
    "        val_split=0.2\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = F1PredictionModel(\n",
    "        sequence_dim=sequences.shape[2],\n",
    "        static_dim=static.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = train_model(model, train_loader, val_loader, epochs=100, learning_rate=0.001)\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model_with_preprocessor(model, preprocessor, sequences.shape[2], static.shape[1], 'f1_prediction_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceSimulator:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.race_features = RaceFeatures()\n",
    "        \n",
    "    def simulate_lap(self, current_state):\n",
    "        \"\"\"\n",
    "        Predict lap time and uncertainty for a single lap.\n",
    "        \"\"\"\n",
    "        # Prepare input data\n",
    "        sequence = current_state['sequence']\n",
    "        static = current_state['static']\n",
    "        \n",
    "        # Convert to tensors\n",
    "        sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0)  # Add batch dimension\n",
    "        static_tensor = torch.FloatTensor(static).unsqueeze(0)\n",
    "        \n",
    "        # Predict with uncertainty\n",
    "        mean_pred, std_pred = predict_with_uncertainty(\n",
    "            self.model, \n",
    "            {'sequence': sequence_tensor, 'static': static_tensor},\n",
    "            n_samples=50\n",
    "        )\n",
    "        \n",
    "        # Inverse transform the prediction\n",
    "        lap_time = self.preprocessor.lap_time_scaler.inverse_transform([[mean_pred]])[0][0]\n",
    "        uncertainty = self.preprocessor.lap_time_scaler.inverse_transform([[std_pred]])[0][0]\n",
    "        \n",
    "        return lap_time, uncertainty\n",
    "    \n",
    "    def simulate_full_race(self, initial_state, strategy):\n",
    "        \"\"\"\n",
    "        Simulate the entire race lap by lap based on the provided strategy.\n",
    "        \"\"\"\n",
    "        lap_times = []\n",
    "        uncertainties = []\n",
    "        current_state = initial_state.copy()\n",
    "        \n",
    "        total_laps = strategy.total_laps\n",
    "        for lap in range(1, total_laps + 1):\n",
    "            # Update dynamic features based on strategy and lap number\n",
    "            current_state = self.update_state(current_state, lap, strategy)\n",
    "            \n",
    "            # Simulate lap\n",
    "            lap_time, uncertainty = self.simulate_lap(current_state)\n",
    "            lap_times.append(lap_time)\n",
    "            uncertainties.append(uncertainty)\n",
    "            \n",
    "            # Update sequence data for the next lap\n",
    "            current_state['sequence'] = self.update_sequence(\n",
    "                current_state['sequence'], lap_time, current_state['dynamic']\n",
    "            )\n",
    "        \n",
    "        return lap_times, uncertainties\n",
    "    \n",
    "    def update_state(self, state, lap, strategy):\n",
    "        \"\"\"\n",
    "        Update the state for the next lap based on the strategy.\n",
    "        \"\"\"\n",
    "        # Update tire age\n",
    "        state['dynamic']['tire_age'] += 1\n",
    "        \n",
    "        # Check for pit stops\n",
    "        if lap in strategy.pit_stop_laps:\n",
    "            # Reset tire age and update tire compound\n",
    "            state['dynamic']['tire_age'] = 0\n",
    "            state['dynamic']['tire_compound'] = strategy.pit_stop_compounds[lap]\n",
    "        \n",
    "        # Update fuel load\n",
    "        state['dynamic']['fuel_load'] -= strategy.fuel_consumption_per_lap\n",
    "        \n",
    "        # Update other dynamic features as needed\n",
    "        # ...\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def update_sequence(self, sequence, new_lap_time, dynamic_features):\n",
    "        \"\"\"\n",
    "        Update the sequence data with the latest lap information.\n",
    "        \"\"\"\n",
    "        # Remove the oldest lap data\n",
    "        sequence = sequence[1:]\n",
    "        # Append the new lap data\n",
    "        new_sequence_entry = np.hstack((\n",
    "            [new_lap_time],\n",
    "            [dynamic_features[feature] for feature in self.race_features.dynamic_features]\n",
    "        ))\n",
    "        sequence = np.vstack([sequence, new_sequence_entry])\n",
    "        return sequence\n",
    "    \n",
    "    # Additional helper methods as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceStrategy:\n",
    "    def __init__(self, total_laps, pit_stops):\n",
    "        \"\"\"\n",
    "        pit_stops: List of dictionaries with keys 'lap' and 'compound'\n",
    "        Example: [{'lap': 15, 'compound': 'MEDIUM'}, {'lap': 30, 'compound': 'SOFT'}]\n",
    "        \"\"\"\n",
    "        self.total_laps = total_laps\n",
    "        self.pit_stops = pit_stops  # List of pit stop events\n",
    "        self.pit_stop_laps = [stop['lap'] for stop in pit_stops]\n",
    "        self.pit_stop_compounds = {stop['lap']: stop['compound'] for stop in pit_stops}\n",
    "        self.fuel_consumption_per_lap = 1.5  # Example value, adjust as needed\n",
    "    \n",
    "    def evaluate(self, simulator, initial_state):\n",
    "        \"\"\"\n",
    "        Simulate the race using this strategy and return total race time.\n",
    "        \"\"\"\n",
    "        lap_times, uncertainties = simulator.simulate_full_race(initial_state, self)\n",
    "        total_time = sum(lap_times)\n",
    "        return total_time, lap_times, uncertainties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceStrategyOptimizer:\n",
    "    def __init__(self, simulator):\n",
    "        self.simulator = simulator\n",
    "    \n",
    "    def optimize(self, initial_conditions, constraints):\n",
    "        \"\"\"\n",
    "        Find the optimal strategy given initial conditions and constraints.\n",
    "        \"\"\"\n",
    "        best_strategy = None\n",
    "        best_time = float('inf')\n",
    "        \n",
    "        # Generate possible strategies within constraints\n",
    "        possible_strategies = self.generate_strategies(constraints)\n",
    "        \n",
    "        # Evaluate each strategy\n",
    "        for strategy in possible_strategies:\n",
    "            total_time, _, _ = strategy.evaluate(self.simulator, initial_conditions)\n",
    "            if total_time < best_time:\n",
    "                best_time = total_time\n",
    "                best_strategy = strategy\n",
    "        \n",
    "        return best_strategy\n",
    "    \n",
    "    def generate_strategies(self, constraints):\n",
    "        \"\"\"\n",
    "        Generate possible strategies based on constraints.\n",
    "        \"\"\"\n",
    "        min_pit_stops = constraints.get('min_pit_stops', 1)\n",
    "        max_pit_stops = constraints.get('max_pit_stops', 3)\n",
    "        available_compounds = constraints.get('available_compounds', ['SOFT', 'MEDIUM', 'HARD'])\n",
    "        total_laps = constraints.get('total_laps', 50)\n",
    "        \n",
    "        strategies = []\n",
    "        \n",
    "        # Example: Generate strategies with different pit stop laps and compounds\n",
    "        for num_pit_stops in range(min_pit_stops, max_pit_stops + 1):\n",
    "            pit_stop_laps_options = self.get_pit_stop_lap_combinations(total_laps, num_pit_stops)\n",
    "            for pit_stop_laps in pit_stop_laps_options:\n",
    "                for compounds in self.get_compound_combinations(available_compounds, num_pit_stops):\n",
    "                    pit_stops = [{'lap': lap, 'compound': comp} for lap, comp in zip(pit_stop_laps, compounds)]\n",
    "                    strategy = RaceStrategy(total_laps, pit_stops)\n",
    "                    strategies.append(strategy)\n",
    "        return strategies\n",
    "    \n",
    "    def get_pit_stop_lap_combinations(self, total_laps, num_pit_stops):\n",
    "        \"\"\"\n",
    "        Generate possible pit stop lap combinations.\n",
    "        \"\"\"\n",
    "        from itertools import combinations\n",
    "        lap_numbers = range(5, total_laps - 5)  # Avoid pitting too early or too late\n",
    "        return combinations(lap_numbers, num_pit_stops)\n",
    "    \n",
    "    def get_compound_combinations(self, compounds, num_pit_stops):\n",
    "        \"\"\"\n",
    "        Generate possible combinations of compounds for pit stops.\n",
    "        \"\"\"\n",
    "        from itertools import product\n",
    "        return product(compounds, repeat=num_pit_stops)\n",
    "    \n",
    "    def load_model_with_preprocessor(path: str, sequence_dim: int, static_dim: int, hidden_dim: int = 64, num_layers: int = 2):\n",
    "        \"\"\"\n",
    "        Load the model and preprocessor from a saved file.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "        \n",
    "        # Recreate the model architecture\n",
    "        model = F1PredictionModel(\n",
    "            sequence_dim=sequence_dim,\n",
    "            static_dim=static_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        # Recreate the preprocessor\n",
    "        preprocessor = F1DataPreprocessor()\n",
    "        preprocessor.lap_time_scaler = checkpoint['lap_time_scaler']\n",
    "        preprocessor.dynamic_scaler = checkpoint['dynamic_scaler']\n",
    "        preprocessor.static_scaler = checkpoint['static_scaler']\n",
    "        \n",
    "        print(f\"Model and preprocessor loaded from {path}\")\n",
    "        return model, preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_with_preprocessor(path: str) -> Tuple[F1PredictionModel, F1DataPreprocessor]:\n",
    "   \"\"\"Load saved model and preprocessor\"\"\"\n",
    "   checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "   \n",
    "   model = F1PredictionModel(\n",
    "       sequence_dim=checkpoint['sequence_dim'], \n",
    "       static_dim=checkpoint['static_dim']\n",
    "   )\n",
    "   model.load_state_dict(checkpoint['model_state_dict'])\n",
    "   \n",
    "   preprocessor = F1DataPreprocessor()\n",
    "   preprocessor.lap_time_scaler = checkpoint['lap_time_scaler']\n",
    "   preprocessor.dynamic_scaler = checkpoint['dynamic_scaler']\n",
    "   preprocessor.static_scaler = checkpoint['static_scaler']\n",
    "   \n",
    "   return model, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model and preprocessor are now ready for simulation or further training\n",
    "model, preprocessor = load_model_with_preprocessor('f1_prediction_model.pth')\n",
    "\n",
    "# Assume 'model' and 'preprocessor' have been loaded or trained\n",
    "simulator = RaceSimulator(model, preprocessor)\n",
    "\n",
    "# Step 1: Define static features\n",
    "static_features_list = [\n",
    "    'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "    'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "    'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time'\n",
    "]\n",
    "\n",
    "static_features = np.array([\n",
    "    0.8,   # driver_overall_skill\n",
    "    0.75,  # driver_circuit_skill\n",
    "    0.7,   # driver_consistency\n",
    "    0.9,   # driver_reliability\n",
    "    0.6,   # driver_aggression\n",
    "    0.5,   # driver_risk_taking\n",
    "    88000, # fp1_median_time\n",
    "    87500, # fp2_median_time\n",
    "    87000, # fp3_median_time\n",
    "    86000  # quali_time\n",
    "])\n",
    "\n",
    "# Step 2: Define initial dynamic features for previous laps\n",
    "dynamic_features_list = [\n",
    "    'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "    'air_temp', 'humidity', 'TrackStatus', 'is_pit_lap'\n",
    "]\n",
    "\n",
    "# Common values\n",
    "track_temp = 35.0\n",
    "air_temp = 25.0\n",
    "humidity = 50.0\n",
    "TrackStatus = 1\n",
    "is_pit_lap = 0\n",
    "\n",
    "dynamic_features = np.array([\n",
    "    [0, 100, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap],  # Lap 1\n",
    "    [1, 98.5, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap],  # Lap 2\n",
    "    [2, 97, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap]     # Lap 3\n",
    "])\n",
    "\n",
    "# Tire compound (not scaled)\n",
    "tire_compound = 0  # Example value for 'HARD' tires\n",
    "tire_compound_column = np.full((dynamic_features.shape[0], 1), tire_compound)\n",
    "\n",
    "# Lap times in milliseconds\n",
    "lap_times = np.array([90000, 89500, 89200])  # Laps 1-3\n",
    "\n",
    "# Step 3: Combine lap times, tire_compound, and dynamic features\n",
    "sequence = np.hstack((\n",
    "    lap_times.reshape(-1, 1),  # Lap times\n",
    "    tire_compound_column,      # Tire compound\n",
    "    dynamic_features           # Dynamic features\n",
    "))\n",
    "\n",
    "# Step 4: Scale the lap times\n",
    "lap_times_scaled = preprocessor.lap_time_scaler.transform(lap_times.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Step 5: Scale the dynamic features (excluding 'tire_compound')\n",
    "dynamic_features_to_scale = sequence[:, 2:]  # Exclude lap times and 'tire_compound'\n",
    "dynamic_scaled = preprocessor.dynamic_scaler.transform(dynamic_features_to_scale)\n",
    "\n",
    "# Step 6: Reconstruct the scaled sequence\n",
    "sequence_scaled = np.hstack((\n",
    "    lap_times_scaled.reshape(-1, 1),  # Scaled lap times\n",
    "    tire_compound_column,             # Tire compound (not scaled)\n",
    "    dynamic_scaled                    # Scaled dynamic features\n",
    "))\n",
    "\n",
    "# Step 7: Scale the static features\n",
    "static_scaled = preprocessor.static_scaler.transform(static_features.reshape(1, -1)).flatten()\n",
    "\n",
    "# Step 8: Prepare current dynamic features\n",
    "current_dynamic_features = {\n",
    "    'tire_age': 3,\n",
    "    'fuel_load': 95.5,\n",
    "    'track_position': 1,\n",
    "    'track_temp': 35.0,\n",
    "    'air_temp': 25.0,\n",
    "    'humidity': 50.0,\n",
    "    'TrackStatus': 1,\n",
    "    'is_pit_lap': 0,\n",
    "    'tire_compound': 0\n",
    "}\n",
    "\n",
    "# Extract and scale dynamic features (excluding 'tire_compound')\n",
    "dynamic_feature_values = np.array([\n",
    "    current_dynamic_features[feature] for feature in dynamic_features_list\n",
    "])\n",
    "\n",
    "dynamic_scaled_current = preprocessor.dynamic_scaler.transform(dynamic_feature_values.reshape(1, -1)).flatten()\n",
    "\n",
    "# Step 9: Update the initial state\n",
    "initial_state = {\n",
    "    'sequence': sequence_scaled,   # Shape: (window_size, sequence_dim)\n",
    "    'static': static_scaled,       # Shape: (static_dim,)\n",
    "    'dynamic': current_dynamic_features\n",
    "}\n",
    "\n",
    "\n",
    "# Define constraints\n",
    "constraints = {\n",
    "    'min_pit_stops': 1,\n",
    "    'max_pit_stops': 2,\n",
    "    'available_compounds': [3, 2, 1],\n",
    "    'total_laps': 50\n",
    "}\n",
    "\n",
    "# Create optimizer and find the best strategy\n",
    "optimizer = RaceStrategyOptimizer(simulator)\n",
    "best_strategy = optimizer.optimize(initial_state, constraints)\n",
    "\n",
    "# Evaluate the best strategy\n",
    "total_time, lap_times, uncertainties = best_strategy.evaluate(simulator, initial_state)\n",
    "print(f\"Optimal total race time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# %% [markdown]\n",
    "# # Phase 1\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Define the RaceFeatures dataclass\n",
    "@dataclass\n",
    "class RaceFeatures:\n",
    "    \"\"\"Data structure for race features\"\"\"\n",
    "    static_features: List[str] = field(default_factory=lambda: [\n",
    "        'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "        'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "        'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time'\n",
    "    ])\n",
    "    \n",
    "    dynamic_features: List[str] = field(default_factory=lambda: [\n",
    "        'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "        'air_temp', 'humidity', 'tire_compound', 'TrackStatus', 'is_pit_lap'\n",
    "    ])\n",
    "    \n",
    "    target: str = 'milliseconds'\n",
    "\n",
    "# Define the F1Dataset class\n",
    "class F1Dataset(Dataset):\n",
    "    def __init__(self, sequences, static_features, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.static_features = torch.FloatTensor(static_features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.sequences[idx],\n",
    "            'static': self.static_features[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "\n",
    "# Define the F1DataPreprocessor class\n",
    "class F1DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.static_scaler = StandardScaler()\n",
    "        self.dynamic_scaler = StandardScaler()\n",
    "        self.lap_time_scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_sequence_data(self, df: pd.DataFrame, window_size: int = 3) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare sequential data with sliding window and apply scaling\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        static_features = []\n",
    "        targets = []\n",
    "        \n",
    "        # Instantiate RaceFeatures\n",
    "        race_features = RaceFeatures()\n",
    "        \n",
    "        # Sort the dataframe to ensure consistent ordering\n",
    "        df = df.sort_values(['raceId', 'driverId', 'lap'])\n",
    "        \n",
    "        # Group by race and driver\n",
    "        for (race_id, driver_id), group in df.groupby(['raceId', 'driverId']):\n",
    "            group = group.sort_values('lap')\n",
    "            \n",
    "            # Extract static features (assumed to be constant per driver per race)\n",
    "            static = group[race_features.static_features].iloc[0].values\n",
    "            static_features.append(static)\n",
    "            \n",
    "            # Extract dynamic features and target\n",
    "            lap_times = group[race_features.target].values.reshape(-1, 1)  # Shape: (num_laps, 1)\n",
    "            dynamic = group[race_features.dynamic_features].values  # Shape: (num_laps, num_dynamic_features)\n",
    "            \n",
    "            # Apply scaling\n",
    "            # Note: Scalers should be fitted on the training data to prevent data leakage.\n",
    "            # Here, for simplicity, we're fitting on the entire dataset. For a real-world scenario,\n",
    "            # consider splitting the data first before fitting the scalers.\n",
    "            dynamic_features_to_scale = [col for col in race_features.dynamic_features if col != 'tire_compound']\n",
    "            tire_compounds = dynamic[:, race_features.dynamic_features.index('tire_compound')].reshape(-1, 1)\n",
    "            other_dynamic = dynamic[:, [race_features.dynamic_features.index(col) for col in dynamic_features_to_scale]]\n",
    "            \n",
    "            lap_times_scaled = self.lap_time_scaler.fit_transform(lap_times).flatten()\n",
    "            other_dynamic_scaled = self.dynamic_scaler.fit_transform(other_dynamic)\n",
    "            static_scaled = self.static_scaler.fit_transform(static.reshape(1, -1)).flatten()\n",
    "            \n",
    "            dynamic_scaled = np.hstack((tire_compounds, other_dynamic_scaled))\n",
    "            \n",
    "            # Create sequences\n",
    "            # Create sequences\n",
    "        for i in range(len(lap_times_scaled) - window_size):\n",
    "            sequence_lap_times = lap_times_scaled[i:i+window_size].reshape(-1, 1)  # Shape: (window_size, 1)\n",
    "            sequence_dynamic = dynamic_scaled[i:i+window_size]  # Shape: (window_size, num_dynamic_features)\n",
    "            sequence = np.hstack((sequence_lap_times, sequence_dynamic))  # Shape: (window_size, 1 + num_dynamic_features)\n",
    "            sequences.append(sequence)\n",
    "            static_features.append(static_scaled)\n",
    "            targets.append(lap_times_scaled[i + window_size])\n",
    "        \n",
    "        return (np.array(sequences), \n",
    "                np.array(static_features), \n",
    "                np.array(targets))\n",
    "\n",
    "    \n",
    "    def create_train_val_loaders(\n",
    "        self, \n",
    "        sequences: np.ndarray, \n",
    "        static_features: np.ndarray, \n",
    "        targets: np.ndarray,\n",
    "        batch_size: int = 32,\n",
    "        val_split: float = 0.2\n",
    "    ) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create train and validation dataloaders with given split ratio\n",
    "        \"\"\"\n",
    "        dataset = F1Dataset(sequences, static_features, targets)\n",
    "        \n",
    "        # Calculate lengths for split\n",
    "        val_size = int(len(dataset) * val_split)\n",
    "        train_size = len(dataset) - val_size\n",
    "        \n",
    "        # Split dataset\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "class F1PredictionModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 sequence_dim: int,\n",
    "                 static_dim: int,\n",
    "                 hidden_dim: int = 64,\n",
    "                 num_layers: int = 2,\n",
    "                 dropout_prob: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTM for sequential features with dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=sequence_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Static features processing with dropout\n",
    "        self.static_network = nn.Sequential(\n",
    "            nn.Linear(static_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Combine everything\n",
    "        self.final_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence, static):\n",
    "        # Process sequence through LSTM\n",
    "        lstm_out, _ = self.lstm(sequence)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Output of the last time step\n",
    "        \n",
    "        # Process static features\n",
    "        static_out = self.static_network(static)\n",
    "        \n",
    "        # Combine LSTM output and static features\n",
    "        combined = torch.cat([lstm_out, static_out], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.final_network(combined)\n",
    "        \n",
    "        return prediction.squeeze()\n",
    "\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                epochs: int = 10,\n",
    "                learning_rate: float = 0.001,\n",
    "                lap_time_scaler: StandardScaler = None,  # Pass the lap time scaler\n",
    "                device: Optional[str] = None) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train the model and return training history including MAE in milliseconds\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_maes = []\n",
    "        for batch in train_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            static = batch['static'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(sequences, static)\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Calculate MAE in normalized scale\n",
    "            mae = torch.mean(torch.abs(predictions - targets)).item()\n",
    "            train_maes.append(mae)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_maes = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch['sequence'].to(device)\n",
    "                static = batch['static'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                predictions = model(sequences, static)\n",
    "                loss = criterion(predictions, targets)\n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Calculate MAE in normalized scale\n",
    "                mae_normalized = torch.mean(torch.abs(predictions - targets)).item()\n",
    "                val_maes.append(mae_normalized)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        train_mae_normalized = np.mean(train_maes)\n",
    "        val_mae_normalized = np.mean(val_maes)\n",
    "        \n",
    "        # Convert MAE back to milliseconds using the inverse scaler\n",
    "        if lap_time_scaler:\n",
    "            train_mae_ms = lap_time_scaler.inverse_transform([[train_mae_normalized]])[0][0]\n",
    "            val_mae_ms = lap_time_scaler.inverse_transform([[val_mae_normalized]])[0][0]\n",
    "        else:\n",
    "            train_mae_ms, val_mae_ms = train_mae_normalized, val_mae_normalized\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mae'].append(train_mae_ms)\n",
    "        history['val_mae'].append(val_mae_ms)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train MAE: {train_mae_ms:.2f} ms, Val MAE: {val_mae_ms:.2f} ms')\n",
    "    \n",
    "    return history\n",
    "\n",
    "def predict_with_uncertainty(model, inputs, n_samples=100):\n",
    "    model.train()  # Enable dropout layers\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            prediction = model(**inputs).cpu().numpy()\n",
    "            predictions.append(prediction)\n",
    "    predictions = np.array(predictions)\n",
    "    mean_prediction = predictions.mean(axis=0)\n",
    "    std_prediction = predictions.std(axis=0)\n",
    "    return mean_prediction, std_prediction\n",
    "\n",
    "# Define a function to save the model\n",
    "def save_model_with_preprocessor(model, preprocessor, sequence_dim, static_dim, path: str):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'lap_time_scaler': preprocessor.lap_time_scaler,\n",
    "        'dynamic_scaler': preprocessor.dynamic_scaler, \n",
    "        'static_scaler': preprocessor.static_scaler,\n",
    "        'sequence_dim': sequence_dim,\n",
    "        'static_dim': static_dim\n",
    "    }, path)\n",
    "    print(f\"Model and preprocessor saved to {path}\")\n",
    "\n",
    "\n",
    "# Now, integrate your code snippets into data preprocessing\n",
    "def load_and_preprocess_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from CSV files and preprocess it to create the enhanced_laps DataFrame.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    na_values = ['\\\\N', 'NaN', '']\n",
    "    lap_times = pd.read_csv('../../data/raw_data/lap_times.csv', na_values=na_values)\n",
    "    drivers = pd.read_csv('../../data/raw_data/drivers.csv', na_values=na_values)\n",
    "    races = pd.read_csv('../../data/raw_data/races.csv', na_values=na_values)\n",
    "    circuits = pd.read_csv('../../data/raw_data/circuits.csv', na_values=na_values)\n",
    "    pit_stops = pd.read_csv('../../data/raw_data/pit_stops.csv', na_values=na_values)\n",
    "    pit_stops.rename(columns={'milliseconds' : 'pitstop_milliseconds'}, inplace=True)\n",
    "    results = pd.read_csv('../../data/raw_data/results.csv', na_values=na_values)\n",
    "    results.rename(columns={'milliseconds' : 'racetime_milliseconds'}, inplace=True)\n",
    "\n",
    "    qualifying = pd.read_csv('../../data/raw_data/qualifying.csv', na_values=na_values)\n",
    "    status = pd.read_csv('../../data/raw_data/status.csv', na_values=na_values)\n",
    "    weather_data = pd.read_csv('../../data/raw_data/ff1_weather.csv', na_values=na_values)\n",
    "    practice_sessions = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "    # Load the tire data\n",
    "    tire_data = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "\n",
    "    # Convert date columns to datetime\n",
    "    races['date'] = pd.to_datetime(races['date'])\n",
    "    results['date'] = results['raceId'].map(races.set_index('raceId')['date'])\n",
    "    lap_times['date'] = lap_times['raceId'].map(races.set_index('raceId')['date'])\n",
    "    \n",
    "    # Merge dataframes\n",
    "    laps = lap_times.merge(drivers, on='driverId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(races, on='raceId', how='left', suffixes=('', '_race'))\n",
    "    laps.rename(columns={'quali_time' : 'quali_date_time'}, inplace=True)\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(circuits, on='circuitId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(results[['raceId', 'driverId', 'positionOrder', 'grid', 'racetime_milliseconds', 'fastestLap', 'statusId']], on=['raceId', 'driverId'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(status, on='statusId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(pit_stops[['raceId', 'driverId', 'lap', 'pitstop_milliseconds']], on=['raceId', 'driverId', 'lap'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps['pitstop_milliseconds'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Add weather information\n",
    "    # Filter weather data to include only the Race session\n",
    "    weather_data = weather_data[weather_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge weather data with races to get raceId\n",
    "    weather_data = weather_data.merge(\n",
    "        races[['raceId', 'year', 'name']], \n",
    "        left_on=['EventName', 'Year'],\n",
    "        right_on=['name', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Compute cumulative time from the start of the race for each driver\n",
    "    laps.sort_values(['raceId', 'driverId', 'lap'], inplace=True)\n",
    "    laps['cumulative_milliseconds'] = laps.groupby(['raceId', 'driverId'])['milliseconds'].cumsum()\n",
    "    laps['seconds_from_start'] = laps['cumulative_milliseconds'] / 1000\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Use 'Time' in weather_data as 'seconds_from_start'\n",
    "    weather_data['seconds_from_start'] = weather_data['Time']\n",
    "    \n",
    "    # Standardize text data\n",
    "    tire_data['Compound'] = tire_data['Compound'].str.upper()\n",
    "    tire_data['EventName'] = tire_data['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Filter for race sessions only\n",
    "    tire_data = tire_data[tire_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge with races to get raceId\n",
    "    tire_data = tire_data.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    tire_data['Driver'] = tire_data['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    tire_data['driverId'] = tire_data['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Rename 'LapNumber' to 'lap' and ensure integer type\n",
    "    tire_data.rename(columns={'LapNumber': 'lap'}, inplace=True)\n",
    "    tire_data['lap'] = tire_data['lap'].astype(int)\n",
    "    laps['lap'] = laps['lap'].astype(int)\n",
    "    \n",
    "    # Create compound mapping (ordered from hardest to softest)\n",
    "    compound_mapping = {\n",
    "        'UNKNOWN': 0,\n",
    "        'HARD': 1,\n",
    "        'MEDIUM': 2,\n",
    "        'SOFT': 3,\n",
    "        'INTERMEDIATE': 4,\n",
    "        'WET': 5\n",
    "    }\n",
    "    \n",
    "    # Merge tire_data with laps\n",
    "    laps = laps.merge(\n",
    "        tire_data[['raceId', 'driverId', 'lap', 'Compound', 'TrackStatus']],\n",
    "        on=['raceId', 'driverId', 'lap'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Handle missing compounds and apply numeric encoding\n",
    "    laps['Compound'].fillna('UNKNOWN', inplace=True)\n",
    "    laps['tire_compound'] = laps['Compound'].map(compound_mapping)\n",
    "    \n",
    "    # Drop the original Compound column if desired\n",
    "    laps.drop('Compound', axis=1, inplace=True)\n",
    "    \n",
    "    # Standardize names\n",
    "    practice_sessions['EventName'] = practice_sessions['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Merge practice_sessions with races to get raceId\n",
    "    practice_sessions = practice_sessions.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    practice_sessions['Driver'] = practice_sessions['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    practice_sessions['driverId'] = practice_sessions['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Convert LapTime to milliseconds\n",
    "    practice_sessions['LapTime_ms'] = practice_sessions['LapTime'].apply(lambda x: pd.to_timedelta(x).total_seconds() * 1000)\n",
    "    \n",
    "    # Calculate median lap times for each driver in each session\n",
    "    session_medians = practice_sessions.groupby(['raceId', 'driverId', 'SessionName'])['LapTime_ms'].median().reset_index()\n",
    "    \n",
    "    # Pivot the data to have sessions as columns\n",
    "    session_medians_pivot = session_medians.pivot_table(\n",
    "        index=['raceId', 'driverId'],\n",
    "        columns='SessionName',\n",
    "        values='LapTime_ms'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    session_medians_pivot.rename(columns={\n",
    "        'FP1': 'fp1_median_time',\n",
    "        'FP2': 'fp2_median_time',\n",
    "        'FP3': 'fp3_median_time',\n",
    "        'Q': 'quali_time'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    laps = laps.merge(\n",
    "    session_medians_pivot,\n",
    "    on=['raceId', 'driverId'],\n",
    "    how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing practice times with global median or a placeholder value\n",
    "    global_median_fp1 = laps['fp1_median_time'].median()\n",
    "    laps['fp1_median_time'].fillna(global_median_fp1, inplace=True)\n",
    "    \n",
    "    # Repeat for other sessions\n",
    "    global_median_fp2 = laps['fp2_median_time'].median()\n",
    "    laps['fp2_median_time'].fillna(global_median_fp2, inplace=True)\n",
    "    \n",
    "    global_median_fp3 = laps['fp3_median_time'].median()\n",
    "    laps['fp3_median_time'].fillna(global_median_fp3, inplace=True)\n",
    "    \n",
    "    global_median_quali = laps['quali_time'].median()\n",
    "    laps['quali_time'].fillna(global_median_quali, inplace=True)\n",
    "\n",
    "    \n",
    "    # Create a binary indicator for pit stops\n",
    "    laps['is_pit_lap'] = laps['pitstop_milliseconds'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    \n",
    "    # Define a function to match weather data to laps\n",
    "    def match_weather_to_lap(race_laps, race_weather):\n",
    "        \"\"\"\n",
    "        For each lap, find the closest weather measurement in time\n",
    "        \"\"\"\n",
    "        race_laps = race_laps.sort_values('seconds_from_start')\n",
    "        race_weather = race_weather.sort_values('seconds_from_start')\n",
    "        merged = pd.merge_asof(\n",
    "            race_laps,\n",
    "            race_weather,\n",
    "            on='seconds_from_start',\n",
    "            direction='nearest'\n",
    "        )\n",
    "        return merged\n",
    "\n",
    "    # Apply matching per race\n",
    "    matched_laps_list = []\n",
    "    for race_id in laps['raceId'].unique():\n",
    "        print(f'Matching for {race_id}')\n",
    "        race_laps = laps[laps['raceId'] == race_id]\n",
    "        race_weather = weather_data[weather_data['raceId'] == race_id]\n",
    "        \n",
    "        if not race_weather.empty:\n",
    "            matched = match_weather_to_lap(race_laps, race_weather)\n",
    "            print(f\"Matched DataFrame shape: {matched.shape}\")\n",
    "            matched_laps_list.append(matched)\n",
    "        else:\n",
    "            matched_laps_list.append(race_laps)  # No weather data for this race\n",
    "\n",
    "    # Concatenate all matched laps\n",
    "    laps = pd.concat(matched_laps_list, ignore_index=True)\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Fill missing weather data with default values\n",
    "    laps['track_temp'] = laps['TrackTemp'].fillna(25.0)\n",
    "    laps['air_temp'] = laps['AirTemp'].fillna(20.0)\n",
    "    laps['humidity'] = laps['Humidity'].fillna(50.0)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    # Create driver names\n",
    "    drivers['driver_name'] = drivers['forename'] + ' ' + drivers['surname']\n",
    "    driver_mapping = drivers[['driverId', 'driver_name']].copy()\n",
    "    driver_mapping.set_index('driverId', inplace=True)\n",
    "    driver_names = driver_mapping['driver_name'].to_dict()\n",
    "    \n",
    "    # Map statusId to status descriptions\n",
    "    status_dict = status.set_index('statusId')['status'].to_dict()\n",
    "    results['status'] = results['statusId'].map(status_dict)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    def calculate_aggression(driver_results):\n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default aggression for new drivers\n",
    "        \n",
    "        # Only consider recent races for more current behavior\n",
    "        recent_results = driver_results.sort_values('date', ascending=False).head(20)\n",
    "        \n",
    "        # Calculate overtaking metrics\n",
    "        positions_gained = recent_results['grid'] - recent_results['positionOrder']\n",
    "        \n",
    "        # Calculate risk metrics\n",
    "        dnf_rate = (recent_results['status'] != 'Finished').mean()\n",
    "        incidents = (recent_results['statusId'].isin([\n",
    "            4,  # Collision\n",
    "            5,  # Spun off\n",
    "            6,  # Accident\n",
    "            20, # Collision damage\n",
    "            82, # Collision with another driver\n",
    "        ])).mean()\n",
    "        \n",
    "        # Calculate overtaking success rate (normalized between 0-1)\n",
    "        positive_overtakes = (positions_gained > 0).sum()\n",
    "        negative_overtakes = (positions_gained < 0).sum()\n",
    "        total_overtake_attempts = positive_overtakes + negative_overtakes\n",
    "        overtake_success_rate = positive_overtakes / total_overtake_attempts if total_overtake_attempts > 0 else 0.5\n",
    "        \n",
    "        # Normalize average positions gained (0-1)\n",
    "        avg_positions_gained = positions_gained[positions_gained > 0].mean() if len(positions_gained[positions_gained > 0]) > 0 else 0\n",
    "        max_possible_gain = 20  # Maximum grid positions that could be gained\n",
    "        normalized_gains = np.clip(avg_positions_gained / max_possible_gain, 0, 1)\n",
    "        \n",
    "        # Normalize risk factors (0-1)\n",
    "        normalized_dnf = np.clip(dnf_rate, 0, 1)\n",
    "        normalized_incidents = np.clip(incidents, 0, 1)\n",
    "        \n",
    "        # Calculate component scores (each between 0-1)\n",
    "        overtaking_component = (normalized_gains * 0.6 + overtake_success_rate * 0.4)\n",
    "        risk_component = (normalized_dnf * 0.5 + normalized_incidents * 0.5)\n",
    "        \n",
    "        # Combine components with weights (ensuring sum of weights = 1)\n",
    "        weights = {\n",
    "            'overtaking': 0.4,  # Aggressive overtaking\n",
    "            'risk': 0.5,       # Risk-taking behavior\n",
    "            'baseline': 0.1    # Baseline aggression\n",
    "        }\n",
    "        \n",
    "        aggression = (\n",
    "            overtaking_component * weights['overtaking'] +\n",
    "            risk_component * weights['risk'] +\n",
    "            0.5 * weights['baseline']  # Baseline aggression factor\n",
    "        )\n",
    "        \n",
    "        # Add small random variation while maintaining 0-1 bounds\n",
    "        variation = np.random.normal(0, 0.02)\n",
    "        aggression = np.clip(aggression + variation, 0, 1)\n",
    "        \n",
    "        return aggression\n",
    "    \n",
    "    def calculate_skill(driver_data, results_data, circuit_id):\n",
    "        driver_results = results_data[\n",
    "            (results_data['driverId'] == driver_data['driverId']) & \n",
    "            (results_data['circuitId'] == circuit_id)\n",
    "        ].sort_values('date', ascending=False).head(10)  # Use last 10 races at circuit\n",
    "        \n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default skill\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        avg_finish_pos = driver_results['positionOrder'].mean()\n",
    "        avg_quali_pos = driver_results['grid'].mean()\n",
    "        points_per_race = driver_results['points'].mean()\n",
    "        fastest_laps = (driver_results['rank'] == 1).mean()  # Add fastest lap consideration\n",
    "        \n",
    "        # Improved normalization (exponential decay for positions)\n",
    "        normalized_finish_pos = np.exp(-avg_finish_pos/5) # Better spread of values\n",
    "        normalized_quali_pos = np.exp(-avg_quali_pos/5)\n",
    "        \n",
    "        # Points normalization with improved scaling\n",
    "        max_points_per_race = 26  # Maximum possible points (25 + 1 fastest lap)\n",
    "        normalized_points = points_per_race / max_points_per_race\n",
    "        \n",
    "        # Weighted combination with more factors\n",
    "        weights = {\n",
    "            'finish': 0.35,\n",
    "            'quali': 0.25,\n",
    "            'points': 0.25,\n",
    "            'fastest_laps': 0.15\n",
    "        }\n",
    "        \n",
    "        skill = (\n",
    "            weights['finish'] * normalized_finish_pos +\n",
    "            weights['quali'] * normalized_quali_pos +\n",
    "            weights['points'] * normalized_points +\n",
    "            weights['fastest_laps'] * fastest_laps\n",
    "        )\n",
    "        \n",
    "        # Add random variation to prevent identical skills\n",
    "        skill = np.clip(skill + np.random.normal(0, 0.05), 0.1, 1.0)\n",
    "        \n",
    "        return skill\n",
    "    \n",
    "    # First merge results with races to get circuitId\n",
    "    results = results.merge(\n",
    "        races[['raceId', 'circuitId']], \n",
    "        on='raceId',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Now calculate driver aggression and skill\n",
    "    driver_aggression = {}\n",
    "    driver_skill = {}\n",
    "    for driver_id in drivers['driverId'].unique():\n",
    "        driver_results = results[results['driverId'] == driver_id]\n",
    "        aggression = calculate_aggression(driver_results)\n",
    "        driver_aggression[driver_id] = aggression\n",
    "        \n",
    "        # Now we have circuit_id from the merge\n",
    "        recent_race = driver_results.sort_values('date', ascending=False).head(1)\n",
    "        if not recent_race.empty:\n",
    "            circuit_id = recent_race['circuitId'].iloc[0]\n",
    "            skill = calculate_skill({'driverId': driver_id}, results, circuit_id)\n",
    "            driver_skill[driver_id] = skill\n",
    "        else:\n",
    "            driver_skill[driver_id] = 0.5  # Default skill for new drivers\n",
    "    \n",
    "    # Map calculated aggression and skill back to laps DataFrame\n",
    "    laps['driver_aggression'] = laps['driverId'].map(driver_aggression)\n",
    "    laps['driver_overall_skill'] = laps['driverId'].map(driver_skill)\n",
    "    laps['driver_circuit_skill'] = laps['driver_overall_skill']  # For simplicity, using overall skill\n",
    "    laps['driver_consistency'] = 0.5  # Placeholder\n",
    "    laps['driver_reliability'] = 0.5  # Placeholder\n",
    "    laps['driver_risk_taking'] = laps['driver_aggression']  # Assuming similar to aggression\n",
    "    \n",
    "    # Dynamic features\n",
    "    laps['tire_age'] = laps.groupby(['raceId', 'driverId'])['lap'].cumcount()\n",
    "    laps['fuel_load'] = laps.groupby(['raceId', 'driverId'])['lap'].transform(lambda x: x.max() - x + 1)\n",
    "    laps['track_position'] = laps['position']  # Assuming 'position' is available in laps data\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    # Create an instance of RaceFeatures\n",
    "    race_features = RaceFeatures()\n",
    "\n",
    "    \n",
    "    laps['TrackStatus'].fillna(1, inplace=True)  # 1 = regular racing status\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    required_columns = race_features.static_features + race_features.dynamic_features\n",
    "    # Before dropping NaN values\n",
    "    print(\"\\nNaN counts in required columns:\")\n",
    "    for col in required_columns:\n",
    "        nan_count = laps[col].isna().sum()\n",
    "        total_rows = len(laps)\n",
    "        if nan_count > 0:\n",
    "            print(f\"{col}: {nan_count} NaN values ({(nan_count/total_rows*100):.2f}% of rows)\")\n",
    "    missing_columns = set(required_columns) - set(laps.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Drop rows with missing values in required columns\n",
    "    laps.to_csv('laps_withNan.csv', index=False)\n",
    "    laps = laps.dropna(subset=required_columns)\n",
    "    \n",
    "    print(laps.shape)\n",
    "    \n",
    "    return laps\n",
    "\n",
    "# Update the main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    enhanced_laps = load_and_preprocess_data()\n",
    "    \n",
    "    enhanced_laps.drop(columns=['position', 'time', 'driverRef', 'number', 'R', 'S', 'code', 'forename', 'surname', 'url_x', 'url_race', 'name_x', 'circuitRef', 'name_y', 'location', 'country', 'url_y', 'positionOrder', 'fastestLap', 'cumulative_milliseconds', 'seconds_from_start', 'raceId_x', 'year_x', 'Time', 'TrackTemp', 'AirTemp', 'Humidity', 'name', 'year_y', 'raceId_y'], inplace=True)\n",
    "    \n",
    "    # Save the preprocessed laps DataFrame for inspection\n",
    "    enhanced_laps.to_csv('enhanced_laps_before_training.csv', index=False)\n",
    "    print(enhanced_laps.shape)\n",
    "    \n",
    "    print(\"Enhanced laps DataFrame saved to 'enhanced_laps_before_training.csv'\")\n",
    "    \n",
    "    preprocessor = F1DataPreprocessor()\n",
    "    sequences, static, targets = preprocessor.prepare_sequence_data(enhanced_laps, window_size=3)\n",
    "    \n",
    "    # Create train and validation loaders\n",
    "    train_loader, val_loader = preprocessor.create_train_val_loaders(\n",
    "        sequences, \n",
    "        static, \n",
    "        targets,\n",
    "        batch_size=32,\n",
    "        val_split=0.2\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = F1PredictionModel(\n",
    "        sequence_dim=sequences.shape[2],\n",
    "        static_dim=static.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = train_model(model, train_loader, val_loader, epochs=100, learning_rate=0.001)\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model_with_preprocessor(model, preprocessor, sequences.shape[2], static.shape[1], 'f1_prediction_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# %% [markdown]\n",
    "# # Phase 2\n",
    "\n",
    "# %%\n",
    "class RaceSimulator:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.race_features = RaceFeatures()\n",
    "        \n",
    "    def simulate_lap(self, current_state):\n",
    "        \"\"\"\n",
    "        Predict lap time and uncertainty for a single lap.\n",
    "        \"\"\"\n",
    "        # Prepare input data\n",
    "        sequence = current_state['sequence']\n",
    "        static = current_state['static']\n",
    "        \n",
    "        # Convert to tensors\n",
    "        sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0)  # Add batch dimension\n",
    "        static_tensor = torch.FloatTensor(static).unsqueeze(0)\n",
    "        \n",
    "        # Predict with uncertainty\n",
    "        mean_pred, std_pred = predict_with_uncertainty(\n",
    "            self.model, \n",
    "            {'sequence': sequence_tensor, 'static': static_tensor},\n",
    "            n_samples=50\n",
    "        )\n",
    "        \n",
    "        # Inverse transform the prediction\n",
    "        lap_time = self.preprocessor.lap_time_scaler.inverse_transform([[mean_pred]])[0][0]\n",
    "        uncertainty = self.preprocessor.lap_time_scaler.inverse_transform([[std_pred]])[0][0]\n",
    "        \n",
    "        return lap_time, uncertainty\n",
    "    \n",
    "    def simulate_full_race(self, initial_state, strategy):\n",
    "        \"\"\"\n",
    "        Simulate the entire race lap by lap based on the provided strategy.\n",
    "        \"\"\"\n",
    "        lap_times = []\n",
    "        uncertainties = []\n",
    "        current_state = initial_state.copy()\n",
    "        \n",
    "        total_laps = strategy.total_laps\n",
    "        for lap in range(1, total_laps + 1):\n",
    "            # Update dynamic features based on strategy and lap number\n",
    "            current_state = self.update_state(current_state, lap, strategy)\n",
    "            \n",
    "            # Simulate lap\n",
    "            lap_time, uncertainty = self.simulate_lap(current_state)\n",
    "            lap_times.append(lap_time)\n",
    "            uncertainties.append(uncertainty)\n",
    "            \n",
    "            # Update sequence data for the next lap\n",
    "            current_state['sequence'] = self.update_sequence(\n",
    "                current_state['sequence'], lap_time, current_state['dynamic']\n",
    "            )\n",
    "        \n",
    "        return lap_times, uncertainties\n",
    "    \n",
    "    def update_state(self, state, lap, strategy):\n",
    "        \"\"\"\n",
    "        Update the state for the next lap based on the strategy.\n",
    "        \"\"\"\n",
    "        # Update tire age\n",
    "        state['dynamic']['tire_age'] += 1\n",
    "        \n",
    "        # Check for pit stops\n",
    "        if lap in strategy.pit_stop_laps:\n",
    "            # Reset tire age and update tire compound\n",
    "            state['dynamic']['tire_age'] = 0\n",
    "            state['dynamic']['tire_compound'] = strategy.pit_stop_compounds[lap]\n",
    "        \n",
    "        # Update fuel load\n",
    "        state['dynamic']['fuel_load'] -= strategy.fuel_consumption_per_lap\n",
    "        \n",
    "        # Update other dynamic features as needed\n",
    "        # ...\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def update_sequence(self, sequence, new_lap_time, dynamic_features):\n",
    "        \"\"\"\n",
    "        Update the sequence data with the latest lap information.\n",
    "        \"\"\"\n",
    "        # Remove the oldest lap data\n",
    "        sequence = sequence[1:]\n",
    "        # Append the new lap data\n",
    "        new_sequence_entry = np.hstack((\n",
    "            [new_lap_time],\n",
    "            [dynamic_features[feature] for feature in self.race_features.dynamic_features]\n",
    "        ))\n",
    "        sequence = np.vstack([sequence, new_sequence_entry])\n",
    "        return sequence\n",
    "    \n",
    "    # Additional helper methods as needed\n",
    "\n",
    "\n",
    "# %%\n",
    "class RaceStrategy:\n",
    "    def __init__(self, total_laps, pit_stops):\n",
    "        \"\"\"\n",
    "        pit_stops: List of dictionaries with keys 'lap' and 'compound'\n",
    "        Example: [{'lap': 15, 'compound': 'MEDIUM'}, {'lap': 30, 'compound': 'SOFT'}]\n",
    "        \"\"\"\n",
    "        self.total_laps = total_laps\n",
    "        self.pit_stops = pit_stops  # List of pit stop events\n",
    "        self.pit_stop_laps = [stop['lap'] for stop in pit_stops]\n",
    "        self.pit_stop_compounds = {stop['lap']: stop['compound'] for stop in pit_stops}\n",
    "        self.fuel_consumption_per_lap = 1.5  # Example value, adjust as needed\n",
    "    \n",
    "    def evaluate(self, simulator, initial_state):\n",
    "        \"\"\"\n",
    "        Simulate the race using this strategy and return total race time.\n",
    "        \"\"\"\n",
    "        lap_times, uncertainties = simulator.simulate_full_race(initial_state, self)\n",
    "        total_time = sum(lap_times)\n",
    "        return total_time, lap_times, uncertainties\n",
    "\n",
    "\n",
    "# %%\n",
    "class RaceStrategyOptimizer:\n",
    "    def __init__(self, simulator):\n",
    "        self.simulator = simulator\n",
    "    \n",
    "    def optimize(self, initial_conditions, constraints):\n",
    "        \"\"\"\n",
    "        Find the optimal strategy given initial conditions and constraints.\n",
    "        \"\"\"\n",
    "        best_strategy = None\n",
    "        best_time = float('inf')\n",
    "        \n",
    "        # Generate possible strategies within constraints\n",
    "        possible_strategies = self.generate_strategies(constraints)\n",
    "        \n",
    "        # Evaluate each strategy\n",
    "        for strategy in possible_strategies:\n",
    "            total_time, _, _ = strategy.evaluate(self.simulator, initial_conditions)\n",
    "            if total_time < best_time:\n",
    "                best_time = total_time\n",
    "                best_strategy = strategy\n",
    "        \n",
    "        return best_strategy\n",
    "    \n",
    "    def generate_strategies(self, constraintas):\n",
    "        \"\"\"\n",
    "        Generate possible strategies based on constraints.\n",
    "        \"\"\"\n",
    "        min_pit_stops = constraints.get('min_pit_stops', 1)\n",
    "        max_pit_stops = constraints.get('max_pit_stops', 3)\n",
    "        available_compounds = constraints.get('available_compounds', ['SOFT', 'MEDIUM', 'HARD'])\n",
    "        total_laps = constraints.get('total_laps', 50)\n",
    "        \n",
    "        strategies = []\n",
    "        \n",
    "        # Example: Generate strategies with different pit stop laps and compounds\n",
    "        for num_pit_stops in range(min_pit_stops, max_pit_stops + 1):\n",
    "            pit_stop_laps_options = self.get_pit_stop_lap_combinations(total_laps, num_pit_stops)\n",
    "            for pit_stop_laps in pit_stop_laps_options:\n",
    "                for compounds in self.get_compound_combinations(available_compounds, num_pit_stops):\n",
    "                    pit_stops = [{'lap': lap, 'compound': comp} for lap, comp in zip(pit_stop_laps, compounds)]\n",
    "                    strategy = RaceStrategy(total_laps, pit_stops)\n",
    "                    strategies.append(strategy)\n",
    "        return strategies\n",
    "    \n",
    "    def get_pit_stop_lap_combinations(self, total_laps, num_pit_stops):\n",
    "        \"\"\"\n",
    "        Generate possible pit stop lap combinations.\n",
    "        \"\"\"\n",
    "        from itertools import combinations\n",
    "        lap_numbers = range(5, total_laps - 5)  # Avoid pitting too early or too late\n",
    "        return combinations(lap_numbers, num_pit_stops)\n",
    "    \n",
    "    def get_compound_combinations(self, compounds, num_pit_stops):\n",
    "        \"\"\"\n",
    "        Generate possible combinations of compounds for pit stops.\n",
    "        \"\"\"\n",
    "        from itertools import product\n",
    "        return product(compounds, repeat=num_pit_stops)\n",
    "    \n",
    "    def load_model_with_preprocessor(path: str, sequence_dim: int, static_dim: int, hidden_dim: int = 64, num_layers: int = 2):\n",
    "        \"\"\"\n",
    "        Load the model and preprocessor from a saved file.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "        \n",
    "        # Recreate the model architecture\n",
    "        model = F1PredictionModel(\n",
    "            sequence_dim=sequence_dim,\n",
    "            static_dim=static_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        # Recreate the preprocessor\n",
    "        preprocessor = F1DataPreprocessor()\n",
    "        preprocessor.lap_time_scaler = checkpoint['lap_time_scaler']\n",
    "        preprocessor.dynamic_scaler = checkpoint['dynamic_scaler']\n",
    "        preprocessor.static_scaler = checkpoint['static_scaler']\n",
    "        \n",
    "        print(f\"Model and preprocessor loaded from {path}\")\n",
    "        return model, preprocessor\n",
    "\n",
    "\n",
    "# %%\n",
    "def load_model_with_preprocessor(path: str) -> Tuple[F1PredictionModel, F1DataPreprocessor]:\n",
    "   \"\"\"Load saved model and preprocessor\"\"\"\n",
    "   checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "   \n",
    "   model = F1PredictionModel(\n",
    "       sequence_dim=checkpoint['sequence_dim'], \n",
    "       static_dim=checkpoint['static_dim']\n",
    "   )\n",
    "   model.load_state_dict(checkpoint['model_state_dict'])\n",
    "   \n",
    "   preprocessor = F1DataPreprocessor()\n",
    "   preprocessor.lap_time_scaler = checkpoint['lap_time_scaler']\n",
    "   preprocessor.dynamic_scaler = checkpoint['dynamic_scaler']\n",
    "   preprocessor.static_scaler = checkpoint['static_scaler']\n",
    "   \n",
    "   return model, preprocessor\n",
    "\n",
    "# %%\n",
    "# The model and preprocessor are now ready for simulation or further training\n",
    "model, preprocessor = load_model_with_preprocessor('f1_prediction_model.pth')\n",
    "\n",
    "# Assume 'model' and 'preprocessor' have been loaded or trained\n",
    "simulator = RaceSimulator(model, preprocessor)\n",
    "\n",
    "# Step 1: Define static features\n",
    "static_features_list = [\n",
    "    'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "    'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "    'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time'\n",
    "]\n",
    "\n",
    "static_features = np.array([\n",
    "    0.8,   # driver_overall_skill\n",
    "    0.75,  # driver_circuit_skill\n",
    "    0.7,   # driver_consistency\n",
    "    0.9,   # driver_reliability\n",
    "    0.6,   # driver_aggression\n",
    "    0.5,   # driver_risk_taking\n",
    "    88000, # fp1_median_time\n",
    "    87500, # fp2_median_time\n",
    "    87000, # fp3_median_time\n",
    "    86000  # quali_time\n",
    "])\n",
    "\n",
    "# Step 2: Define initial dynamic features for previous laps\n",
    "dynamic_features_list = [\n",
    "    'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "    'air_temp', 'humidity', 'TrackStatus', 'is_pit_lap'\n",
    "]\n",
    "\n",
    "# Common values\n",
    "track_temp = 35.0\n",
    "air_temp = 25.0\n",
    "humidity = 50.0\n",
    "TrackStatus = 1\n",
    "is_pit_lap = 0\n",
    "\n",
    "dynamic_features = np.array([\n",
    "    [0, 100, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap],  # Lap 1\n",
    "    [1, 98.5, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap],  # Lap 2\n",
    "    [2, 97, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap]     # Lap 3\n",
    "])\n",
    "\n",
    "# Tire compound (not scaled)\n",
    "tire_compound = 0  # Example value for 'HARD' tires\n",
    "tire_compound_column = np.full((dynamic_features.shape[0], 1), tire_compound)\n",
    "\n",
    "# Lap times in milliseconds\n",
    "lap_times = np.array([90000, 89500, 89200])  # Laps 1-3\n",
    "\n",
    "# Step 3: Combine lap times, tire_compound, and dynamic features\n",
    "sequence = np.hstack((\n",
    "    lap_times.reshape(-1, 1),  # Lap times\n",
    "    tire_compound_column,      # Tire compound\n",
    "    dynamic_features           # Dynamic features\n",
    "))\n",
    "\n",
    "# Step 4: Scale the lap times\n",
    "lap_times_scaled = preprocessor.lap_time_scaler.transform(lap_times.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Step 5: Scale the dynamic features (excluding 'tire_compound')\n",
    "dynamic_features_to_scale = sequence[:, 2:]  # Exclude lap times and 'tire_compound'\n",
    "dynamic_scaled = preprocessor.dynamic_scaler.transform(dynamic_features_to_scale)\n",
    "\n",
    "# Step 6: Reconstruct the scaled sequence\n",
    "sequence_scaled = np.hstack((\n",
    "    lap_times_scaled.reshape(-1, 1),  # Scaled lap times\n",
    "    tire_compound_column,             # Tire compound (not scaled)\n",
    "    dynamic_scaled                    # Scaled dynamic features\n",
    "))\n",
    "\n",
    "# Step 7: Scale the static features\n",
    "static_scaled = preprocessor.static_scaler.transform(static_features.reshape(1, -1)).flatten()\n",
    "\n",
    "# Step 8: Prepare current dynamic features\n",
    "current_dynamic_features = {\n",
    "    'tire_age': 3,\n",
    "    'fuel_load': 95.5,\n",
    "    'track_position': 1,\n",
    "    'track_temp': 35.0,\n",
    "    'air_temp': 25.0,\n",
    "    'humidity': 50.0,\n",
    "    'TrackStatus': 1,\n",
    "    'is_pit_lap': 0,\n",
    "    'tire_compound': 0\n",
    "}\n",
    "\n",
    "# Extract and scale dynamic features (excluding 'tire_compound')\n",
    "dynamic_feature_values = np.array([\n",
    "    current_dynamic_features[feature] for feature in dynamic_features_list\n",
    "])\n",
    "\n",
    "dynamic_scaled_current = preprocessor.dynamic_scaler.transform(dynamic_feature_values.reshape(1, -1)).flatten()\n",
    "\n",
    "# Step 9: Update the initial state\n",
    "initial_state = {\n",
    "    'sequence': sequence_scaled,   # Shape: (window_size, sequence_dim)\n",
    "    'static': static_scaled,       # Shape: (static_dim,)\n",
    "    'dynamic': current_dynamic_features\n",
    "}\n",
    "\n",
    "\n",
    "# Define constraints\n",
    "constraints = {\n",
    "    'min_pit_stops': 1,\n",
    "    'max_pit_stops': 2,\n",
    "    'available_compounds': [3, 2, 1],\n",
    "    'total_laps': 50\n",
    "}\n",
    "\n",
    "# Create optimizer and find the best strategy\n",
    "optimizer = RaceStrategyOptimizer(simulator)\n",
    "best_strategy = optimizer.optimize(initial_state, constraints)\n",
    "\n",
    "# Evaluate the best strategy\n",
    "total_time, lap_times, uncertainties = best_strategy.evaluate(simulator, initial_state)\n",
    "print(f\"Optimal total race time: {total_time:.2f} seconds\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "First of all, I need somekind of logging or process bar during the simulations.\n",
    "\n",
    "Second, I think if the model is working so far (we can try out other models later)\n",
    "\n",
    "The goal now would be to not only have a tool to find out the best strategy, but also to predict whole races. I image it like this:\n",
    "Inputs:\n",
    "-data from before, free practice etc.\n",
    "-circuitId to tell the model which circuit the race is on\n",
    "-weatherforecast: in our case we could just do a train test split with the races (including all their laps) and since we know the weather data, we can just assume that that is the forecast\n",
    "-which drivers will drive for which constructor through driverId (maybe each driver as an agent with (skill. agression etc.?)\n",
    "-\n",
    "-Pitstop, I assume optimizing to get the best strategy for each driver would take a while, for now lets just say we predefined one strategy for all of them (when to stop and what compounds)\n",
    "\n",
    "When we introduce these new variables we probably need to add them the model as well?\n",
    "\n",
    "Third, I think we need to introduce some like Global Race Events i.e Retirements, Safetey car etc. One could probably train models for this as well, e.g. which lap a safety car is most likely, how likely and when a driver / car will retire. But for now we maybe can also just predefined those things.\n",
    "structure all the ideas I just outlined and think about whats missing and in which steps to implement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
