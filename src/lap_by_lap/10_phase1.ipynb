{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fb0bbdee5a394c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:46:55.129913Z",
     "start_time": "2024-11-28T17:46:49.334643Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_6686/3940932896.py:301: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  practice_sessions = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_6686/3940932896.py:303: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tire_data = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586171, 15)\n",
      "(586171, 32)\n",
      "(586171, 40)\n",
      "(586171, 45)\n",
      "(586171, 46)\n",
      "(586171, 47)\n",
      "(586171, 47)\n",
      "(586171, 49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_6686/3940932896.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['pitstop_milliseconds'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_6686/3940932896.py:396: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['Compound'].fillna('UNKNOWN', inplace=True)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_6686/3940932896.py:449: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['fp1_median_time'].fillna(global_median_fp1, inplace=True)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_6686/3940932896.py:453: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['fp2_median_time'].fillna(global_median_fp2, inplace=True)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_6686/3940932896.py:456: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['fp3_median_time'].fillna(global_median_fp3, inplace=True)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_6686/3940932896.py:459: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['quali_time'].fillna(global_median_quali, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching for 1\n",
      "Matching for 2\n",
      "Matching for 3\n",
      "Matching for 4\n",
      "Matching for 5\n",
      "Matching for 6\n",
      "Matching for 7\n",
      "Matching for 8\n",
      "Matching for 9\n",
      "Matching for 10\n",
      "Matching for 11\n",
      "Matching for 12\n",
      "Matching for 13\n",
      "Matching for 14\n",
      "Matching for 15\n",
      "Matching for 16\n",
      "Matching for 17\n",
      "Matching for 18\n",
      "Matching for 19\n",
      "Matching for 20\n",
      "Matching for 21\n",
      "Matching for 22\n",
      "Matching for 23\n",
      "Matching for 24\n",
      "Matching for 25\n",
      "Matching for 26\n",
      "Matching for 27\n",
      "Matching for 28\n",
      "Matching for 29\n",
      "Matching for 30\n",
      "Matching for 31\n",
      "Matching for 32\n",
      "Matching for 33\n",
      "Matching for 34\n",
      "Matching for 35\n",
      "Matching for 36\n",
      "Matching for 37\n",
      "Matching for 38\n",
      "Matching for 39\n",
      "Matching for 40\n",
      "Matching for 41\n",
      "Matching for 42\n",
      "Matching for 43\n",
      "Matching for 44\n",
      "Matching for 45\n",
      "Matching for 46\n",
      "Matching for 47\n",
      "Matching for 48\n",
      "Matching for 49\n",
      "Matching for 50\n",
      "Matching for 51\n",
      "Matching for 52\n",
      "Matching for 53\n",
      "Matching for 54\n",
      "Matching for 55\n",
      "Matching for 56\n",
      "Matching for 57\n",
      "Matching for 58\n",
      "Matching for 59\n",
      "Matching for 60\n",
      "Matching for 61\n",
      "Matching for 62\n",
      "Matching for 63\n",
      "Matching for 64\n",
      "Matching for 65\n",
      "Matching for 66\n",
      "Matching for 67\n",
      "Matching for 68\n",
      "Matching for 69\n",
      "Matching for 70\n",
      "Matching for 71\n",
      "Matching for 72\n",
      "Matching for 73\n",
      "Matching for 74\n",
      "Matching for 75\n",
      "Matching for 76\n",
      "Matching for 77\n",
      "Matching for 78\n",
      "Matching for 79\n",
      "Matching for 80\n",
      "Matching for 81\n",
      "Matching for 82\n",
      "Matching for 83\n",
      "Matching for 84\n",
      "Matching for 85\n",
      "Matching for 86\n",
      "Matching for 87\n",
      "Matching for 88\n",
      "Matching for 89\n",
      "Matching for 90\n",
      "Matching for 91\n",
      "Matching for 92\n",
      "Matching for 93\n",
      "Matching for 94\n",
      "Matching for 95\n",
      "Matching for 96\n",
      "Matching for 97\n",
      "Matching for 98\n",
      "Matching for 99\n",
      "Matching for 100\n",
      "Matching for 101\n",
      "Matching for 102\n",
      "Matching for 103\n",
      "Matching for 104\n",
      "Matching for 105\n",
      "Matching for 106\n",
      "Matching for 107\n",
      "Matching for 108\n",
      "Matching for 109\n",
      "Matching for 110\n",
      "Matching for 111\n",
      "Matching for 112\n",
      "Matching for 113\n",
      "Matching for 114\n",
      "Matching for 115\n",
      "Matching for 116\n",
      "Matching for 117\n",
      "Matching for 118\n",
      "Matching for 119\n",
      "Matching for 120\n",
      "Matching for 121\n",
      "Matching for 122\n",
      "Matching for 123\n",
      "Matching for 124\n",
      "Matching for 125\n",
      "Matching for 126\n",
      "Matching for 127\n",
      "Matching for 128\n",
      "Matching for 129\n",
      "Matching for 130\n",
      "Matching for 131\n",
      "Matching for 132\n",
      "Matching for 133\n",
      "Matching for 134\n",
      "Matching for 135\n",
      "Matching for 136\n",
      "Matching for 137\n",
      "Matching for 138\n",
      "Matching for 139\n",
      "Matching for 140\n",
      "Matching for 141\n",
      "Matching for 142\n",
      "Matching for 143\n",
      "Matching for 144\n",
      "Matching for 145\n",
      "Matching for 146\n",
      "Matching for 147\n",
      "Matching for 148\n",
      "Matching for 149\n",
      "Matching for 150\n",
      "Matching for 151\n",
      "Matching for 152\n",
      "Matching for 153\n",
      "Matching for 154\n",
      "Matching for 155\n",
      "Matching for 156\n",
      "Matching for 157\n",
      "Matching for 158\n",
      "Matching for 159\n",
      "Matching for 160\n",
      "Matching for 161\n",
      "Matching for 162\n",
      "Matching for 163\n",
      "Matching for 164\n",
      "Matching for 165\n",
      "Matching for 166\n",
      "Matching for 167\n",
      "Matching for 168\n",
      "Matching for 169\n",
      "Matching for 170\n",
      "Matching for 171\n",
      "Matching for 172\n",
      "Matching for 173\n",
      "Matching for 174\n",
      "Matching for 175\n",
      "Matching for 176\n",
      "Matching for 177\n",
      "Matching for 178\n",
      "Matching for 179\n",
      "Matching for 180\n",
      "Matching for 181\n",
      "Matching for 182\n",
      "Matching for 183\n",
      "Matching for 184\n",
      "Matching for 185\n",
      "Matching for 186\n",
      "Matching for 187\n",
      "Matching for 188\n",
      "Matching for 189\n",
      "Matching for 190\n",
      "Matching for 191\n",
      "Matching for 192\n",
      "Matching for 193\n",
      "Matching for 194\n",
      "Matching for 195\n",
      "Matching for 196\n",
      "Matching for 197\n",
      "Matching for 198\n",
      "Matching for 199\n",
      "Matching for 200\n",
      "Matching for 201\n",
      "Matching for 202\n",
      "Matching for 203\n",
      "Matching for 204\n",
      "Matching for 205\n",
      "Matching for 206\n",
      "Matching for 207\n",
      "Matching for 208\n",
      "Matching for 209\n",
      "Matching for 210\n",
      "Matching for 211\n",
      "Matching for 212\n",
      "Matching for 213\n",
      "Matching for 214\n",
      "Matching for 215\n",
      "Matching for 216\n",
      "Matching for 217\n",
      "Matching for 218\n",
      "Matching for 219\n",
      "Matching for 220\n",
      "Matching for 221\n",
      "Matching for 222\n",
      "Matching for 223\n",
      "Matching for 224\n",
      "Matching for 225\n",
      "Matching for 226\n",
      "Matching for 227\n",
      "Matching for 228\n",
      "Matching for 229\n",
      "Matching for 230\n",
      "Matching for 231\n",
      "Matching for 232\n",
      "Matching for 233\n",
      "Matching for 234\n",
      "Matching for 235\n",
      "Matching for 236\n",
      "Matching for 237\n",
      "Matching for 238\n",
      "Matching for 239\n",
      "Matching for 337\n",
      "Matching for 338\n",
      "Matching for 339\n",
      "Matching for 340\n",
      "Matching for 341\n",
      "Matching for 342\n",
      "Matching for 343\n",
      "Matching for 344\n",
      "Matching for 345\n",
      "Matching for 346\n",
      "Matching for 347\n",
      "Matching for 348\n",
      "Matching for 349\n",
      "Matching for 350\n",
      "Matching for 351\n",
      "Matching for 352\n",
      "Matching for 353\n",
      "Matching for 354\n",
      "Matching for 355\n",
      "Matching for 841\n",
      "Matching for 842\n",
      "Matching for 843\n",
      "Matching for 844\n",
      "Matching for 845\n",
      "Matching for 846\n",
      "Matching for 847\n",
      "Matching for 848\n",
      "Matching for 849\n",
      "Matching for 850\n",
      "Matching for 851\n",
      "Matching for 852\n",
      "Matching for 853\n",
      "Matching for 854\n",
      "Matching for 855\n",
      "Matching for 856\n",
      "Matching for 857\n",
      "Matching for 858\n",
      "Matching for 859\n",
      "Matching for 860\n",
      "Matching for 861\n",
      "Matching for 862\n",
      "Matching for 863\n",
      "Matching for 864\n",
      "Matching for 865\n",
      "Matching for 866\n",
      "Matching for 867\n",
      "Matching for 868\n",
      "Matching for 869\n",
      "Matching for 870\n",
      "Matching for 871\n",
      "Matching for 872\n",
      "Matching for 873\n",
      "Matching for 874\n",
      "Matching for 875\n",
      "Matching for 876\n",
      "Matching for 877\n",
      "Matching for 878\n",
      "Matching for 879\n",
      "Matching for 880\n",
      "Matching for 881\n",
      "Matching for 882\n",
      "Matching for 883\n",
      "Matching for 884\n",
      "Matching for 885\n",
      "Matching for 886\n",
      "Matching for 887\n",
      "Matching for 888\n",
      "Matching for 890\n",
      "Matching for 891\n",
      "Matching for 892\n",
      "Matching for 893\n",
      "Matching for 894\n",
      "Matching for 895\n",
      "Matching for 896\n",
      "Matching for 897\n",
      "Matching for 898\n",
      "Matching for 899\n",
      "Matching for 900\n",
      "Matching for 901\n",
      "Matching for 902\n",
      "Matching for 903\n",
      "Matching for 904\n",
      "Matching for 905\n",
      "Matching for 906\n",
      "Matching for 907\n",
      "Matching for 908\n",
      "Matching for 909\n",
      "Matching for 910\n",
      "Matching for 911\n",
      "Matching for 912\n",
      "Matching for 913\n",
      "Matching for 914\n",
      "Matching for 915\n",
      "Matching for 916\n",
      "Matching for 917\n",
      "Matching for 918\n",
      "Matching for 926\n",
      "Matching for 927\n",
      "Matching for 928\n",
      "Matching for 929\n",
      "Matching for 930\n",
      "Matching for 931\n",
      "Matching for 932\n",
      "Matching for 933\n",
      "Matching for 934\n",
      "Matching for 936\n",
      "Matching for 937\n",
      "Matching for 938\n",
      "Matching for 939\n",
      "Matching for 940\n",
      "Matching for 941\n",
      "Matching for 942\n",
      "Matching for 943\n",
      "Matching for 944\n",
      "Matching for 945\n",
      "Matching for 948\n",
      "Matching for 949\n",
      "Matching for 950\n",
      "Matching for 951\n",
      "Matching for 952\n",
      "Matching for 953\n",
      "Matching for 954\n",
      "Matching for 955\n",
      "Matching for 956\n",
      "Matching for 957\n",
      "Matching for 958\n",
      "Matching for 959\n",
      "Matching for 960\n",
      "Matching for 961\n",
      "Matching for 962\n",
      "Matching for 963\n",
      "Matching for 964\n",
      "Matching for 965\n",
      "Matching for 966\n",
      "Matching for 967\n",
      "Matching for 968\n",
      "Matching for 969\n",
      "Matching for 970\n",
      "Matching for 971\n",
      "Matching for 972\n",
      "Matching for 973\n",
      "Matching for 974\n",
      "Matching for 975\n",
      "Matching for 976\n",
      "Matching for 977\n",
      "Matching for 978\n",
      "Matching for 979\n",
      "Matching for 980\n",
      "Matching for 981\n",
      "Matching for 982\n",
      "Matching for 983\n",
      "Matching for 984\n",
      "Matching for 985\n",
      "Matching for 986\n",
      "Matching for 987\n",
      "Matching for 988\n",
      "Matching for 989\n",
      "Matched DataFrame shape: (937, 74)\n",
      "Matching for 990\n",
      "Matched DataFrame shape: (997, 74)\n",
      "Matching for 991\n",
      "Matched DataFrame shape: (1115, 74)\n",
      "Matching for 992\n",
      "Matched DataFrame shape: (841, 74)\n",
      "Matching for 993\n",
      "Matched DataFrame shape: (1016, 74)\n",
      "Matching for 994\n",
      "Matching for 995\n",
      "Matched DataFrame shape: (1182, 74)\n",
      "Matching for 996\n",
      "Matched DataFrame shape: (915, 74)\n",
      "Matching for 997\n",
      "Matched DataFrame shape: (1239, 74)\n",
      "Matching for 998\n",
      "Matched DataFrame shape: (898, 74)\n",
      "Matching for 999\n",
      "Matched DataFrame shape: (1250, 74)\n",
      "Matching for 1000\n",
      "Matched DataFrame shape: (1230, 74)\n",
      "Matching for 1001\n",
      "Matched DataFrame shape: (690, 74)\n",
      "Matching for 1002\n",
      "Matched DataFrame shape: (925, 74)\n",
      "Matching for 1003\n",
      "Matched DataFrame shape: (1145, 74)\n",
      "Matching for 1004\n",
      "Matched DataFrame shape: (948, 74)\n",
      "Matching for 1005\n",
      "Matched DataFrame shape: (976, 74)\n",
      "Matching for 1006\n",
      "Matched DataFrame shape: (931, 74)\n",
      "Matching for 1007\n",
      "Matched DataFrame shape: (1242, 74)\n",
      "Matching for 1008\n",
      "Matched DataFrame shape: (1318, 74)\n",
      "Matching for 1009\n",
      "Matched DataFrame shape: (938, 74)\n",
      "Matching for 1010\n",
      "Matching for 1011\n",
      "Matched DataFrame shape: (1081, 74)\n",
      "Matching for 1012\n",
      "Matched DataFrame shape: (1046, 74)\n",
      "Matching for 1013\n",
      "Matched DataFrame shape: (947, 74)\n",
      "Matching for 1014\n",
      "Matched DataFrame shape: (1274, 74)\n",
      "Matching for 1015\n",
      "Matched DataFrame shape: (1489, 74)\n",
      "Matching for 1016\n",
      "Matched DataFrame shape: (1310, 74)\n",
      "Matching for 1017\n",
      "Matched DataFrame shape: (1036, 74)\n",
      "Matching for 1018\n",
      "Matched DataFrame shape: (1401, 74)\n",
      "Matching for 1019\n",
      "Matched DataFrame shape: (913, 74)\n",
      "Matching for 1020\n",
      "Matched DataFrame shape: (1054, 74)\n",
      "Matching for 1021\n",
      "Matched DataFrame shape: (1358, 74)\n",
      "Matching for 1022\n",
      "Matched DataFrame shape: (784, 74)\n",
      "Matching for 1023\n",
      "Matched DataFrame shape: (990, 74)\n",
      "Matching for 1024\n",
      "Matched DataFrame shape: (1162, 74)\n",
      "Matching for 1025\n",
      "Matched DataFrame shape: (900, 74)\n",
      "Matching for 1026\n",
      "Matched DataFrame shape: (986, 74)\n",
      "Matching for 1027\n",
      "Matched DataFrame shape: (1370, 74)\n",
      "Matching for 1028\n",
      "Matched DataFrame shape: (1030, 74)\n",
      "Matching for 1029\n",
      "Matched DataFrame shape: (1381, 74)\n",
      "Matching for 1030\n",
      "Matched DataFrame shape: (1075, 74)\n",
      "Matching for 1031\n",
      "Matched DataFrame shape: (1140, 74)\n",
      "Matching for 1032\n",
      "Matched DataFrame shape: (1226, 74)\n",
      "Matching for 1033\n",
      "Matched DataFrame shape: (1327, 74)\n",
      "Matching for 1034\n",
      "Matched DataFrame shape: (895, 74)\n",
      "Matching for 1035\n",
      "Matched DataFrame shape: (1025, 74)\n",
      "Matching for 1036\n",
      "Matched DataFrame shape: (1274, 74)\n",
      "Matching for 1037\n",
      "Matched DataFrame shape: (766, 74)\n",
      "Matching for 1038\n",
      "Matched DataFrame shape: (924, 74)\n",
      "Matching for 1039\n",
      "Matched DataFrame shape: (778, 74)\n",
      "Matching for 1040\n",
      "Matched DataFrame shape: (946, 74)\n",
      "Matching for 1041\n",
      "Matched DataFrame shape: (1017, 74)\n",
      "Matching for 1042\n",
      "Matched DataFrame shape: (1288, 74)\n",
      "Matching for 1043\n",
      "Matched DataFrame shape: (1128, 74)\n",
      "Matching for 1044\n",
      "Matched DataFrame shape: (1076, 74)\n",
      "Matching for 1045\n",
      "Matched DataFrame shape: (1016, 74)\n",
      "Matching for 1046\n",
      "Matched DataFrame shape: (1531, 74)\n",
      "Matching for 1047\n",
      "Matched DataFrame shape: (1043, 74)\n",
      "Matching for 1051\n",
      "Matched DataFrame shape: (1112, 74)\n",
      "Matching for 1052\n",
      "Matched DataFrame shape: (1026, 74)\n",
      "Matching for 1053\n",
      "Matched DataFrame shape: (1124, 74)\n",
      "Matching for 1054\n",
      "Matched DataFrame shape: (1244, 74)\n",
      "Matching for 1055\n",
      "Matched DataFrame shape: (1246, 74)\n",
      "Matching for 1056\n",
      "Matched DataFrame shape: (1418, 74)\n",
      "Matching for 1057\n",
      "Matched DataFrame shape: (941, 74)\n",
      "Matching for 1058\n",
      "Matched DataFrame shape: (1296, 74)\n",
      "Matching for 1059\n",
      "Matched DataFrame shape: (1051, 74)\n",
      "Matching for 1060\n",
      "Matched DataFrame shape: (1336, 74)\n",
      "Matching for 1061\n",
      "Matched DataFrame shape: (969, 74)\n",
      "Matching for 1062\n",
      "Matched DataFrame shape: (981, 74)\n",
      "Matching for 1063\n",
      "Matched DataFrame shape: (20, 74)\n",
      "Matching for 1064\n",
      "Matched DataFrame shape: (1361, 74)\n",
      "Matching for 1065\n",
      "Matched DataFrame shape: (889, 74)\n",
      "Matching for 1066\n",
      "Matched DataFrame shape: (1025, 74)\n",
      "Matching for 1067\n",
      "Matched DataFrame shape: (1147, 74)\n",
      "Matching for 1069\n",
      "Matched DataFrame shape: (1044, 74)\n",
      "Matching for 1070\n",
      "Matched DataFrame shape: (1259, 74)\n",
      "Matching for 1071\n",
      "Matched DataFrame shape: (1360, 74)\n",
      "Matching for 1072\n",
      "Matched DataFrame shape: (841, 74)\n",
      "Matching for 1073\n",
      "Matched DataFrame shape: (998, 74)\n",
      "Matching for 1074\n",
      "Matched DataFrame shape: (1123, 74)\n",
      "Matching for 1075\n",
      "Matched DataFrame shape: (816, 74)\n",
      "Matching for 1076\n",
      "Matched DataFrame shape: (1042, 74)\n",
      "Matching for 1077\n",
      "Matched DataFrame shape: (1131, 74)\n",
      "Matching for 1078\n",
      "Matched DataFrame shape: (1055, 74)\n",
      "Matching for 1079\n",
      "Matched DataFrame shape: (1230, 74)\n",
      "Matching for 1080\n",
      "Matched DataFrame shape: (1176, 74)\n",
      "Matching for 1081\n",
      "Matched DataFrame shape: (889, 74)\n",
      "Matching for 1082\n",
      "Matched DataFrame shape: (1262, 74)\n",
      "Matching for 1083\n",
      "Matched DataFrame shape: (811, 74)\n",
      "Matching for 1084\n",
      "Matched DataFrame shape: (1323, 74)\n",
      "Matching for 1085\n",
      "Matched DataFrame shape: (953, 74)\n",
      "Matching for 1086\n",
      "Matched DataFrame shape: (1382, 74)\n",
      "Matching for 1087\n",
      "Matched DataFrame shape: (790, 74)\n",
      "Matching for 1088\n",
      "Matched DataFrame shape: (1391, 74)\n",
      "Matching for 1089\n",
      "Matched DataFrame shape: (969, 74)\n",
      "Matching for 1091\n",
      "Matched DataFrame shape: (941, 74)\n",
      "Matching for 1092\n",
      "Matched DataFrame shape: (504, 74)\n",
      "Matching for 1093\n",
      "Matched DataFrame shape: (990, 74)\n",
      "Matching for 1094\n",
      "Matched DataFrame shape: (1378, 74)\n",
      "Matching for 1095\n",
      "Matched DataFrame shape: (1256, 74)\n",
      "Matching for 1096\n",
      "Matched DataFrame shape: (1117, 74)\n",
      "Matching for 1098\n",
      "Matched DataFrame shape: (1055, 74)\n",
      "Matching for 1099\n",
      "Matched DataFrame shape: (942, 74)\n",
      "Matching for 1100\n",
      "Matched DataFrame shape: (995, 74)\n",
      "Matching for 1101\n",
      "Matched DataFrame shape: (961, 74)\n",
      "Matching for 1102\n",
      "Matched DataFrame shape: (1138, 74)\n",
      "Matching for 1104\n",
      "Matched DataFrame shape: (1513, 74)\n",
      "Matching for 1105\n",
      "Matched DataFrame shape: (1312, 74)\n",
      "Matching for 1106\n",
      "Matched DataFrame shape: (1315, 74)\n",
      "Matching for 1107\n",
      "Matched DataFrame shape: (1353, 74)\n",
      "Matching for 1108\n",
      "Matched DataFrame shape: (970, 74)\n",
      "Matching for 1109\n",
      "Matched DataFrame shape: (1252, 74)\n",
      "Matching for 1110\n",
      "Matched DataFrame shape: (815, 74)\n",
      "Matching for 1111\n",
      "Matched DataFrame shape: (1341, 74)\n",
      "Matching for 1112\n",
      "Matched DataFrame shape: (955, 74)\n",
      "Matching for 1113\n",
      "Matched DataFrame shape: (1084, 74)\n",
      "Matching for 1114\n",
      "Matched DataFrame shape: (880, 74)\n",
      "Matching for 1115\n",
      "Matched DataFrame shape: (1005, 74)\n",
      "Matching for 1116\n",
      "Matched DataFrame shape: (1014, 74)\n",
      "Matching for 1117\n",
      "Matched DataFrame shape: (1280, 74)\n",
      "Matching for 1118\n",
      "Matched DataFrame shape: (1106, 74)\n",
      "Matching for 1119\n",
      "Matched DataFrame shape: (943, 74)\n",
      "Matching for 1120\n",
      "Matched DataFrame shape: (1157, 74)\n",
      "Matching for 1121\n",
      "Matching for 1122\n",
      "Matching for 1123\n",
      "Matching for 1124\n",
      "Matching for 1125\n",
      "Matching for 1126\n",
      "Matching for 1127\n",
      "Matching for 1128\n",
      "Matching for 1129\n",
      "Matching for 1130\n",
      "Matching for 1131\n",
      "Matching for 1132\n",
      "Matching for 1133\n",
      "Matching for 1134\n",
      "Matching for 1135\n",
      "Matching for 1136\n",
      "Matching for 1137\n",
      "Matching for 1138\n",
      "Matching for 1139\n",
      "Matching for 1140\n",
      "Matching for 1141\n",
      "(586171, 76)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing required columns: {'track_status'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 708\u001b[0m\n\u001b[1;32m    705\u001b[0m     save_model(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_prediction_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 708\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 674\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     enhanced_laps \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# Save the preprocessed laps DataFrame for inspection\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     enhanced_laps\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menhanced_laps_before_training.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[12], line 662\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    660\u001b[0m missing_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(required_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(laps\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# Drop rows with missing values in required columns\u001b[39;00m\n\u001b[1;32m    665\u001b[0m laps \u001b[38;5;241m=\u001b[39m laps\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39mrequired_columns)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing required columns: {'track_status'}"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Define the RaceFeatures dataclass\n",
    "@dataclass\n",
    "class RaceFeatures:\n",
    "    \"\"\"Data structure for race features\"\"\"\n",
    "    static_features: List[str] = field(default_factory=lambda: [\n",
    "        'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "        'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "        'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time'\n",
    "    ])\n",
    "    \n",
    "    dynamic_features: List[str] = field(default_factory=lambda: [\n",
    "        'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "        'air_temp', 'humidity', 'tire_compound', 'track_status', 'is_pit_lap'\n",
    "    ])\n",
    "    \n",
    "    target: str = 'milliseconds'\n",
    "\n",
    "# Define the F1Dataset class\n",
    "class F1Dataset(Dataset):\n",
    "    def __init__(self, sequences, static_features, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.static_features = torch.FloatTensor(static_features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.sequences[idx],\n",
    "            'static': self.static_features[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "\n",
    "# Define the F1DataPreprocessor class\n",
    "class F1DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.static_scaler = StandardScaler()\n",
    "        self.dynamic_scaler = StandardScaler()\n",
    "        self.lap_time_scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_sequence_data(self, df: pd.DataFrame, window_size: int = 3) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare sequential data with sliding window and apply scaling\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        static_features = []\n",
    "        targets = []\n",
    "        \n",
    "        # Instantiate RaceFeatures\n",
    "        race_features = RaceFeatures()\n",
    "        \n",
    "        # Sort the dataframe to ensure consistent ordering\n",
    "        df = df.sort_values(['raceId', 'driverId', 'lap'])\n",
    "        \n",
    "        # Group by race and driver\n",
    "        for (race_id, driver_id), group in df.groupby(['raceId', 'driverId']):\n",
    "            group = group.sort_values('lap')\n",
    "            \n",
    "            # Extract static features (assumed to be constant per driver per race)\n",
    "            static = group[race_features.static_features].iloc[0].values\n",
    "            static_features.append(static)\n",
    "            \n",
    "            # Extract dynamic features and target\n",
    "            lap_times = group[race_features.target].values.reshape(-1, 1)  # Shape: (num_laps, 1)\n",
    "            dynamic = group[race_features.dynamic_features].values  # Shape: (num_laps, num_dynamic_features)\n",
    "            \n",
    "            # Apply scaling\n",
    "            # Note: Scalers should be fitted on the training data to prevent data leakage.\n",
    "            # Here, for simplicity, we're fitting on the entire dataset. For a real-world scenario,\n",
    "            # consider splitting the data first before fitting the scalers.\n",
    "            dynamic_features_to_scale = [col for col in race_features.dynamic_features if col != 'tire_compound']\n",
    "            tire_compounds = dynamic[:, race_features.dynamic_features.index('tire_compound')].reshape(-1, 1)\n",
    "            other_dynamic = dynamic[:, [race_features.dynamic_features.index(col) for col in dynamic_features_to_scale]]\n",
    "            \n",
    "            lap_times_scaled = self.lap_time_scaler.fit_transform(lap_times).flatten()\n",
    "            other_dynamic_scaled = self.dynamic_scaler.fit_transform(other_dynamic)\n",
    "            static_scaled = self.static_scaler.fit_transform(static.reshape(1, -1)).flatten()\n",
    "            \n",
    "            dynamic_scaled = np.hstack((tire_compounds, other_dynamic_scaled))\n",
    "            \n",
    "            # Create sequences\n",
    "            # Create sequences\n",
    "        for i in range(len(lap_times_scaled) - window_size):\n",
    "            sequence_lap_times = lap_times_scaled[i:i+window_size].reshape(-1, 1)  # Shape: (window_size, 1)\n",
    "            sequence_dynamic = dynamic_scaled[i:i+window_size]  # Shape: (window_size, num_dynamic_features)\n",
    "            sequence = np.hstack((sequence_lap_times, sequence_dynamic))  # Shape: (window_size, 1 + num_dynamic_features)\n",
    "            sequences.append(sequence)\n",
    "            static_features.append(static_scaled)\n",
    "            targets.append(lap_times_scaled[i + window_size])\n",
    "        \n",
    "        return (np.array(sequences), \n",
    "                np.array(static_features), \n",
    "                np.array(targets))\n",
    "\n",
    "    \n",
    "    def create_train_val_loaders(\n",
    "        self, \n",
    "        sequences: np.ndarray, \n",
    "        static_features: np.ndarray, \n",
    "        targets: np.ndarray,\n",
    "        batch_size: int = 32,\n",
    "        val_split: float = 0.2\n",
    "    ) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create train and validation dataloaders with given split ratio\n",
    "        \"\"\"\n",
    "        dataset = F1Dataset(sequences, static_features, targets)\n",
    "        \n",
    "        # Calculate lengths for split\n",
    "        val_size = int(len(dataset) * val_split)\n",
    "        train_size = len(dataset) - val_size\n",
    "        \n",
    "        # Split dataset\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "# Define the F1PredictionModel class\n",
    "class F1PredictionModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 sequence_dim: int,\n",
    "                 static_dim: int,\n",
    "                 hidden_dim: int = 64,\n",
    "                 num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTM for sequential features\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=sequence_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Static features processing\n",
    "        self.static_network = nn.Sequential(\n",
    "            nn.Linear(static_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Combine everything\n",
    "        self.final_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence, static):\n",
    "        # Process sequence through LSTM\n",
    "        lstm_out, _ = self.lstm(sequence)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take the output of the last time step\n",
    "        \n",
    "        # Process static features\n",
    "        static_out = self.static_network(static)\n",
    "        \n",
    "        # Combine LSTM output and static features\n",
    "        combined = torch.cat([lstm_out, static_out], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.final_network(combined)\n",
    "        \n",
    "        return prediction.squeeze()\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                epochs: int = 10,\n",
    "                learning_rate: float = 0.001,\n",
    "                lap_time_scaler: StandardScaler = None,  # Pass the lap time scaler\n",
    "                device: Optional[str] = None) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train the model and return training history including MAE in milliseconds\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_maes = []\n",
    "        for batch in train_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            static = batch['static'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(sequences, static)\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Calculate MAE in normalized scale\n",
    "            mae = torch.mean(torch.abs(predictions - targets)).item()\n",
    "            train_maes.append(mae)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_maes = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch['sequence'].to(device)\n",
    "                static = batch['static'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                predictions = model(sequences, static)\n",
    "                loss = criterion(predictions, targets)\n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Calculate MAE in normalized scale\n",
    "                mae_normalized = torch.mean(torch.abs(predictions - targets)).item()\n",
    "                val_maes.append(mae_normalized)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        train_mae_normalized = np.mean(train_maes)\n",
    "        val_mae_normalized = np.mean(val_maes)\n",
    "        \n",
    "        # Convert MAE back to milliseconds using the inverse scaler\n",
    "        if lap_time_scaler:\n",
    "            train_mae_ms = lap_time_scaler.inverse_transform([[train_mae_normalized]])[0][0]\n",
    "            val_mae_ms = lap_time_scaler.inverse_transform([[val_mae_normalized]])[0][0]\n",
    "        else:\n",
    "            train_mae_ms, val_mae_ms = train_mae_normalized, val_mae_normalized\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mae'].append(train_mae_ms)\n",
    "        history['val_mae'].append(val_mae_ms)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train MAE: {train_mae_ms:.2f} ms, Val MAE: {val_mae_ms:.2f} ms')\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to save the model\n",
    "def save_model(model: nn.Module, path: str):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "# Now, integrate your code snippets into data preprocessing\n",
    "def load_and_preprocess_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from CSV files and preprocess it to create the enhanced_laps DataFrame.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    na_values = ['\\\\N', 'NaN', '']\n",
    "    lap_times = pd.read_csv('../../data/raw_data/lap_times.csv', na_values=na_values)\n",
    "    drivers = pd.read_csv('../../data/raw_data/drivers.csv', na_values=na_values)\n",
    "    races = pd.read_csv('../../data/raw_data/races.csv', na_values=na_values)\n",
    "    circuits = pd.read_csv('../../data/raw_data/circuits.csv', na_values=na_values)\n",
    "    pit_stops = pd.read_csv('../../data/raw_data/pit_stops.csv', na_values=na_values)\n",
    "    pit_stops.rename(columns={'milliseconds' : 'pitstop_milliseconds'}, inplace=True)\n",
    "    results = pd.read_csv('../../data/raw_data/results.csv', na_values=na_values)\n",
    "    results.rename(columns={'milliseconds' : 'racetime_milliseconds'}, inplace=True)\n",
    "\n",
    "    qualifying = pd.read_csv('../../data/raw_data/qualifying.csv', na_values=na_values)\n",
    "    status = pd.read_csv('../../data/raw_data/status.csv', na_values=na_values)\n",
    "    weather_data = pd.read_csv('../../data/raw_data/ff1_weather.csv', na_values=na_values)\n",
    "    practice_sessions = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "    # Load the tire data\n",
    "    tire_data = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    races['date'] = pd.to_datetime(races['date'])\n",
    "    results['date'] = results['raceId'].map(races.set_index('raceId')['date'])\n",
    "    lap_times['date'] = lap_times['raceId'].map(races.set_index('raceId')['date'])\n",
    "    \n",
    "    # Merge dataframes\n",
    "    laps = lap_times.merge(drivers, on='driverId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(races, on='raceId', how='left', suffixes=('', '_race'))\n",
    "    laps.rename(columns={'quali_time' : 'quali_date_time'}, inplace=True)\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(circuits, on='circuitId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(results[['raceId', 'driverId', 'positionOrder', 'grid', 'racetime_milliseconds', 'fastestLap', 'statusId']], on=['raceId', 'driverId'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(status, on='statusId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(pit_stops[['raceId', 'driverId', 'lap', 'pitstop_milliseconds']], on=['raceId', 'driverId', 'lap'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps['pitstop_milliseconds'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Add weather information\n",
    "    # Filter weather data to include only the Race session\n",
    "    weather_data = weather_data[weather_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge weather data with races to get raceId\n",
    "    weather_data = weather_data.merge(\n",
    "        races[['raceId', 'year', 'name']], \n",
    "        left_on=['EventName', 'Year'],\n",
    "        right_on=['name', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Compute cumulative time from the start of the race for each driver\n",
    "    laps.sort_values(['raceId', 'driverId', 'lap'], inplace=True)\n",
    "    laps['cumulative_milliseconds'] = laps.groupby(['raceId', 'driverId'])['milliseconds'].cumsum()\n",
    "    laps['seconds_from_start'] = laps['cumulative_milliseconds'] / 1000\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Use 'Time' in weather_data as 'seconds_from_start'\n",
    "    weather_data['seconds_from_start'] = weather_data['Time']\n",
    "    \n",
    "    # Standardize text data\n",
    "    tire_data['Compound'] = tire_data['Compound'].str.upper()\n",
    "    tire_data['EventName'] = tire_data['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Filter for race sessions only\n",
    "    tire_data = tire_data[tire_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge with races to get raceId\n",
    "    tire_data = tire_data.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    tire_data['Driver'] = tire_data['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    tire_data['driverId'] = tire_data['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Rename 'LapNumber' to 'lap' and ensure integer type\n",
    "    tire_data.rename(columns={'LapNumber': 'lap'}, inplace=True)\n",
    "    tire_data['lap'] = tire_data['lap'].astype(int)\n",
    "    laps['lap'] = laps['lap'].astype(int)\n",
    "    \n",
    "    # Create compound mapping (ordered from hardest to softest)\n",
    "    compound_mapping = {\n",
    "        'UNKNOWN': 0,\n",
    "        'HARD': 1,\n",
    "        'MEDIUM': 2,\n",
    "        'SOFT': 3,\n",
    "        'INTERMEDIATE': 4,\n",
    "        'WET': 5\n",
    "    }\n",
    "    \n",
    "    # Merge tire_data with laps\n",
    "    laps = laps.merge(\n",
    "        tire_data[['raceId', 'driverId', 'lap', 'Compound', 'TrackStatus']],\n",
    "        on=['raceId', 'driverId', 'lap'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    laps.rename(columns={'TrackStatus' : 'track_status'} )\n",
    "    \n",
    "    # Handle missing compounds and apply numeric encoding\n",
    "    laps['Compound'].fillna('UNKNOWN', inplace=True)\n",
    "    laps['tire_compound'] = laps['Compound'].map(compound_mapping)\n",
    "    \n",
    "    # Drop the original Compound column if desired\n",
    "    laps.drop('Compound', axis=1, inplace=True)\n",
    "    \n",
    "    # Standardize names\n",
    "    practice_sessions['EventName'] = practice_sessions['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Merge practice_sessions with races to get raceId\n",
    "    practice_sessions = practice_sessions.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    practice_sessions['Driver'] = practice_sessions['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    practice_sessions['driverId'] = practice_sessions['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Convert LapTime to milliseconds\n",
    "    practice_sessions['LapTime_ms'] = practice_sessions['LapTime'].apply(lambda x: pd.to_timedelta(x).total_seconds() * 1000)\n",
    "    \n",
    "    # Calculate median lap times for each driver in each session\n",
    "    session_medians = practice_sessions.groupby(['raceId', 'driverId', 'SessionName'])['LapTime_ms'].median().reset_index()\n",
    "    \n",
    "    # Pivot the data to have sessions as columns\n",
    "    session_medians_pivot = session_medians.pivot_table(\n",
    "        index=['raceId', 'driverId'],\n",
    "        columns='SessionName',\n",
    "        values='LapTime_ms'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    session_medians_pivot.rename(columns={\n",
    "        'FP1': 'fp1_median_time',\n",
    "        'FP2': 'fp2_median_time',\n",
    "        'FP3': 'fp3_median_time',\n",
    "        'Q': 'quali_time'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    laps = laps.merge(\n",
    "    session_medians_pivot,\n",
    "    on=['raceId', 'driverId'],\n",
    "    how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing practice times with global median or a placeholder value\n",
    "    global_median_fp1 = laps['fp1_median_time'].median()\n",
    "    laps['fp1_median_time'].fillna(global_median_fp1, inplace=True)\n",
    "    \n",
    "    # Repeat for other sessions\n",
    "    global_median_fp2 = laps['fp2_median_time'].median()\n",
    "    laps['fp2_median_time'].fillna(global_median_fp2, inplace=True)\n",
    "    \n",
    "    global_median_fp3 = laps['fp3_median_time'].median()\n",
    "    laps['fp3_median_time'].fillna(global_median_fp3, inplace=True)\n",
    "    \n",
    "    global_median_quali = laps['quali_time'].median()\n",
    "    laps['quali_time'].fillna(global_median_quali, inplace=True)\n",
    "\n",
    "    \n",
    "    # Create a binary indicator for pit stops\n",
    "    laps['is_pit_lap'] = laps['pitstop_milliseconds'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    \n",
    "    # Define a function to match weather data to laps\n",
    "    def match_weather_to_lap(race_laps, race_weather):\n",
    "        \"\"\"\n",
    "        For each lap, find the closest weather measurement in time\n",
    "        \"\"\"\n",
    "        race_laps = race_laps.sort_values('seconds_from_start')\n",
    "        race_weather = race_weather.sort_values('seconds_from_start')\n",
    "        merged = pd.merge_asof(\n",
    "            race_laps,\n",
    "            race_weather,\n",
    "            on='seconds_from_start',\n",
    "            direction='nearest'\n",
    "        )\n",
    "        return merged\n",
    "\n",
    "    # Apply matching per race\n",
    "    matched_laps_list = []\n",
    "    for race_id in laps['raceId'].unique():\n",
    "        print(f'Matching for {race_id}')\n",
    "        race_laps = laps[laps['raceId'] == race_id]\n",
    "        race_weather = weather_data[weather_data['raceId'] == race_id]\n",
    "        \n",
    "        if not race_weather.empty:\n",
    "            matched = match_weather_to_lap(race_laps, race_weather)\n",
    "            print(f\"Matched DataFrame shape: {matched.shape}\")\n",
    "            matched_laps_list.append(matched)\n",
    "        else:\n",
    "            matched_laps_list.append(race_laps)  # No weather data for this race\n",
    "\n",
    "    # Concatenate all matched laps\n",
    "    laps = pd.concat(matched_laps_list, ignore_index=True)\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Fill missing weather data with default values\n",
    "    laps['track_temp'] = laps['TrackTemp'].fillna(25.0)\n",
    "    laps['air_temp'] = laps['AirTemp'].fillna(20.0)\n",
    "    laps['humidity'] = laps['Humidity'].fillna(50.0)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    # Create driver names\n",
    "    drivers['driver_name'] = drivers['forename'] + ' ' + drivers['surname']\n",
    "    driver_mapping = drivers[['driverId', 'driver_name']].copy()\n",
    "    driver_mapping.set_index('driverId', inplace=True)\n",
    "    driver_names = driver_mapping['driver_name'].to_dict()\n",
    "    \n",
    "    # Map statusId to status descriptions\n",
    "    status_dict = status.set_index('statusId')['status'].to_dict()\n",
    "    results['status'] = results['statusId'].map(status_dict)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    def calculate_aggression(driver_results):\n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default aggression for new drivers\n",
    "        \n",
    "        # Only consider recent races for more current behavior\n",
    "        recent_results = driver_results.sort_values('date', ascending=False).head(20)\n",
    "        \n",
    "        # Calculate overtaking metrics\n",
    "        positions_gained = recent_results['grid'] - recent_results['positionOrder']\n",
    "        \n",
    "        # Calculate risk metrics\n",
    "        dnf_rate = (recent_results['status'] != 'Finished').mean()\n",
    "        incidents = (recent_results['statusId'].isin([\n",
    "            4,  # Collision\n",
    "            5,  # Spun off\n",
    "            6,  # Accident\n",
    "            20, # Collision damage\n",
    "            82, # Collision with another driver\n",
    "        ])).mean()\n",
    "        \n",
    "        # Calculate overtaking success rate (normalized between 0-1)\n",
    "        positive_overtakes = (positions_gained > 0).sum()\n",
    "        negative_overtakes = (positions_gained < 0).sum()\n",
    "        total_overtake_attempts = positive_overtakes + negative_overtakes\n",
    "        overtake_success_rate = positive_overtakes / total_overtake_attempts if total_overtake_attempts > 0 else 0.5\n",
    "        \n",
    "        # Normalize average positions gained (0-1)\n",
    "        avg_positions_gained = positions_gained[positions_gained > 0].mean() if len(positions_gained[positions_gained > 0]) > 0 else 0\n",
    "        max_possible_gain = 20  # Maximum grid positions that could be gained\n",
    "        normalized_gains = np.clip(avg_positions_gained / max_possible_gain, 0, 1)\n",
    "        \n",
    "        # Normalize risk factors (0-1)\n",
    "        normalized_dnf = np.clip(dnf_rate, 0, 1)\n",
    "        normalized_incidents = np.clip(incidents, 0, 1)\n",
    "        \n",
    "        # Calculate component scores (each between 0-1)\n",
    "        overtaking_component = (normalized_gains * 0.6 + overtake_success_rate * 0.4)\n",
    "        risk_component = (normalized_dnf * 0.5 + normalized_incidents * 0.5)\n",
    "        \n",
    "        # Combine components with weights (ensuring sum of weights = 1)\n",
    "        weights = {\n",
    "            'overtaking': 0.4,  # Aggressive overtaking\n",
    "            'risk': 0.5,       # Risk-taking behavior\n",
    "            'baseline': 0.1    # Baseline aggression\n",
    "        }\n",
    "        \n",
    "        aggression = (\n",
    "            overtaking_component * weights['overtaking'] +\n",
    "            risk_component * weights['risk'] +\n",
    "            0.5 * weights['baseline']  # Baseline aggression factor\n",
    "        )\n",
    "        \n",
    "        # Add small random variation while maintaining 0-1 bounds\n",
    "        variation = np.random.normal(0, 0.02)\n",
    "        aggression = np.clip(aggression + variation, 0, 1)\n",
    "        \n",
    "        return aggression\n",
    "    \n",
    "    def calculate_skill(driver_data, results_data, circuit_id):\n",
    "        driver_results = results_data[\n",
    "            (results_data['driverId'] == driver_data['driverId']) & \n",
    "            (results_data['circuitId'] == circuit_id)\n",
    "        ].sort_values('date', ascending=False).head(10)  # Use last 10 races at circuit\n",
    "        \n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default skill\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        avg_finish_pos = driver_results['positionOrder'].mean()\n",
    "        avg_quali_pos = driver_results['grid'].mean()\n",
    "        points_per_race = driver_results['points'].mean()\n",
    "        fastest_laps = (driver_results['rank'] == 1).mean()  # Add fastest lap consideration\n",
    "        \n",
    "        # Improved normalization (exponential decay for positions)\n",
    "        normalized_finish_pos = np.exp(-avg_finish_pos/5) # Better spread of values\n",
    "        normalized_quali_pos = np.exp(-avg_quali_pos/5)\n",
    "        \n",
    "        # Points normalization with improved scaling\n",
    "        max_points_per_race = 26  # Maximum possible points (25 + 1 fastest lap)\n",
    "        normalized_points = points_per_race / max_points_per_race\n",
    "        \n",
    "        # Weighted combination with more factors\n",
    "        weights = {\n",
    "            'finish': 0.35,\n",
    "            'quali': 0.25,\n",
    "            'points': 0.25,\n",
    "            'fastest_laps': 0.15\n",
    "        }\n",
    "        \n",
    "        skill = (\n",
    "            weights['finish'] * normalized_finish_pos +\n",
    "            weights['quali'] * normalized_quali_pos +\n",
    "            weights['points'] * normalized_points +\n",
    "            weights['fastest_laps'] * fastest_laps\n",
    "        )\n",
    "        \n",
    "        # Add random variation to prevent identical skills\n",
    "        skill = np.clip(skill + np.random.normal(0, 0.05), 0.1, 1.0)\n",
    "        \n",
    "        return skill\n",
    "    \n",
    "    # First merge results with races to get circuitId\n",
    "    results = results.merge(\n",
    "        races[['raceId', 'circuitId']], \n",
    "        on='raceId',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Now calculate driver aggression and skill\n",
    "    driver_aggression = {}\n",
    "    driver_skill = {}\n",
    "    for driver_id in drivers['driverId'].unique():\n",
    "        driver_results = results[results['driverId'] == driver_id]\n",
    "        aggression = calculate_aggression(driver_results)\n",
    "        driver_aggression[driver_id] = aggression\n",
    "        \n",
    "        # Now we have circuit_id from the merge\n",
    "        recent_race = driver_results.sort_values('date', ascending=False).head(1)\n",
    "        if not recent_race.empty:\n",
    "            circuit_id = recent_race['circuitId'].iloc[0]\n",
    "            skill = calculate_skill({'driverId': driver_id}, results, circuit_id)\n",
    "            driver_skill[driver_id] = skill\n",
    "        else:\n",
    "            driver_skill[driver_id] = 0.5  # Default skill for new drivers\n",
    "    \n",
    "    # Map calculated aggression and skill back to laps DataFrame\n",
    "    laps['driver_aggression'] = laps['driverId'].map(driver_aggression)\n",
    "    laps['driver_overall_skill'] = laps['driverId'].map(driver_skill)\n",
    "    laps['driver_circuit_skill'] = laps['driver_overall_skill']  # For simplicity, using overall skill\n",
    "    laps['driver_consistency'] = 0.5  # Placeholder\n",
    "    laps['driver_reliability'] = 0.5  # Placeholder\n",
    "    laps['driver_risk_taking'] = laps['driver_aggression']  # Assuming similar to aggression\n",
    "    \n",
    "    # Dynamic features\n",
    "    laps['tire_age'] = laps.groupby(['raceId', 'driverId'])['lap'].cumcount()\n",
    "    laps['fuel_load'] = laps.groupby(['raceId', 'driverId'])['lap'].transform(lambda x: x.max() - x + 1)\n",
    "    laps['track_position'] = laps['position']  # Assuming 'position' is available in laps data\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    # Create an instance of RaceFeatures\n",
    "    race_features = RaceFeatures()\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    required_columns = race_features.static_features + race_features.dynamic_features\n",
    "    missing_columns = set(required_columns) - set(laps.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Drop rows with missing values in required columns\n",
    "    laps = laps.dropna(subset=required_columns)\n",
    "    \n",
    "    print(laps.shape)\n",
    "    \n",
    "    return laps\n",
    "\n",
    "# Update the main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    enhanced_laps = load_and_preprocess_data()\n",
    "    \n",
    "    # Save the preprocessed laps DataFrame for inspection\n",
    "    enhanced_laps.to_csv('enhanced_laps_before_training.csv', index=False)\n",
    "    print(enhanced_laps.shape)\n",
    "    enhanced_laps.drop(columns=['position', 'time', 'driverRef', 'number', 'code', 'forename', 'surname', 'url_x', 'url_race', 'name_x', 'circuitRef', 'name_y', 'location', 'country', 'url_y', 'positionOrder', 'fastestLap', 'cumulative_milliseconds', 'seconds_from_start', 'raceId_x', 'year_x', 'Time', 'TrackTemp', 'AirTemp', 'Humidity', 'name', 'year_y', 'raceId_y'], inplace=True)\n",
    "    \n",
    "    print(\"Enhanced laps DataFrame saved to 'enhanced_laps_before_training.csv'\")\n",
    "    \n",
    "    preprocessor = F1DataPreprocessor()\n",
    "    sequences, static, targets = preprocessor.prepare_sequence_data(enhanced_laps, window_size=3)\n",
    "    \n",
    "    # Create train and validation loaders\n",
    "    train_loader, val_loader = preprocessor.create_train_val_loaders(\n",
    "        sequences, \n",
    "        static, \n",
    "        targets,\n",
    "        batch_size=32,\n",
    "        val_split=0.2\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = F1PredictionModel(\n",
    "        sequence_dim=sequences.shape[2],\n",
    "        static_dim=static.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = train_model(model, train_loader, val_loader, epochs=20, learning_rate=0.001)\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model(model, 'f1_prediction_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2272f8f0007d77fc",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b610d07e42b0b7",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378a276a6135c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceEffects:\n",
    "    \"\"\"Class to handle various race effects\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_tire_degradation(lap_number: int, stint_lap: int, compound: str = 'medium') -> float:\n",
    "        \"\"\"\n",
    "        Calculate tire degradation effect\n",
    "        stint_lap: Lap number since last pit stop\n",
    "        \"\"\"\n",
    "        # Tire degradation factors for different compounds\n",
    "        deg_factors = {\n",
    "            'soft': 0.015,\n",
    "            'medium': 0.010,\n",
    "            'hard': 0.007\n",
    "        }\n",
    "\n",
    "        base_deg = deg_factors.get(compound, 0.010)\n",
    "        return 1.0 + (base_deg * stint_lap)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_fuel_effect(fuel_load: float) -> float:\n",
    "        \"\"\"Calculate time penalty due to fuel load\"\"\"\n",
    "        # Approximate 0.3s per 10kg of fuel\n",
    "        return 1.0 + (fuel_load * 0.03)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_traffic_effect(position: int, gap_to_front: float) -> float:\n",
    "        \"\"\"Calculate effect of traffic\"\"\"\n",
    "        if gap_to_front < 1.5:  # Within DRS range\n",
    "            return 0.97  # 3% faster\n",
    "        elif gap_to_front < 0.5:  # In dirty air\n",
    "            return 1.03  # 3% slower\n",
    "        return 1.0\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_weather_effect(temp_change: float, rain_intensity: float = 0.0) -> float:\n",
    "        \"\"\"Calculate weather effects on lap time\"\"\"\n",
    "        # Temperature effect (optimal temp assumed to be 35°C)\n",
    "        temp_effect = 1.0 + abs(temp_change - 35.0) * 0.002\n",
    "\n",
    "        # Rain effect (0.0 to 1.0)\n",
    "        rain_effect = 1.0 + (rain_intensity * 0.2)\n",
    "\n",
    "        return temp_effect * rain_effect\n",
    "\n",
    "\n",
    "def visualize_race_simulation(simulation_results: Dict[str, List[float]], pit_stops: List[int] = None):\n",
    "    \"\"\"Visualize race simulation results\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    lap_numbers = list(range(1, len(simulation_results['predictions']) + 1))\n",
    "    lap_times = [t / 1000 for t in simulation_results['predictions']]  # Convert to seconds\n",
    "    uncertainties = simulation_results['uncertainties']\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Plot lap times\n",
    "    plt.plot(lap_numbers, lap_times, 'b-', label='Lap Times')\n",
    "\n",
    "    # Plot uncertainty bands\n",
    "    upper_bound = [t * (1 + u) for t, u in zip(lap_times, uncertainties)]\n",
    "    lower_bound = [t * (1 - u) for t, u in zip(lap_times, uncertainties)]\n",
    "    plt.fill_between(lap_numbers, lower_bound, upper_bound, alpha=0.2, color='blue')\n",
    "\n",
    "    # Mark pit stops\n",
    "    if pit_stops:\n",
    "        for pit_lap in pit_stops:\n",
    "            plt.axvline(x=pit_lap, color='r', linestyle='--', alpha=0.5)\n",
    "            plt.text(pit_lap, min(lap_times), 'Pit Stop', rotation=90)\n",
    "\n",
    "    plt.title('Race Simulation Results')\n",
    "    plt.xlabel('Lap Number')\n",
    "    plt.ylabel('Lap Time (seconds)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Update RacePredictionManager's simulation method\n",
    "def simulate_race(self,\n",
    "                  practice_data: pd.DataFrame,\n",
    "                  static_features: np.ndarray,\n",
    "                  n_laps: int,\n",
    "                  pit_stop_laps: List[int] = None,\n",
    "                  weather_forecast: Dict = None,\n",
    "                  traffic_scenario: Dict = None) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Enhanced race simulation with weather and traffic effects\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    uncertainties = []\n",
    "    current_stint_lap = 0\n",
    "    fuel_load = 100.0  # Initial fuel load in kg\n",
    "\n",
    "    # Initialize sequence with practice data\n",
    "    current_sequence = self._prepare_initial_sequence(practice_data)\n",
    "\n",
    "    for lap in range(n_laps):\n",
    "        # Update race conditions\n",
    "        current_stint_lap = 0 if lap in (pit_stop_laps or []) else current_stint_lap + 1\n",
    "        fuel_load = max(0, fuel_load - 2.0)  # Fuel consumption per lap\n",
    "\n",
    "        # Calculate combined effects\n",
    "        tire_effect = RaceEffects.calculate_tire_degradation(lap, current_stint_lap)\n",
    "        fuel_effect = RaceEffects.calculate_fuel_effect(fuel_load)\n",
    "\n",
    "        # Get weather effect if forecast provided\n",
    "        weather_effect = 1.0\n",
    "        if weather_forecast and lap in weather_forecast:\n",
    "            weather_effect = RaceEffects.calculate_weather_effect(\n",
    "                weather_forecast[lap]['temp'],\n",
    "                weather_forecast[lap].get('rain', 0.0)\n",
    "            )\n",
    "\n",
    "        # Get traffic effect if scenario provided\n",
    "        traffic_effect = 1.0\n",
    "        if traffic_scenario and lap in traffic_scenario:\n",
    "            traffic_effect = RaceEffects.calculate_traffic_effect(\n",
    "                traffic_scenario[lap]['position'],\n",
    "                traffic_scenario[lap]['gap_to_front']\n",
    "            )\n",
    "\n",
    "        # Make base prediction\n",
    "        with torch.no_grad():\n",
    "            base_prediction = self.model(\n",
    "                torch.FloatTensor(current_sequence).unsqueeze(0),\n",
    "                torch.FloatTensor(static_features).unsqueeze(0)\n",
    "            )\n",
    "\n",
    "        # Apply all effects\n",
    "        predicted_time = self._denormalize_prediction(base_prediction.item())\n",
    "        predicted_time *= (tire_effect * fuel_effect * weather_effect * traffic_effect)\n",
    "\n",
    "        predictions.append(predicted_time)\n",
    "        uncertainties.append(self._calculate_uncertainty(lap))\n",
    "\n",
    "        # Update sequence for next prediction\n",
    "        current_sequence = self._update_simulation_sequence(\n",
    "            current_sequence,\n",
    "            self.lap_time_scaler.transform([[predicted_time]])[0][0]\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'uncertainties': uncertainties\n",
    "    }\n",
    "\n",
    "\n",
    "class RacePredictionManager:\n",
    "    def __init__(self, model: F1PredictionModel, window_size: int = 3):\n",
    "        self.model = model\n",
    "        self.window_size = window_size\n",
    "        self.lap_buffer = []  # For storing recent laps in real-time mode\n",
    "        self.predictions = []  # Store all predictions\n",
    "        self.race_features = RaceFeatures()  # Initialize RaceFeatures\n",
    "\n",
    "        # Scalers from training\n",
    "        self.lap_time_scaler = None\n",
    "        self.dynamic_scaler = None\n",
    "        self.static_scaler = None\n",
    "\n",
    "    def set_scalers(self, lap_time_scaler, dynamic_scaler, static_scaler):\n",
    "        \"\"\"Set the scalers used during training\"\"\"\n",
    "        self.lap_time_scaler = lap_time_scaler\n",
    "        self.dynamic_scaler = dynamic_scaler\n",
    "        self.static_scaler = static_scaler\n",
    "\n",
    "    def real_time_predict(self,\n",
    "                          current_lap_data: dict,\n",
    "                          static_features: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Make real-time predictions during a race\n",
    "        \n",
    "        Args:\n",
    "            current_lap_data: Dict containing lap time and dynamic features\n",
    "            static_features: Array of static driver/circuit features\n",
    "        \"\"\"\n",
    "        # Update lap buffer\n",
    "        if len(self.lap_buffer) >= self.window_size:\n",
    "            self.lap_buffer.pop(0)\n",
    "        self.lap_buffer.append(current_lap_data)\n",
    "\n",
    "        # If we don't have enough laps yet, use a simple baseline\n",
    "        if len(self.lap_buffer) < self.window_size:\n",
    "            return self._baseline_prediction(current_lap_data)\n",
    "\n",
    "        # Prepare features\n",
    "        sequence = self._prepare_sequence(self.lap_buffer)\n",
    "        static = self._prepare_static(static_features)\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model(\n",
    "                torch.FloatTensor(sequence).unsqueeze(0),\n",
    "                torch.FloatTensor(static).unsqueeze(0)\n",
    "            )\n",
    "\n",
    "        # Convert prediction back to milliseconds\n",
    "        predicted_time = self._denormalize_prediction(prediction.item())\n",
    "        self.predictions.append(predicted_time)\n",
    "\n",
    "        return predicted_time\n",
    "\n",
    "    def _prepare_sequence(self, lap_buffer: List[dict]) -> np.ndarray:\n",
    "        \"\"\"Prepare sequence data for model input\"\"\"\n",
    "        sequence = []\n",
    "        for lap in lap_buffer:\n",
    "            lap_features = [\n",
    "                lap['milliseconds'],  # Normalized lap time\n",
    "                *[lap[feat] for feat in self.race_features.dynamic_features]\n",
    "            ]\n",
    "            sequence.append(lap_features)\n",
    "        return np.array(sequence)\n",
    "\n",
    "    def _prepare_static(self, static_features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Prepare static features for model input\"\"\"\n",
    "        return self.static_scaler.transform(static_features.reshape(1, -1)).flatten()\n",
    "\n",
    "    def _baseline_prediction(self, current_lap_data: dict) -> float:\n",
    "        \"\"\"Simple baseline prediction for cold start\"\"\"\n",
    "        return current_lap_data['milliseconds']\n",
    "\n",
    "    def _denormalize_prediction(self, prediction: float) -> float:\n",
    "        \"\"\"Convert normalized prediction back to milliseconds\"\"\"\n",
    "        return self.lap_time_scaler.inverse_transform([[prediction]])[0][0]\n",
    "\n",
    "    def simulate_race(self,\n",
    "                      practice_data: pd.DataFrame,\n",
    "                      static_features: np.ndarray,\n",
    "                      n_laps: int,\n",
    "                      pit_stop_laps: List[int] = None) -> Dict[str, List[float]]:\n",
    "        \"\"\"\n",
    "        Simulate an entire race\n",
    "        \n",
    "        Args:\n",
    "            practice_data: DataFrame containing practice session data\n",
    "            static_features: Array of static driver/circuit features\n",
    "            n_laps: Number of laps to simulate\n",
    "            pit_stop_laps: List of planned pit stop laps\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        uncertainties = []\n",
    "\n",
    "        # Initialize sequence with practice data\n",
    "        initial_sequence = self._prepare_initial_sequence(practice_data)\n",
    "        current_sequence = initial_sequence\n",
    "\n",
    "        for lap in range(n_laps):\n",
    "            # Adjust features for pit stops\n",
    "            if pit_stop_laps and lap in pit_stop_laps:\n",
    "                current_sequence = self._adjust_for_pit_stop(current_sequence)\n",
    "\n",
    "            # Make prediction\n",
    "            with torch.no_grad():\n",
    "                prediction = self.model(\n",
    "                    torch.FloatTensor(current_sequence).unsqueeze(0),\n",
    "                    torch.FloatTensor(static_features).unsqueeze(0)\n",
    "                )\n",
    "\n",
    "            # Add uncertainty based on prediction horizon\n",
    "            uncertainty = self._calculate_uncertainty(lap)\n",
    "\n",
    "            # Convert prediction to milliseconds\n",
    "            predicted_time = self._denormalize_prediction(prediction.item())\n",
    "\n",
    "            predictions.append(predicted_time)\n",
    "            uncertainties.append(uncertainty)\n",
    "\n",
    "            # Update sequence for next prediction\n",
    "            current_sequence = self._update_simulation_sequence(\n",
    "                current_sequence, prediction.item()\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            'predictions': predictions,\n",
    "            'uncertainties': uncertainties\n",
    "        }\n",
    "\n",
    "    def _prepare_initial_sequence(self, practice_data: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Prepare initial sequence from practice data\"\"\"\n",
    "        if len(practice_data) < self.window_size:\n",
    "            raise ValueError(f\"Practice data must contain at least {self.window_size} laps\")\n",
    "\n",
    "        sequence = []\n",
    "        for _, lap in practice_data.head(self.window_size).iterrows():\n",
    "            lap_features = [\n",
    "                lap['milliseconds'],  # Normalized lap time\n",
    "                *[lap[feat] for feat in self.race_features.dynamic_features]\n",
    "            ]\n",
    "            sequence.append(lap_features)\n",
    "        return np.array(sequence)\n",
    "\n",
    "    def _adjust_for_pit_stop(self, sequence: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Adjust sequence features for pit stop\"\"\"\n",
    "        sequence_copy = sequence.copy()\n",
    "        sequence_copy[-1, 1] = 0  # Reset tire age\n",
    "        sequence_copy[-1, 0] += 20000  # Add 20 seconds in milliseconds\n",
    "        return sequence_copy\n",
    "\n",
    "    def _update_simulation_sequence(self,\n",
    "                                    sequence: np.ndarray,\n",
    "                                    new_prediction: float) -> np.ndarray:\n",
    "        \"\"\"Update sequence with new prediction for simulation\"\"\"\n",
    "        new_sequence = sequence.copy()\n",
    "        new_sequence = np.roll(new_sequence, -1, axis=0)\n",
    "        new_sequence[-1, 0] = new_prediction\n",
    "\n",
    "        # Update dynamic features\n",
    "        new_sequence[-1, 1:] = self._update_dynamic_features(new_sequence[-2, 1:])\n",
    "        return new_sequence\n",
    "\n",
    "    def _update_dynamic_features(self, previous_features: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Update dynamic features for next lap\"\"\"\n",
    "        updated_features = previous_features.copy()\n",
    "        updated_features[0] += 1  # Increment tire age\n",
    "        updated_features[1] -= 1  # Decrease fuel load\n",
    "        return updated_features\n",
    "\n",
    "    def _calculate_uncertainty(self, lap_number: int) -> float:\n",
    "        \"\"\"Calculate prediction uncertainty based on lap number\"\"\"\n",
    "        base_uncertainty = 0.01  # 1% base uncertainty\n",
    "        horizon_factor = 1 + (lap_number * 0.001)  # Increase by 0.1% per lap\n",
    "        return base_uncertainty * horizon_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a37b2c32e92c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "# Update RacePredictionManager's simulation method\n",
    "def simulate_race(self,\n",
    "                  practice_data: pd.DataFrame,\n",
    "                  static_features: np.ndarray,\n",
    "                  n_laps: int,\n",
    "                  pit_stop_laps: List[int] = None,\n",
    "                  weather_forecast: Dict = None,\n",
    "                  traffic_scenario: Dict = None) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Enhanced race simulation with weather and traffic effects\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    uncertainties = []\n",
    "    current_stint_lap = 0\n",
    "    fuel_load = 100.0  # Initial fuel load in kg\n",
    "\n",
    "    # Initialize sequence with practice data\n",
    "    current_sequence = self._prepare_initial_sequence(practice_data)\n",
    "\n",
    "    for lap in range(n_laps):\n",
    "        # Update race conditions\n",
    "        current_stint_lap = 0 if lap in (pit_stop_laps or []) else current_stint_lap + 1\n",
    "        fuel_load = max(0, fuel_load - 2.0)  # Fuel consumption per lap\n",
    "\n",
    "        # Calculate combined effects\n",
    "        tire_effect = RaceEffects.calculate_tire_degradation(lap, current_stint_lap)\n",
    "        fuel_effect = RaceEffects.calculate_fuel_effect(fuel_load)\n",
    "\n",
    "        # Get weather effect if forecast provided\n",
    "        weather_effect = 1.0\n",
    "        if weather_forecast and lap in weather_forecast:\n",
    "            weather_effect = RaceEffects.calculate_weather_effect(\n",
    "                weather_forecast[lap]['temp'],\n",
    "                weather_forecast[lap].get('rain', 0.0)\n",
    "            )\n",
    "\n",
    "        # Get traffic effect if scenario provided\n",
    "        traffic_effect = 1.0\n",
    "        if traffic_scenario and lap in traffic_scenario:\n",
    "            traffic_effect = RaceEffects.calculate_traffic_effect(\n",
    "                traffic_scenario[lap]['position'],\n",
    "                traffic_scenario[lap]['gap_to_front']\n",
    "            )\n",
    "\n",
    "        # Make base prediction\n",
    "        with torch.no_grad():\n",
    "            base_prediction = self.model(\n",
    "                torch.FloatTensor(current_sequence).unsqueeze(0),\n",
    "                torch.FloatTensor(static_features).unsqueeze(0)\n",
    "            )\n",
    "\n",
    "        # Apply all effects\n",
    "        predicted_time = self._denormalize_prediction(base_prediction.item())\n",
    "        predicted_time *= (tire_effect * fuel_effect * weather_effect * traffic_effect)\n",
    "\n",
    "        predictions.append(predicted_time)\n",
    "        uncertainties.append(self._calculate_uncertainty(lap))\n",
    "\n",
    "        # Update sequence for next prediction\n",
    "        current_sequence = self._update_simulation_sequence(\n",
    "            current_sequence,\n",
    "            self.lap_time_scaler.transform([[predicted_time]])[0][0]\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'uncertainties': uncertainties\n",
    "    }\n",
    "\n",
    "\n",
    "def predict_race():\n",
    "    # Load the trained model and scalers\n",
    "    model, scalers = load_model_with_scalers('f1_prediction_model_with_scalers.pth')\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Initialize race manager\n",
    "    race_manager = RacePredictionManager(model, window_size=3)\n",
    "    race_manager.set_scalers(\n",
    "        scalers['lap_time_scaler'],\n",
    "        scalers['dynamic_scaler'],\n",
    "        scalers['static_scaler']\n",
    "    )\n",
    "\n",
    "    # Example static features for a driver\n",
    "    driver_static_features = np.array([\n",
    "        0.8,  # driver_overall_skill\n",
    "        0.75,  # driver_circuit_skill\n",
    "        0.85,  # driver_consistency\n",
    "        0.9,  # driver_reliability\n",
    "        0.7,  # driver_aggression\n",
    "        0.65  # driver_risk_taking\n",
    "    ])\n",
    "\n",
    "    # 1. Real-time prediction example\n",
    "    print(\"Real-time prediction example:\")\n",
    "    for lap in range(1, 6):  # Simulate first 5 laps\n",
    "        # Simulate current lap data\n",
    "        current_lap_data = {\n",
    "            'milliseconds': 80000 + np.random.normal(0, 500),  # ~80 seconds with some variation\n",
    "            'tire_age': lap,\n",
    "            'fuel_load': 100 - (lap * 2),  # Decreasing fuel load\n",
    "            'track_position': 5,  # Example position\n",
    "            'track_temp': 35.0,\n",
    "            'air_temp': 25.0,\n",
    "            'humidity': 60.0\n",
    "        }\n",
    "\n",
    "        prediction = race_manager.real_time_predict(\n",
    "            current_lap_data=current_lap_data,\n",
    "            static_features=driver_static_features\n",
    "        )\n",
    "\n",
    "        print(f\"Lap {lap} - Predicted time: {prediction / 1000:.3f} seconds\")\n",
    "\n",
    "    # 2. Race simulation example\n",
    "    print(\"\\nRace simulation example:\")\n",
    "\n",
    "    # Create example practice session data\n",
    "    practice_data = pd.DataFrame({\n",
    "        'milliseconds': [80500, 80300, 80100],  # Example lap times\n",
    "        'tire_age': [1, 2, 3],\n",
    "        'fuel_load': [98, 96, 94],\n",
    "        'track_position': [5, 5, 5],\n",
    "        'track_temp': [35.0, 35.0, 35.0],\n",
    "        'air_temp': [25.0, 25.0, 25.0],\n",
    "        'humidity': [60.0, 60.0, 60.0]\n",
    "    })\n",
    "\n",
    "    # Add weather and traffic scenarios\n",
    "    weather_forecast = {\n",
    "        10: {'temp': 40.0, 'rain': 0.0},  # Hot weather at lap 10\n",
    "        20: {'temp': 38.0, 'rain': 0.3},  # Light rain at lap 20\n",
    "    }\n",
    "\n",
    "    traffic_scenario = {\n",
    "        5: {'position': 2, 'gap_to_front': 1.2},  # DRS range\n",
    "        15: {'position': 4, 'gap_to_front': 0.3},  # Dirty air\n",
    "    }\n",
    "\n",
    "    # Update race simulation call\n",
    "    race_simulation = race_manager.simulate_race(\n",
    "        practice_data=practice_data,\n",
    "        static_features=driver_static_features,\n",
    "        n_laps=50,\n",
    "        pit_stop_laps=[15, 35],\n",
    "        weather_forecast=weather_forecast,\n",
    "        traffic_scenario=traffic_scenario\n",
    "    )\n",
    "\n",
    "    # Print simulation results\n",
    "    print(\"\\nRace simulation results:\")\n",
    "    print(f\"Total laps simulated: {len(race_simulation['predictions'])}\")\n",
    "    print(f\"Average lap time: {np.mean(race_simulation['predictions']) / 1000:.3f} seconds\")\n",
    "    print(f\"Fastest lap: {min(race_simulation['predictions']) / 1000:.3f} seconds\")\n",
    "    print(f\"Slowest lap: {max(race_simulation['predictions']) / 1000:.3f} seconds\")\n",
    "\n",
    "    # Print lap times around pit stops\n",
    "    pit_stops = [15, 35]\n",
    "    for pit_lap in pit_stops:\n",
    "        print(f\"\\nLap times around pit stop at lap {pit_lap}:\")\n",
    "        start_lap = 0\n",
    "        end_lap = len(race_simulation['predictions'])\n",
    "        for lap in range(start_lap, end_lap):\n",
    "            print(f\"Lap {lap + 1}: {race_simulation['predictions'][lap] / 1000:.3f} seconds \"\n",
    "                  f\"(Uncertainty: ±{race_simulation['uncertainties'][lap] * 100:.1f}%)\")\n",
    "\n",
    "    visualize_race_simulation(race_simulation, pit_stop_laps=[15, 35])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_race()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6dffef5d4e7623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
