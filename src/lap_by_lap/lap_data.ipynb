{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# F1 Lap Times Data Processing and Feature Engineering\n",
    "\n",
    "This notebook loads raw Formula 1 data, preprocesses it, and engineers new features related to driver performance attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Define NA Values and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('Data Loaded Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Inspect Lap Times Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_times.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Merge Lap Times with Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge lap times with preprocessed data on 'raceId' and 'driverId'\n",
    "data = lap_times.merge(preprocessed, on=['raceId', 'driverId'], how='left')\n",
    "\n",
    "# Drop the 'time' column and rename 'milliseconds' to 'lap_time'\n",
    "if 'time' in data.columns:\n",
    "    data.drop(columns=['time'], inplace=True)\n",
    "if 'milliseconds' in data.columns:\n",
    "    data.rename(columns={'milliseconds': 'lap_time'}, inplace=True)\n",
    "else:\n",
    "    print('Column \"milliseconds\" not found in merged data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Define Functions and Classes for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lap_data(lap_times_df: pd.DataFrame, races_df: pd.DataFrame, results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare lap times data by merging with races and results data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lap_times_df: DataFrame containing lap times\n",
    "    races_df: DataFrame containing race information\n",
    "    results_df: DataFrame containing race results\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with merged lap times, race dates, and circuit information\n",
    "    \"\"\"\n",
    "    # Get required columns from races\n",
    "    race_info = races_df[['raceId', 'date', 'year', 'circuitId']].copy()\n",
    "    \n",
    "    # Merge lap times with race information\n",
    "    enhanced_laps = lap_times_df.merge(\n",
    "        race_info,\n",
    "        on='raceId',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    enhanced_laps['date'] = pd.to_datetime(enhanced_laps['date'])\n",
    "    \n",
    "    return enhanced_laps\n",
    "\n",
    "class LapAttributeCalculator:\n",
    "    def __init__(self, lap_times_df: pd.DataFrame, races_df: pd.DataFrame, results_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Initialize with required dataframes.\n",
    "        \"\"\"\n",
    "        # Merge results with races to include 'date'\n",
    "        if 'date' not in results_df.columns:\n",
    "            print('Merging results with races to include \"date\" column.')\n",
    "            results_df = results_df.merge(races_df[['raceId', 'date']], on='raceId', how='left')\n",
    "            if 'date' not in results_df.columns:\n",
    "                raise KeyError('After merging, \"date\" column is still missing in results_df.')\n",
    "        else:\n",
    "            results_df['date'] = pd.to_datetime(results_df['date'])\n",
    "        \n",
    "        self.results = results_df.copy()\n",
    "        self.base_data = prepare_lap_data(lap_times_df, races_df, results_df)\n",
    "        \n",
    "    def add_driver_attributes(self, n_previous_races: int = 20) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add driver performance attributes to each lap.\n",
    "        \"\"\"\n",
    "        enhanced_laps = self.base_data.copy()\n",
    "        print(f\"Starting with {len(enhanced_laps)} laps\")\n",
    "        \n",
    "        # Ensure 'driverId' is present in base_data\n",
    "        if 'driverId' not in enhanced_laps.columns:\n",
    "            raise KeyError('driverId column is missing from the base data.')\n",
    "            \n",
    "        # Process each unique driver-race combination\n",
    "        unique_combinations = enhanced_laps[['driverId', 'raceId', 'date', 'circuitId']].drop_duplicates()\n",
    "        print(f\"Processing {len(unique_combinations)} unique driver-race combinations\")\n",
    "        \n",
    "        attributes_list = []\n",
    "        for _, combo in unique_combinations.iterrows():\n",
    "            # Get previous results for this driver up to this race\n",
    "            previous_results = self.results[\n",
    "                (self.results['driverId'] == combo['driverId']) &\n",
    "                (self.results['date'] < combo['date'])\n",
    "            ].sort_values('date', ascending=False).head(n_previous_races)\n",
    "            \n",
    "            # Calculate attributes\n",
    "            attributes = {\n",
    "                'raceId': combo['raceId'],\n",
    "                'driverId': combo['driverId'],\n",
    "                **self._calculate_driver_attributes(\n",
    "                    previous_results,\n",
    "                    combo['circuitId'],\n",
    "                    combo['date']\n",
    "                )\n",
    "            }\n",
    "            attributes_list.append(attributes)\n",
    "        \n",
    "        # Convert to DataFrame and merge with laps\n",
    "        print(\"Creating attributes DataFrame...\")\n",
    "        attributes_df = pd.DataFrame(attributes_list)\n",
    "        \n",
    "        # Merge attributes with original lap data\n",
    "        print(\"Merging attributes with lap data...\")\n",
    "        enhanced_laps = enhanced_laps.merge(\n",
    "            attributes_df,\n",
    "            on=['raceId', 'driverId'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        print(f\"Final dataframe has {len(enhanced_laps)} rows and {len(enhanced_laps.columns)} columns\")\n",
    "        return enhanced_laps\n",
    "    \n",
    "    def _calculate_driver_attributes(self, previous_results: pd.DataFrame, \n",
    "                                   circuit_id: int, race_date: pd.Timestamp) -> Dict[str, float]:\n",
    "        \"\"\"Calculate all driver attributes based on previous races.\"\"\"\n",
    "        if previous_results.empty:\n",
    "            return self._get_default_attributes()\n",
    "        \n",
    "        # Calculate attributes\n",
    "        circuit_results = previous_results[previous_results['circuitId'] == circuit_id]\n",
    "        \n",
    "        attributes = {\n",
    "            # Skill metrics\n",
    "            'driver_overall_skill': self._calculate_skill(previous_results),\n",
    "            'driver_circuit_skill': self._calculate_skill(circuit_results) if not circuit_results.empty else 0.5,\n",
    "            \n",
    "            # Race performance metrics\n",
    "            'driver_consistency': self._calculate_consistency(previous_results),\n",
    "            'driver_reliability': self._calculate_reliability(previous_results),\n",
    "            'driver_aggression': self._calculate_aggression(previous_results),\n",
    "            'driver_risk_taking': self._calculate_risk_taking(previous_results),\n",
    "            \n",
    "            # Position and points metrics\n",
    "            'avg_finish_position': previous_results['positionOrder'].mean(),\n",
    "            'avg_grid_position': previous_results['grid'].mean(),\n",
    "            'points_per_race': previous_results['points'].mean(),\n",
    "            \n",
    "            # Overtaking metrics\n",
    "            **self._calculate_overtaking_metrics(previous_results),\n",
    "            \n",
    "            # DNF and completion metrics\n",
    "            **self._calculate_completion_metrics(previous_results),\n",
    "            \n",
    "            # Circuit specific performance\n",
    "            **self._calculate_circuit_metrics(circuit_results)\n",
    "        }\n",
    "        \n",
    "        return attributes\n",
    "    \n",
    "    def _calculate_overtaking_metrics(self, results: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calculate overtaking-related metrics.\"\"\"\n",
    "        if results.empty:\n",
    "            return {'overtakes_per_race': 0.5, 'overtake_success_rate': 0.5}\n",
    "            \n",
    "        positions_gained = results['grid'] - results['positionOrder']\n",
    "        positive_overtakes = (positions_gained > 0).sum()\n",
    "        total_overtakes = len(positions_gained[positions_gained != 0])\n",
    "        \n",
    "        return {\n",
    "            'overtakes_per_race': positions_gained.mean(),\n",
    "            'overtake_success_rate': positive_overtakes / total_overtakes if total_overtakes > 0 else 0.5\n",
    "        }\n",
    "    \n",
    "    def _calculate_reliability(self, driver_results: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate driver reliability score.\"\"\"\n",
    "        if driver_results.empty:\n",
    "            return 0.5\n",
    "            \n",
    "        # Calculate finish rate\n",
    "        finish_rate = (driver_results['statusId'] == 1).mean()  # Status 1 = Finished\n",
    "        \n",
    "        # Calculate mechanical failure rate\n",
    "        mechanical_status_ids = [2, 3, 4, 5, 6]  # Mechanical failures\n",
    "        mechanical_failure_rate = (driver_results['statusId'].isin(mechanical_status_ids)).mean()\n",
    "        \n",
    "        # Combine metrics\n",
    "        reliability = (finish_rate * 0.7 + (1 - mechanical_failure_rate) * 0.3)\n",
    "        \n",
    "        return np.clip(reliability, 0.5, 1.0)\n",
    "    \n",
    "    def _calculate_completion_metrics(self, results: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calculate race completion metrics.\"\"\"\n",
    "        if results.empty:\n",
    "            return {'race_completion_rate': 0.5, 'dnf_rate': 0.5}\n",
    "            \n",
    "        completion_rate = (results['statusId'] == 1).mean()\n",
    "        dnf_rate = (results['statusId'] != 1).mean()\n",
    "        \n",
    "        return {\n",
    "            'race_completion_rate': completion_rate,\n",
    "            'dnf_rate': dnf_rate\n",
    "        }\n",
    "    \n",
    "    def _calculate_circuit_metrics(self, circuit_results: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calculate circuit-specific performance metrics.\"\"\"\n",
    "        if circuit_results.empty:\n",
    "            return {\n",
    "                'circuit_avg_position': 10.0,\n",
    "                'circuit_points_average': 0.0,\n",
    "                'circuit_completion_rate': 0.5\n",
    "            }\n",
    "            \n",
    "        return {\n",
    "            'circuit_avg_position': circuit_results['positionOrder'].mean(),\n",
    "            'circuit_points_average': circuit_results['points'].mean(),\n",
    "            'circuit_completion_rate': (circuit_results['statusId'] == 1).mean()\n",
    "        }\n",
    "    \n",
    "    def _calculate_risk_taking(self, driver_results: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate driver risk-taking score.\"\"\"\n",
    "        if driver_results.empty:\n",
    "            return 0.5\n",
    "            \n",
    "        # Calculate various risk metrics\n",
    "        positions_gained = driver_results['grid'] - driver_results['positionOrder']\n",
    "        big_gains = (positions_gained > 5).mean()  # Significant position improvements\n",
    "        incident_rate = (driver_results['statusId'].isin([4, 5, 6, 20, 82])).mean()\n",
    "        \n",
    "        # Combine metrics\n",
    "        risk_score = (big_gains * 0.6 + incident_rate * 0.4)\n",
    "        \n",
    "        return np.clip(risk_score, 0, 1)\n",
    "    \n",
    "    def _calculate_consistency(self, driver_results: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate driver consistency score.\"\"\"\n",
    "        if driver_results.empty:\n",
    "            return 0.5\n",
    "            \n",
    "        # Calculate position variance\n",
    "        pos_std = driver_results['positionOrder'].std()\n",
    "        normalized_std = np.exp(-pos_std/5)  # Lower std = higher consistency\n",
    "        \n",
    "        # Calculate finish rate in points\n",
    "        points_finish_rate = (driver_results['points'] > 0).mean()\n",
    "        \n",
    "        # Combine metrics\n",
    "        consistency = (normalized_std * 0.6 + points_finish_rate * 0.4)\n",
    "        \n",
    "        return np.clip(consistency, 0, 1)\n",
    "    \n",
    "    def _calculate_aggression(self, driver_results: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate driver aggression score based on overtaking and incidents.\"\"\"\n",
    "        if driver_results.empty:\n",
    "            return 0.5\n",
    "            \n",
    "        # Calculate overtaking metrics\n",
    "        positions_gained = driver_results['grid'] - driver_results['positionOrder']\n",
    "        positive_overtakes = (positions_gained > 0).sum()\n",
    "        negative_overtakes = (positions_gained < 0).sum()\n",
    "        total_overtake_attempts = positive_overtakes + negative_overtakes\n",
    "        \n",
    "        # Calculate incident rates\n",
    "        incident_status_ids = [4, 5, 6, 20, 82]  # Collisions, accidents, etc.\n",
    "        incident_rate = (driver_results['statusId'].isin(incident_status_ids)).mean()\n",
    "        \n",
    "        # Calculate components\n",
    "        overtake_success = positive_overtakes / total_overtake_attempts if total_overtake_attempts > 0 else 0.5\n",
    "        avg_positions_gained = positions_gained[positions_gained > 0].mean() if (positions_gained > 0).any() else 0\n",
    "        \n",
    "        # Normalize and combine\n",
    "        normalized_gains = np.clip(avg_positions_gained / 20, 0, 1)  # 20 as max possible positions gained\n",
    "        \n",
    "        # Weight the components\n",
    "        aggression = (\n",
    "            normalized_gains * 0.4 +\n",
    "            overtake_success * 0.3 +\n",
    "            incident_rate * 0.3\n",
    "        )\n",
    "        \n",
    "        return np.clip(aggression, 0, 1)\n",
    "    \n",
    "    def _get_default_attributes(self) -> Dict[str, float]:\n",
    "        \"\"\"Return default attributes for drivers with no previous races.\"\"\"\n",
    "        return {\n",
    "            'driver_overall_skill': 0.5,\n",
    "            'driver_circuit_skill': 0.5,\n",
    "            'driver_consistency': 0.5,\n",
    "            'driver_reliability': 0.5,\n",
    "            'driver_aggression': 0.5,\n",
    "            'driver_risk_taking': 0.5,\n",
    "            'avg_finish_position': 10.0,\n",
    "            'avg_grid_position': 10.0,\n",
    "            'points_per_race': 0.0,\n",
    "            'overtakes_per_race': 0.0,\n",
    "            'overtake_success_rate': 0.5,\n",
    "            'race_completion_rate': 0.5,\n",
    "            'dnf_rate': 0.5,\n",
    "            'circuit_avg_position': 10.0,\n",
    "            'circuit_points_average': 0.0,\n",
    "            'circuit_completion_rate': 0.5\n",
    "        }\n",
    "    \n",
    "    def _calculate_skill(self, results: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculate driver skill score.\"\"\"\n",
    "        if results.empty:\n",
    "            return 0.5\n",
    "            \n",
    "        avg_finish_pos = results['positionOrder'].mean()\n",
    "        avg_quali_pos = results['grid'].mean()\n",
    "        points_per_race = results['points'].mean()\n",
    "        \n",
    "        # Normalize metrics\n",
    "        norm_finish = np.exp(-avg_finish_pos/5)\n",
    "        norm_quali = np.exp(-avg_quali_pos/5)\n",
    "        norm_points = points_per_race / 26  # Assuming 26 is the maximum points per race\n",
    "        \n",
    "        skill = (norm_finish * 0.4 + norm_quali * 0.3 + norm_points * 0.3)\n",
    "        return np.clip(skill, 0.1, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Initialize Calculator and Add Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge results with races to include 'date'\n",
    "if 'date' not in results.columns:\n",
    "    print('Merging results with races to include \"date\" column.')\n",
    "    results = results.merge(races[['raceId', 'date', 'circuitId']], on='raceId', how='left')\n",
    "    if 'date' not in results.columns:\n",
    "        raise KeyError('After merging, \"date\" column is still missing in results.')\n",
    "    if 'circuitId' not in results.columns:\n",
    "         raise KeyError('After merging, \"circuitId\" column is still missing in results.')\n",
    "else:\n",
    "    results['date'] = pd.to_datetime(results['date'])\n",
    "\n",
    "# Verify that 'date' column is present\n",
    "print('Columns in results after merge:', results.columns.tolist())\n",
    "\n",
    "# Initialize calculator\n",
    "calculator = LapAttributeCalculator(\n",
    "    lap_times_df=lap_times,\n",
    "    races_df=races,\n",
    "    results_df=results\n",
    ")\n",
    "\n",
    "# Add attributes to lap times\n",
    "enhanced_laps = calculator.add_driver_attributes(n_previous_races=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## View New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify new feature columns\n",
    "new_features = [col for col in enhanced_laps.columns if col not in lap_times.columns]\n",
    "print(\"New features added:\")\n",
    "print(new_features)\n",
    "\n",
    "# Basic statistics of new features\n",
    "print(\"\\nNew features statistics:\")\n",
    "print(enhanced_laps[new_features].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_laps.drop(columns=['position', 'time'], axis=1, inplace=True)\n",
    "enhanced_laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_laps.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Phase 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Define the RaceFeatures dataclass\n",
    "@dataclass\n",
    "class RaceFeatures:\n",
    "    \"\"\"Data structure for race features\"\"\"\n",
    "    static_features: List[str] = field(default_factory=lambda: [\n",
    "        'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "        'driver_reliability', 'driver_aggression', 'driver_risk_taking'\n",
    "    ])\n",
    "    \n",
    "    dynamic_features: List[str] = field(default_factory=lambda: [\n",
    "        'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "        'air_temp', 'humidity'\n",
    "    ])\n",
    "    \n",
    "    target: str = 'milliseconds'\n",
    "\n",
    "# Define the F1Dataset class\n",
    "class F1Dataset(Dataset):\n",
    "    def __init__(self, sequences, static_features, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.static_features = torch.FloatTensor(static_features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.sequences[idx],\n",
    "            'static': self.static_features[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "\n",
    "# Define the F1DataPreprocessor class\n",
    "class F1DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.static_scaler = StandardScaler()\n",
    "        self.dynamic_scaler = StandardScaler()\n",
    "        self.lap_time_scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_sequence_data(self, df: pd.DataFrame, window_size: int = 3) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare sequential data with sliding window and apply scaling\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        static_features = []\n",
    "        targets = []\n",
    "        \n",
    "        # Sort the dataframe to ensure consistent ordering\n",
    "        df = df.sort_values(['raceId', 'driverId', 'lap'])\n",
    "        \n",
    "        # Group by race and driver\n",
    "        for (race_id, driver_id), group in df.groupby(['raceId', 'driverId']):\n",
    "            group = group.sort_values('lap')\n",
    "            \n",
    "            # Extract static features (assumed to be constant per driver per race)\n",
    "            static = group[RaceFeatures.static_features].iloc[0].values\n",
    "            static_features.append(static)\n",
    "            \n",
    "            # Extract dynamic features and target\n",
    "            lap_times = group[RaceFeatures.target].values.reshape(-1, 1)  # Shape: (num_laps, 1)\n",
    "            dynamic = group[RaceFeatures.dynamic_features].values  # Shape: (num_laps, num_dynamic_features)\n",
    "            \n",
    "            # Apply scaling\n",
    "            # Note: Scalers should be fitted on the training data to prevent data leakage.\n",
    "            # Here, for simplicity, we're fitting on the entire dataset. For a real-world scenario,\n",
    "            # consider splitting the data first before fitting the scalers.\n",
    "            lap_times_scaled = self.lap_time_scaler.fit_transform(lap_times).flatten()\n",
    "            dynamic_scaled = self.dynamic_scaler.fit_transform(dynamic)\n",
    "            static_scaled = self.static_scaler.fit_transform(static.reshape(1, -1)).flatten()\n",
    "            \n",
    "            # Create sequences\n",
    "            for i in range(len(lap_times_scaled) - window_size):\n",
    "                sequence_lap_times = lap_times_scaled[i:i+window_size].reshape(-1, 1)  # Shape: (window_size, 1)\n",
    "                sequence_dynamic = dynamic_scaled[i:i+window_size]  # Shape: (window_size, num_dynamic_features)\n",
    "                sequence = np.hstack((sequence_lap_times, sequence_dynamic))  # Shape: (window_size, 1 + num_dynamic_features)\n",
    "                sequences.append(sequence)\n",
    "                static_features.append(static_scaled)\n",
    "                targets.append(lap_times_scaled[i + window_size])\n",
    "        \n",
    "        return (np.array(sequences), \n",
    "                np.array(static_features), \n",
    "                np.array(targets))\n",
    "    \n",
    "    def create_train_val_loaders(\n",
    "        self, \n",
    "        sequences: np.ndarray, \n",
    "        static_features: np.ndarray, \n",
    "        targets: np.ndarray,\n",
    "        batch_size: int = 32,\n",
    "        val_split: float = 0.2\n",
    "    ) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create train and validation dataloaders with given split ratio\n",
    "        \"\"\"\n",
    "        dataset = F1Dataset(sequences, static_features, targets)\n",
    "        \n",
    "        # Calculate lengths for split\n",
    "        val_size = int(len(dataset) * val_split)\n",
    "        train_size = len(dataset) - val_size\n",
    "        \n",
    "        # Split dataset\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "# Define the F1PredictionModel class\n",
    "class F1PredictionModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 sequence_dim: int,\n",
    "                 static_dim: int,\n",
    "                 hidden_dim: int = 64,\n",
    "                 num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTM for sequential features\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=sequence_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Static features processing\n",
    "        self.static_network = nn.Sequential(\n",
    "            nn.Linear(static_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Combine everything\n",
    "        self.final_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence, static):\n",
    "        # Process sequence through LSTM\n",
    "        lstm_out, _ = self.lstm(sequence)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take the output of the last time step\n",
    "        \n",
    "        # Process static features\n",
    "        static_out = self.static_network(static)\n",
    "        \n",
    "        # Combine LSTM output and static features\n",
    "        combined = torch.cat([lstm_out, static_out], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.final_network(combined)\n",
    "        \n",
    "        return prediction.squeeze()\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                epochs: int = 10,\n",
    "                learning_rate: float = 0.001,\n",
    "                device: Optional[str] = None) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train the model and return training history\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            static = batch['static'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(sequences, static)\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch['sequence'].to(device)\n",
    "                static = batch['static'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                predictions = model(sequences, static)\n",
    "                loss = criterion(predictions, targets)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        # Record losses\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Define a function to save the model\n",
    "def save_model(model: nn.Module, path: str):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "# New function to load and preprocess the data\n",
    "def load_and_preprocess_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from CSV files and preprocess it to create the enhanced_laps DataFrame.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    # Define NA values\n",
    "    na_values = ['\\\\N']\n",
    "    \n",
    "    # Load Data\n",
    "    circuits = pd.read_csv('../../data/raw_data/circuits.csv', na_values=na_values)\n",
    "    constructors = pd.read_csv('../../data/raw_data/constructors.csv', na_values=na_values)\n",
    "    drivers = pd.read_csv('../../data/raw_data/drivers.csv', na_values=na_values)\n",
    "    races = pd.read_csv('../../data/raw_data/races.csv', na_values=na_values)\n",
    "    results = pd.read_csv('../../data/raw_data/results.csv', na_values=na_values)\n",
    "    lap_times = pd.read_csv('../../data/raw_data/lap_times.csv', na_values=na_values)\n",
    "    pit_stops = pd.read_csv('../../data/raw_data/pit_stops.csv', na_values=na_values)\n",
    "    qualifying = pd.read_csv('../../data/raw_data/qualifying.csv', na_values=na_values)\n",
    "    status = pd.read_csv('../../data/raw_data/status.csv', na_values=na_values)\n",
    "    weather_data = pd.read_csv('../../data/raw_data/ff1_weather.csv', na_values=na_values)\n",
    "    practice_sessions = pd.read_csv('../../data/raw_data/ff1_free_practice.csv', na_values=na_values)\n",
    "    \n",
    "    preprocessed = pd.read_csv('../../data/processed/export_v1.csv', na_values=na_values)\n",
    "\n",
    "    # Merge dataframes\n",
    "    laps = lap_times.merge(drivers, on='driverId', how='left')\n",
    "    laps = laps.merge(races, on='raceId', how='left')\n",
    "    laps = laps.merge(circuits, on='circuitId', how='left')\n",
    "\n",
    "    # Add pit stop information\n",
    "    laps = laps.merge(pit_stops[['raceId', 'driverId', 'lap', 'duration']], on=['raceId', 'driverId', 'lap'], how='left')\n",
    "    laps['duration'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
    "\n",
    "    # Add weather information\n",
    "    # This is a placeholder; you'll need to match your actual weather data\n",
    "    laps = laps.merge(weather_data, on=['raceId', 'lap'], how='left')\n",
    "\n",
    "    # Feature Engineering\n",
    "    laps['tire_age'] = laps.groupby(['raceId', 'driverId'])['lap'].cumcount()\n",
    "    laps['fuel_load'] = laps.groupby(['raceId', 'driverId'])['lap'].apply(lambda x: x.max() - x + 1)\n",
    "\n",
    "    # For simplicity, we can assign dummy values to static features\n",
    "    # In a real scenario, you should compute these based on historical data\n",
    "    laps['driver_overall_skill'] = 1.0  # Placeholder\n",
    "    laps['driver_circuit_skill'] = 1.0  # Placeholder\n",
    "    laps['driver_consistency'] = 1.0    # Placeholder\n",
    "    laps['driver_reliability'] = 1.0    # Placeholder\n",
    "    laps['driver_aggression'] = 1.0     # Placeholder\n",
    "    laps['driver_risk_taking'] = 1.0    # Placeholder\n",
    "\n",
    "    # Dynamic features (assuming you have these in your weather_data)\n",
    "    # If not, assign dummy values or extract from available data\n",
    "    laps['track_temp'] = laps['track_temp'].fillna(25.0)  # Placeholder\n",
    "    laps['air_temp'] = laps['air_temp'].fillna(20.0)      # Placeholder\n",
    "    laps['humidity'] = laps['humidity'].fillna(50.0)      # Placeholder\n",
    "\n",
    "    # Ensure that all required columns are present\n",
    "    required_columns = RaceFeatures.static_features + RaceFeatures.dynamic_features + [RaceFeatures.target]\n",
    "    missing_columns = set(required_columns) - set(laps.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    # Drop rows with missing values in required columns\n",
    "    laps = laps.dropna(subset=required_columns)\n",
    "\n",
    "    return laps\n",
    "\n",
    "# Update the main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    enhanced_laps = load_and_preprocess_data()\n",
    "    \n",
    "    preprocessor = F1DataPreprocessor()\n",
    "    sequences, static, targets = preprocessor.prepare_sequence_data(enhanced_laps, window_size=3)\n",
    "    \n",
    "    # Create train and validation loaders\n",
    "    train_loader, val_loader = preprocessor.create_train_val_loaders(\n",
    "        sequences, \n",
    "        static, \n",
    "        targets,\n",
    "        batch_size=32,\n",
    "        val_split=0.2\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = F1PredictionModel(\n",
    "        sequence_dim=sequences.shape[2],\n",
    "        static_dim=static.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = train_model(model, train_loader, val_loader, epochs=20, learning_rate=0.001)\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model(model, 'f1_prediction_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
