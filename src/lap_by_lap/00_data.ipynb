{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Model Training\n",
    "\n",
    "This notebook handles data loading, preprocessing, splitting the dataset, training multiple models, and evaluating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import joblib\n",
    "\n",
    "na_values = ['\\\\N', 'NaN', '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class RaceFeatures:\n",
    "    static_features: List[str] = field(default_factory=lambda: [\n",
    "        'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "        'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "        'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time',\n",
    "        'constructor_performance', 'circuitId'\n",
    "    ])\n",
    "    dynamic_features: List[str] = field(default_factory=lambda: [\n",
    "        'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "        'air_temp', 'humidity', 'tire_compound', 'TrackStatus', 'is_pit_lap'\n",
    "    ])\n",
    "    target: str = 'milliseconds'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation functions\n",
    "def load_raw_data():\n",
    "    lap_times = pd.read_csv('../../data/raw_data/lap_times.csv', na_values=na_values)\n",
    "    drivers = pd.read_csv('../../data/raw_data/drivers.csv', na_values=na_values)\n",
    "    races = pd.read_csv('../../data/raw_data/races.csv', na_values=na_values)\n",
    "    circuits = pd.read_csv('../../data/raw_data/circuits.csv', na_values=na_values)\n",
    "    pit_stops = pd.read_csv('../../data/raw_data/pit_stops.csv', na_values=na_values)\n",
    "    pit_stops.rename(columns={'milliseconds': 'pitstop_milliseconds'}, inplace=True)\n",
    "    results = pd.read_csv('../../data/raw_data/results.csv', na_values=na_values)\n",
    "    results.rename(columns={'milliseconds': 'racetime_milliseconds'}, inplace=True)\n",
    "    qualifying = pd.read_csv('../../data/raw_data/qualifying.csv', na_values=na_values)\n",
    "    status = pd.read_csv('../../data/raw_data/status.csv', na_values=na_values)\n",
    "    weather_data = pd.read_csv('../../data/raw_data/ff1_weather.csv', na_values=na_values)\n",
    "    practice_sessions = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "    tire_data = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "    \n",
    "    return {\n",
    "        'lap_times': lap_times,\n",
    "        'drivers': drivers,\n",
    "        'races': races,\n",
    "        'circuits': circuits,\n",
    "        'pit_stops': pit_stops,\n",
    "        'results': results,\n",
    "        'qualifying': qualifying,\n",
    "        'status': status,\n",
    "        'weather_data': weather_data,\n",
    "        'practice_sessions': practice_sessions,\n",
    "        'tire_data': tire_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    # Load raw data\n",
    "    data = load_raw_data()\n",
    "\n",
    "    lap_times = data['lap_times']\n",
    "    drivers = data['drivers']\n",
    "    races = data['races']\n",
    "    circuits = data['circuits']\n",
    "    pit_stops = data['pit_stops']\n",
    "    results = data['results']\n",
    "    qualifying = data['qualifying']\n",
    "    status = data['status']\n",
    "    weather_data = data['weather_data']\n",
    "    practice_sessions = data['practice_sessions']\n",
    "    tire_data = data['tire_data']\n",
    "    \n",
    "    # Implement data preprocessing steps \n",
    "    # These include merging dataframes, feature engineering, and handling missing values\n",
    "    # Convert date columns to datetime\n",
    "    races['date'] = pd.to_datetime(races['date'])\n",
    "    results['date'] = results['raceId'].map(races.set_index('raceId')['date'])\n",
    "    lap_times['date'] = lap_times['raceId'].map(races.set_index('raceId')['date'])\n",
    "    \n",
    "    # Merge dataframes\n",
    "    laps = lap_times.merge(drivers, on='driverId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(races, on='raceId', how='left', suffixes=('', '_race'))\n",
    "    laps.rename(columns={'quali_time' : 'quali_date_time'}, inplace=True)\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(circuits, on='circuitId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(results[['raceId', 'driverId', 'positionOrder', 'grid', 'racetime_milliseconds', 'fastestLap', 'statusId']], on=['raceId', 'driverId'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(status, on='statusId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(pit_stops[['raceId', 'driverId', 'lap', 'pitstop_milliseconds']], on=['raceId', 'driverId', 'lap'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps['pitstop_milliseconds'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
    "    print(laps.shape)\n",
    "\n",
    "    # Load additional data\n",
    "    constructors = pd.read_csv('../../data/raw_data/constructors.csv', na_values=na_values)\n",
    "    constructor_results = pd.read_csv('../../data/raw_data/constructor_results.csv', na_values=na_values)\n",
    "    constructor_standings = pd.read_csv('../../data/raw_data/constructor_standings.csv', na_values=na_values)\n",
    "    \n",
    "    # Merge constructors with drivers\n",
    "    results = results.merge(constructors[['constructorId', 'name', 'nationality']], on='constructorId', how='left')\n",
    "    results.rename(columns={'name': 'constructor_name', 'nationality': 'constructor_nationality'}, inplace=True)\n",
    "    \n",
    "    # Map driverId to constructorId\n",
    "    driver_constructor = results[['raceId', 'driverId', 'constructorId']].drop_duplicates()\n",
    "    \n",
    "    # Merge driver_constructor into laps\n",
    "    laps = laps.merge(driver_constructor, on=['raceId', 'driverId'], how='left')\n",
    "    \n",
    "    # Add constructor performance metrics\n",
    "    # For simplicity, we'll use the constructor standings position as a performance metric\n",
    "    constructor_standings_latest = constructor_standings.sort_values('raceId', ascending=False).drop_duplicates('constructorId')\n",
    "    constructor_standings_latest = constructor_standings_latest[['constructorId', 'points', 'position']]\n",
    "    constructor_standings_latest.rename(columns={'points': 'constructor_points', 'position': 'constructor_position'}, inplace=True)\n",
    "    \n",
    "    laps = laps.merge(constructor_standings_latest, on='constructorId', how='left')\n",
    "    \n",
    "    # Fill missing constructor performance data\n",
    "    laps['constructor_points'].fillna(laps['constructor_points'].mean(), inplace=True)\n",
    "    laps['constructor_position'].fillna(laps['constructor_position'].max(), inplace=True)\n",
    "    \n",
    "    # Add constructor performance as a static feature\n",
    "    laps['constructor_performance'] = laps['constructor_points']\n",
    "    \n",
    "    # Add circuit characteristics\n",
    "    # For simplicity, let's assume circuit length and type are available in circuits.csv\n",
    "    circuits['circuit_length'] = 5.0  # Placeholder value, replace with actual data if available\n",
    "    circuits['circuit_type'] = 'Permanent'  # Options could be 'Permanent', 'Street', 'Hybrid'\n",
    "    \n",
    "    # Merge circuit data into laps\n",
    "    laps = laps.merge(circuits[['circuitId', 'circuit_length', 'circuit_type']], on='circuitId', how='left')\n",
    "    \n",
    "    # Encode circuit_type as a categorical variable\n",
    "    circuit_type_mapping = {'Permanent': 0, 'Street': 1, 'Hybrid': 2}\n",
    "    laps['circuit_type_encoded'] = laps['circuit_type'].map(circuit_type_mapping)\n",
    "    \n",
    "    # Add weather information\n",
    "    # Filter weather data to include only the Race session\n",
    "    weather_data = weather_data[weather_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge weather data with races to get raceId\n",
    "    weather_data = weather_data.merge(\n",
    "        races[['raceId', 'year', 'name']], \n",
    "        left_on=['EventName', 'Year'],\n",
    "        right_on=['name', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Compute cumulative time from the start of the race for each driver\n",
    "    laps.sort_values(['raceId', 'driverId', 'lap'], inplace=True)\n",
    "    laps['cumulative_milliseconds'] = laps.groupby(['raceId', 'driverId'])['milliseconds'].cumsum()\n",
    "    laps['seconds_from_start'] = laps['cumulative_milliseconds'] / 1000\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Use 'Time' in weather_data as 'seconds_from_start'\n",
    "    weather_data['seconds_from_start'] = weather_data['Time']\n",
    "\n",
    "    \n",
    "    \n",
    "    # Standardize text data\n",
    "    tire_data['Compound'] = tire_data['Compound'].str.upper()\n",
    "    tire_data['EventName'] = tire_data['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Filter for race sessions only\n",
    "    tire_data = tire_data[tire_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge with races to get raceId\n",
    "    tire_data = tire_data.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    tire_data['Driver'] = tire_data['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    tire_data['driverId'] = tire_data['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Rename 'LapNumber' to 'lap' and ensure integer type\n",
    "    tire_data.rename(columns={'LapNumber': 'lap'}, inplace=True)\n",
    "    tire_data['lap'] = tire_data['lap'].astype(int)\n",
    "    laps['lap'] = laps['lap'].astype(int)\n",
    "    \n",
    "    # Create compound mapping (ordered from hardest to softest)\n",
    "    compound_mapping = {\n",
    "        'UNKNOWN': 0,\n",
    "        np.nan: 0,\n",
    "        'HARD': 1,\n",
    "        'MEDIUM': 2,\n",
    "        'SOFT': 3,\n",
    "        'SUPERSOFT': 3,    # Treat as \"Soft\"\n",
    "        'ULTRASOFT': 3,    # Treat as \"Soft\"\n",
    "        'HYPERSOFT': 3,    # Treat as \"Soft\"\n",
    "        'INTERMEDIATE': 4,\n",
    "        'WET': 5\n",
    "    }\n",
    "    # Merge tire_data with laps\n",
    "    laps = laps.merge(\n",
    "        tire_data[['raceId', 'driverId', 'lap', 'Compound', 'TrackStatus']],\n",
    "        on=['raceId', 'driverId', 'lap'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Handle missing compounds and apply numeric encoding\n",
    "    laps['Compound'].fillna('UNKNOWN', inplace=True)\n",
    "    laps['tire_compound'] = laps['Compound'].map(compound_mapping)\n",
    "    \n",
    "    # Drop the original Compound column if desired\n",
    "    laps.drop('Compound', axis=1, inplace=True)\n",
    "    \n",
    "    # Standardize names\n",
    "    practice_sessions['EventName'] = practice_sessions['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Merge practice_sessions with races to get raceId\n",
    "    practice_sessions = practice_sessions.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    practice_sessions['Driver'] = practice_sessions['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    practice_sessions['driverId'] = practice_sessions['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Convert LapTime to milliseconds\n",
    "    practice_sessions['LapTime_ms'] = practice_sessions['LapTime'].apply(lambda x: pd.to_timedelta(x).total_seconds() * 1000)\n",
    "    \n",
    "    # Calculate median lap times for each driver in each session\n",
    "    session_medians = practice_sessions.groupby(['raceId', 'driverId', 'SessionName'])['LapTime_ms'].median().reset_index()\n",
    "    \n",
    "    # Pivot the data to have sessions as columns\n",
    "    session_medians_pivot = session_medians.pivot_table(\n",
    "        index=['raceId', 'driverId'],\n",
    "        columns='SessionName',\n",
    "        values='LapTime_ms'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    session_medians_pivot.rename(columns={\n",
    "        'FP1': 'fp1_median_time',\n",
    "        'FP2': 'fp2_median_time',\n",
    "        'FP3': 'fp3_median_time',\n",
    "        'Q': 'quali_time'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    laps = laps.merge(\n",
    "    session_medians_pivot,\n",
    "    on=['raceId', 'driverId'],\n",
    "    how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing practice times with global median or a placeholder value\n",
    "    global_median_fp1 = laps['fp1_median_time'].median()\n",
    "    laps['fp1_median_time'].fillna(global_median_fp1, inplace=True)\n",
    "    \n",
    "    # Repeat for other sessions\n",
    "    global_median_fp2 = laps['fp2_median_time'].median()\n",
    "    laps['fp2_median_time'].fillna(global_median_fp2, inplace=True)\n",
    "    \n",
    "    global_median_fp3 = laps['fp3_median_time'].median()\n",
    "    laps['fp3_median_time'].fillna(global_median_fp3, inplace=True)\n",
    "    \n",
    "    global_median_quali = laps['quali_time'].median()\n",
    "    laps['quali_time'].fillna(global_median_quali, inplace=True)\n",
    "\n",
    "    \n",
    "    # Create a binary indicator for pit stops\n",
    "    laps['is_pit_lap'] = laps['pitstop_milliseconds'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    \n",
    "    # Define a function to match weather data to laps\n",
    "    def match_weather_to_lap(race_laps, race_weather):\n",
    "        \"\"\"\n",
    "        For each lap, find the closest weather measurement in time\n",
    "        \"\"\"\n",
    "        race_laps = race_laps.sort_values('seconds_from_start')\n",
    "        race_weather = race_weather.sort_values('seconds_from_start')\n",
    "        merged = pd.merge_asof(\n",
    "            race_laps,\n",
    "            race_weather,\n",
    "            on='seconds_from_start',\n",
    "            direction='nearest'\n",
    "        )\n",
    "        return merged\n",
    "\n",
    "    # Apply matching per race\n",
    "    matched_laps_list = []\n",
    "    for race_id in laps['raceId'].unique():\n",
    "        print(f'Matching for {race_id}')\n",
    "        race_laps = laps[laps['raceId'] == race_id]\n",
    "        race_weather = weather_data[weather_data['raceId'] == race_id]\n",
    "        \n",
    "        if not race_weather.empty:\n",
    "            matched = match_weather_to_lap(race_laps, race_weather)\n",
    "            print(f\"Matched DataFrame shape: {matched.shape}\")\n",
    "            matched_laps_list.append(matched)\n",
    "        else:\n",
    "            matched_laps_list.append(race_laps)  # No weather data for this race\n",
    "\n",
    "    # Concatenate all matched laps\n",
    "    laps = pd.concat(matched_laps_list, ignore_index=True)\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Fill missing weather data with default values\n",
    "    laps['track_temp'] = laps['TrackTemp'].fillna(25.0)\n",
    "    laps['air_temp'] = laps['AirTemp'].fillna(20.0)\n",
    "    laps['humidity'] = laps['Humidity'].fillna(50.0)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    # Create driver names\n",
    "    drivers['driver_name'] = drivers['forename'] + ' ' + drivers['surname']\n",
    "    driver_mapping = drivers[['driverId', 'driver_name']].copy()\n",
    "    driver_mapping.set_index('driverId', inplace=True)\n",
    "    driver_names = driver_mapping['driver_name'].to_dict()\n",
    "    \n",
    "    # Map statusId to status descriptions\n",
    "    status_dict = status.set_index('statusId')['status'].to_dict()\n",
    "    results['status'] = results['statusId'].map(status_dict)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    def calculate_aggression(driver_results):\n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default aggression for new drivers\n",
    "        \n",
    "        # Only consider recent races for more current behavior\n",
    "        recent_results = driver_results.sort_values('date', ascending=False).head(20)\n",
    "        \n",
    "        # Calculate overtaking metrics\n",
    "        positions_gained = recent_results['grid'] - recent_results['positionOrder']\n",
    "        \n",
    "        # Calculate risk metrics\n",
    "        dnf_rate = (recent_results['status'] != 'Finished').mean()\n",
    "        incidents = (recent_results['statusId'].isin([\n",
    "            4,  # Collision\n",
    "            5,  # Spun off\n",
    "            6,  # Accident\n",
    "            20, # Collision damage\n",
    "            82, # Collision with another driver\n",
    "        ])).mean()\n",
    "        \n",
    "        # Calculate overtaking success rate (normalized between 0-1)\n",
    "        positive_overtakes = (positions_gained > 0).sum()\n",
    "        negative_overtakes = (positions_gained < 0).sum()\n",
    "        total_overtake_attempts = positive_overtakes + negative_overtakes\n",
    "        overtake_success_rate = positive_overtakes / total_overtake_attempts if total_overtake_attempts > 0 else 0.5\n",
    "        \n",
    "        # Normalize average positions gained (0-1)\n",
    "        avg_positions_gained = positions_gained[positions_gained > 0].mean() if len(positions_gained[positions_gained > 0]) > 0 else 0\n",
    "        max_possible_gain = 20  # Maximum grid positions that could be gained\n",
    "        normalized_gains = np.clip(avg_positions_gained / max_possible_gain, 0, 1)\n",
    "        \n",
    "        # Normalize risk factors (0-1)\n",
    "        normalized_dnf = np.clip(dnf_rate, 0, 1)\n",
    "        normalized_incidents = np.clip(incidents, 0, 1)\n",
    "        \n",
    "        # Calculate component scores (each between 0-1)\n",
    "        overtaking_component = (normalized_gains * 0.6 + overtake_success_rate * 0.4)\n",
    "        risk_component = (normalized_dnf * 0.5 + normalized_incidents * 0.5)\n",
    "        \n",
    "        # Combine components with weights (ensuring sum of weights = 1)\n",
    "        weights = {\n",
    "            'overtaking': 0.4,  # Aggressive overtaking\n",
    "            'risk': 0.5,       # Risk-taking behavior\n",
    "            'baseline': 0.1    # Baseline aggression\n",
    "        }\n",
    "        \n",
    "        aggression = (\n",
    "            overtaking_component * weights['overtaking'] +\n",
    "            risk_component * weights['risk'] +\n",
    "            0.5 * weights['baseline']  # Baseline aggression factor\n",
    "        )\n",
    "        \n",
    "        # Add small random variation while maintaining 0-1 bounds\n",
    "        variation = np.random.normal(0, 0.02)\n",
    "        aggression = np.clip(aggression + variation, 0, 1)\n",
    "        \n",
    "        return aggression\n",
    "    \n",
    "    # Modify calculate_skill function\n",
    "    def calculate_skill(driver_data, results_data, circuit_id, constructor_performance):\n",
    "        driver_results = results_data[\n",
    "            (results_data['driverId'] == driver_data['driverId']) & \n",
    "            (results_data['circuitId'] == circuit_id)\n",
    "        ].sort_values('date', ascending=False).head(10)  # Use last 10 races at circuit\n",
    "        \n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default skill\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        avg_finish_pos = driver_results['positionOrder'].mean()\n",
    "        avg_quali_pos = driver_results['grid'].mean()\n",
    "        points_per_race = driver_results['points'].mean()\n",
    "        fastest_laps = (driver_results['rank'] == 1).mean()\n",
    "        \n",
    "        # Include constructor performance\n",
    "        constructor_factor = np.exp(-constructor_performance / 100)\n",
    "        \n",
    "        # Improved normalization (exponential decay for positions)\n",
    "        normalized_finish_pos = np.exp(-avg_finish_pos/5) # Better spread of values\n",
    "        normalized_quali_pos = np.exp(-avg_quali_pos/5)\n",
    "        \n",
    "        # Points normalization with improved scaling\n",
    "        max_points_per_race = 26  # Maximum possible points (25 + 1 fastest lap)\n",
    "        normalized_points = points_per_race / max_points_per_race\n",
    "        \n",
    "        # Weighted combination with more factors\n",
    "        weights = {\n",
    "            'finish': 0.35,\n",
    "            'quali': 0.25,\n",
    "            'points': 0.25,\n",
    "            'fastest_laps': 0.15\n",
    "        }\n",
    "        \n",
    "        skill = (\n",
    "            weights['finish'] * normalized_finish_pos +\n",
    "            weights['quali'] * normalized_quali_pos +\n",
    "            weights['points'] * normalized_points +\n",
    "            weights['fastest_laps'] * fastest_laps +\n",
    "            0.1 * constructor_factor  # Adjust weight as needed\n",
    "        )\n",
    "        \n",
    "        # Add random variation to prevent identical skills\n",
    "        skill = np.clip(skill + np.random.normal(0, 0.05), 0.1, 1.0)\n",
    "    \n",
    "        return skill\n",
    "    \n",
    "    # First merge results with races to get circuitId\n",
    "    results = results.merge(\n",
    "        races[['raceId', 'circuitId']], \n",
    "        on='raceId',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Now calculate driver aggression and skill\n",
    "    driver_aggression = {}\n",
    "    driver_skill = {}\n",
    "    for driver_id in drivers['driverId'].unique():\n",
    "        driver_results = results[results['driverId'] == driver_id]\n",
    "        aggression = calculate_aggression(driver_results)\n",
    "        driver_aggression[driver_id] = aggression\n",
    "        \n",
    "        # Now we have circuit_id from the merge\n",
    "        recent_race = driver_results.sort_values('date', ascending=False).head(1)\n",
    "        if not recent_race.empty:\n",
    "            circuit_id = recent_race['circuitId'].iloc[0]\n",
    "            constructor_performance = laps.loc[laps['driverId'] == driver_id, 'constructor_performance'].mean()\n",
    "            skill = calculate_skill({'driverId': driver_id}, results, circuit_id, constructor_performance)\n",
    "            driver_skill[driver_id] = skill\n",
    "        else:\n",
    "            driver_skill[driver_id] = 0.5  # Default skill for new drivers\n",
    "    \n",
    "    # Map calculated aggression and skill back to laps DataFrame\n",
    "    laps['driver_aggression'] = laps['driverId'].map(driver_aggression)\n",
    "    laps['driver_overall_skill'] = laps['driverId'].map(driver_skill)\n",
    "    laps['driver_circuit_skill'] = laps['driver_overall_skill']  # For simplicity, using overall skill\n",
    "    laps['driver_consistency'] = 0.5  # Placeholder\n",
    "    laps['driver_reliability'] = 0.5  # Placeholder\n",
    "    laps['driver_risk_taking'] = laps['driver_aggression']  # Assuming similar to aggression\n",
    "    \n",
    "    # Dynamic features\n",
    "    laps['tire_age'] = laps.groupby(['raceId', 'driverId'])['lap'].cumcount()\n",
    "    laps['fuel_load'] = laps.groupby(['raceId', 'driverId'])['lap'].transform(lambda x: x.max() - x + 1)\n",
    "    laps['track_position'] = laps['position']  # Assuming 'position' is available in laps data\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    # Create an instance of RaceFeatures\n",
    "    race_features = RaceFeatures()\n",
    "\n",
    "    \n",
    "    laps['TrackStatus'].fillna(1, inplace=True)  # 1 = regular racing status\n",
    "    \n",
    "    # Update required columns\n",
    "    required_columns = race_features.static_features + race_features.dynamic_features\n",
    "    # Ensure all required columns are present in laps\n",
    "    missing_columns = set(required_columns) - set(laps.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Drop rows with missing values in required columns\n",
    "    laps = laps[laps['year'] >= 2018]\n",
    "    laps = laps.dropna(subset=required_columns)\n",
    "    \n",
    "    # Return the preprocessed DataFrame\n",
    "    return laps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    # Ensure RaceFeatures is available\n",
    "    race_features = RaceFeatures()\n",
    "\n",
    "    # Preprocess data\n",
    "    df = preprocess_data()\n",
    "\n",
    "    # Validate that all required columns are present\n",
    "    required_columns = race_features.static_features + race_features.dynamic_features\n",
    "    missing_columns = set(required_columns) - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "    # Return the processed DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_race(df, test_size=0.2, random_state=42):\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_idx, test_idx = next(gss.split(df, groups=df['raceId']))\n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    return train_df, test_df\n",
    "\n",
    "def save_data_splits(train_df, test_df):\n",
    "    train_df.to_csv('train_data.csv', index=False)\n",
    "    test_df.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_regression_data(df):\n",
    "    X = df[[\n",
    "        'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "        'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "        'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time',\n",
    "        'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "        'air_temp', 'humidity', 'tire_compound', 'TrackStatus', 'is_pit_lap'\n",
    "    ]]\n",
    "    y = df['milliseconds']\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data(df, window_size=3):\n",
    "    # Your existing sequence preparation code\n",
    "    # Return sequences, static_features, targets\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (seconds): 19.00\n",
      "RMSE (seconds): 126.11\n",
      "R2: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_10603/2876371310.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[cat_feature] = X_train[cat_feature].astype(str)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_10603/2876371310.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[cat_feature] = X_test[cat_feature].astype(str)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_10603/2876371310.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[cat_feature] = X_train[cat_feature].astype(str)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_10603/2876371310.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[cat_feature] = X_test[cat_feature].astype(str)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_10603/2876371310.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[cat_feature] = X_train[cat_feature].astype(str)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_10603/2876371310.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[cat_feature] = X_test[cat_feature].astype(str)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "def train_regression_model():\n",
    "    # Load the train and test data\n",
    "    train_df = pd.read_csv('train_data.csv')\n",
    "    test_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "    # Convert target to seconds instead of milliseconds for better numerical stability\n",
    "    train_df['lap_time_seconds'] = train_df['milliseconds'] / 1000\n",
    "    test_df['lap_time_seconds'] = test_df['milliseconds'] / 1000\n",
    "\n",
    "    # Log transform the target variable to handle non-linearity\n",
    "    train_df['lap_time_log'] = np.log(train_df['lap_time_seconds'])\n",
    "    test_df['lap_time_log'] = np.log(test_df['lap_time_seconds'])\n",
    "\n",
    "    # Prepare features\n",
    "    numeric_features = [\n",
    "        'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "        'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "        'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time',\n",
    "        'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "        'air_temp', 'humidity', 'is_pit_lap'\n",
    "    ]\n",
    "    categorical_features = ['tire_compound', 'TrackStatus', 'circuitId',]\n",
    "\n",
    "    # Prepare X and y\n",
    "    X_train = train_df[numeric_features + categorical_features]\n",
    "    y_train = train_df['lap_time_log']\n",
    "    X_test = test_df[numeric_features + categorical_features]\n",
    "    y_test = test_df['lap_time_log']\n",
    "\n",
    "    # Convert categorical features to string type\n",
    "    for cat_feature in categorical_features:\n",
    "        X_train[cat_feature] = X_train[cat_feature].astype(str)\n",
    "        X_test[cat_feature] = X_test[cat_feature].astype(str)\n",
    "\n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.01,\n",
    "            max_depth=5,\n",
    "            random_state=42,\n",
    "            tree_method='hist'  # Faster tree method\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_log = pipeline.predict(X_test)\n",
    "    \n",
    "    # Convert predictions back to seconds\n",
    "    y_pred_seconds = np.exp(y_pred_log)\n",
    "    y_true_seconds = np.exp(y_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_true_seconds, y_pred_seconds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_seconds, y_pred_seconds))\n",
    "    r2 = r2_score(y_true_seconds, y_pred_seconds)\n",
    "\n",
    "    print(f\"MAE (seconds): {mae:.2f}\")\n",
    "    print(f\"RMSE (seconds): {rmse:.2f}\")\n",
    "    print(f\"R2: {r2:.3f}\")\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(pipeline, 'xgboost_model.joblib')\n",
    "\n",
    "    return pipeline, mae, rmse, r2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_model.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model.joblib'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('best_model.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
