{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f49f8388",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fb0bbdee5a394c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T17:46:55.129913Z",
     "start_time": "2024-11-28T17:46:49.334643Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_19862/4217053995.py:325: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  practice_sessions = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_19862/4217053995.py:327: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tire_data = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586171, 15)\n",
      "(586171, 32)\n",
      "(586171, 40)\n",
      "(586171, 45)\n",
      "(586171, 46)\n",
      "(586171, 47)\n",
      "(586171, 47)\n",
      "(586171, 49)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_19862/4217053995.py:348: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['pitstop_milliseconds'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_19862/4217053995.py:418: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['Compound'].fillna('UNKNOWN', inplace=True)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_19862/4217053995.py:471: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['fp1_median_time'].fillna(global_median_fp1, inplace=True)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_19862/4217053995.py:475: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['fp2_median_time'].fillna(global_median_fp2, inplace=True)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_19862/4217053995.py:478: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['fp3_median_time'].fillna(global_median_fp3, inplace=True)\n",
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_19862/4217053995.py:481: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['quali_time'].fillna(global_median_quali, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching for 1\n",
      "Matching for 2\n",
      "Matching for 3\n",
      "Matching for 4\n",
      "Matching for 5\n",
      "Matching for 6\n",
      "Matching for 7\n",
      "Matching for 8\n",
      "Matching for 9\n",
      "Matching for 10\n",
      "Matching for 11\n",
      "Matching for 12\n",
      "Matching for 13\n",
      "Matching for 14\n",
      "Matching for 15\n",
      "Matching for 16\n",
      "Matching for 17\n",
      "Matching for 18\n",
      "Matching for 19\n",
      "Matching for 20\n",
      "Matching for 21\n",
      "Matching for 22\n",
      "Matching for 23\n",
      "Matching for 24\n",
      "Matching for 25\n",
      "Matching for 26\n",
      "Matching for 27\n",
      "Matching for 28\n",
      "Matching for 29\n",
      "Matching for 30\n",
      "Matching for 31\n",
      "Matching for 32\n",
      "Matching for 33\n",
      "Matching for 34\n",
      "Matching for 35\n",
      "Matching for 36\n",
      "Matching for 37\n",
      "Matching for 38\n",
      "Matching for 39\n",
      "Matching for 40\n",
      "Matching for 41\n",
      "Matching for 42\n",
      "Matching for 43\n",
      "Matching for 44\n",
      "Matching for 45\n",
      "Matching for 46\n",
      "Matching for 47\n",
      "Matching for 48\n",
      "Matching for 49\n",
      "Matching for 50\n",
      "Matching for 51\n",
      "Matching for 52\n",
      "Matching for 53\n",
      "Matching for 54\n",
      "Matching for 55\n",
      "Matching for 56\n",
      "Matching for 57\n",
      "Matching for 58\n",
      "Matching for 59\n",
      "Matching for 60\n",
      "Matching for 61\n",
      "Matching for 62\n",
      "Matching for 63\n",
      "Matching for 64\n",
      "Matching for 65\n",
      "Matching for 66\n",
      "Matching for 67\n",
      "Matching for 68\n",
      "Matching for 69\n",
      "Matching for 70\n",
      "Matching for 71\n",
      "Matching for 72\n",
      "Matching for 73\n",
      "Matching for 74\n",
      "Matching for 75\n",
      "Matching for 76\n",
      "Matching for 77\n",
      "Matching for 78\n",
      "Matching for 79\n",
      "Matching for 80\n",
      "Matching for 81\n",
      "Matching for 82\n",
      "Matching for 83\n",
      "Matching for 84\n",
      "Matching for 85\n",
      "Matching for 86\n",
      "Matching for 87\n",
      "Matching for 88\n",
      "Matching for 89\n",
      "Matching for 90\n",
      "Matching for 91\n",
      "Matching for 92\n",
      "Matching for 93\n",
      "Matching for 94\n",
      "Matching for 95\n",
      "Matching for 96\n",
      "Matching for 97\n",
      "Matching for 98\n",
      "Matching for 99\n",
      "Matching for 100\n",
      "Matching for 101\n",
      "Matching for 102\n",
      "Matching for 103\n",
      "Matching for 104\n",
      "Matching for 105\n",
      "Matching for 106\n",
      "Matching for 107\n",
      "Matching for 108\n",
      "Matching for 109\n",
      "Matching for 110\n",
      "Matching for 111\n",
      "Matching for 112\n",
      "Matching for 113\n",
      "Matching for 114\n",
      "Matching for 115\n",
      "Matching for 116\n",
      "Matching for 117\n",
      "Matching for 118\n",
      "Matching for 119\n",
      "Matching for 120\n",
      "Matching for 121\n",
      "Matching for 122\n",
      "Matching for 123\n",
      "Matching for 124\n",
      "Matching for 125\n",
      "Matching for 126\n",
      "Matching for 127\n",
      "Matching for 128\n",
      "Matching for 129\n",
      "Matching for 130\n",
      "Matching for 131\n",
      "Matching for 132\n",
      "Matching for 133\n",
      "Matching for 134\n",
      "Matching for 135\n",
      "Matching for 136\n",
      "Matching for 137\n",
      "Matching for 138\n",
      "Matching for 139\n",
      "Matching for 140\n",
      "Matching for 141\n",
      "Matching for 142\n",
      "Matching for 143\n",
      "Matching for 144\n",
      "Matching for 145\n",
      "Matching for 146\n",
      "Matching for 147\n",
      "Matching for 148\n",
      "Matching for 149\n",
      "Matching for 150\n",
      "Matching for 151\n",
      "Matching for 152\n",
      "Matching for 153\n",
      "Matching for 154\n",
      "Matching for 155\n",
      "Matching for 156\n",
      "Matching for 157\n",
      "Matching for 158\n",
      "Matching for 159\n",
      "Matching for 160\n",
      "Matching for 161\n",
      "Matching for 162\n",
      "Matching for 163\n",
      "Matching for 164\n",
      "Matching for 165\n",
      "Matching for 166\n",
      "Matching for 167\n",
      "Matching for 168\n",
      "Matching for 169\n",
      "Matching for 170\n",
      "Matching for 171\n",
      "Matching for 172\n",
      "Matching for 173\n",
      "Matching for 174\n",
      "Matching for 175\n",
      "Matching for 176\n",
      "Matching for 177\n",
      "Matching for 178\n",
      "Matching for 179\n",
      "Matching for 180\n",
      "Matching for 181\n",
      "Matching for 182\n",
      "Matching for 183\n",
      "Matching for 184\n",
      "Matching for 185\n",
      "Matching for 186\n",
      "Matching for 187\n",
      "Matching for 188\n",
      "Matching for 189\n",
      "Matching for 190\n",
      "Matching for 191\n",
      "Matching for 192\n",
      "Matching for 193\n",
      "Matching for 194\n",
      "Matching for 195\n",
      "Matching for 196\n",
      "Matching for 197\n",
      "Matching for 198\n",
      "Matching for 199\n",
      "Matching for 200\n",
      "Matching for 201\n",
      "Matching for 202\n",
      "Matching for 203\n",
      "Matching for 204\n",
      "Matching for 205\n",
      "Matching for 206\n",
      "Matching for 207\n",
      "Matching for 208\n",
      "Matching for 209\n",
      "Matching for 210\n",
      "Matching for 211\n",
      "Matching for 212\n",
      "Matching for 213\n",
      "Matching for 214\n",
      "Matching for 215\n",
      "Matching for 216\n",
      "Matching for 217\n",
      "Matching for 218\n",
      "Matching for 219\n",
      "Matching for 220\n",
      "Matching for 221\n",
      "Matching for 222\n",
      "Matching for 223\n",
      "Matching for 224\n",
      "Matching for 225\n",
      "Matching for 226\n",
      "Matching for 227\n",
      "Matching for 228\n",
      "Matching for 229\n",
      "Matching for 230\n",
      "Matching for 231\n",
      "Matching for 232\n",
      "Matching for 233\n",
      "Matching for 234\n",
      "Matching for 235\n",
      "Matching for 236\n",
      "Matching for 237\n",
      "Matching for 238\n",
      "Matching for 239\n",
      "Matching for 337\n",
      "Matching for 338\n",
      "Matching for 339\n",
      "Matching for 340\n",
      "Matching for 341\n",
      "Matching for 342\n",
      "Matching for 343\n",
      "Matching for 344\n",
      "Matching for 345\n",
      "Matching for 346\n",
      "Matching for 347\n",
      "Matching for 348\n",
      "Matching for 349\n",
      "Matching for 350\n",
      "Matching for 351\n",
      "Matching for 352\n",
      "Matching for 353\n",
      "Matching for 354\n",
      "Matching for 355\n",
      "Matching for 841\n",
      "Matching for 842\n",
      "Matching for 843\n",
      "Matching for 844\n",
      "Matching for 845\n",
      "Matching for 846\n",
      "Matching for 847\n",
      "Matching for 848\n",
      "Matching for 849\n",
      "Matching for 850\n",
      "Matching for 851\n",
      "Matching for 852\n",
      "Matching for 853\n",
      "Matching for 854\n",
      "Matching for 855\n",
      "Matching for 856\n",
      "Matching for 857\n",
      "Matching for 858\n",
      "Matching for 859\n",
      "Matching for 860\n",
      "Matching for 861\n",
      "Matching for 862\n",
      "Matching for 863\n",
      "Matching for 864\n",
      "Matching for 865\n",
      "Matching for 866\n",
      "Matching for 867\n",
      "Matching for 868\n",
      "Matching for 869\n",
      "Matching for 870\n",
      "Matching for 871\n",
      "Matching for 872\n",
      "Matching for 873\n",
      "Matching for 874\n",
      "Matching for 875\n",
      "Matching for 876\n",
      "Matching for 877\n",
      "Matching for 878\n",
      "Matching for 879\n",
      "Matching for 880\n",
      "Matching for 881\n",
      "Matching for 882\n",
      "Matching for 883\n",
      "Matching for 884\n",
      "Matching for 885\n",
      "Matching for 886\n",
      "Matching for 887\n",
      "Matching for 888\n",
      "Matching for 890\n",
      "Matching for 891\n",
      "Matching for 892\n",
      "Matching for 893\n",
      "Matching for 894\n",
      "Matching for 895\n",
      "Matching for 896\n",
      "Matching for 897\n",
      "Matching for 898\n",
      "Matching for 899\n",
      "Matching for 900\n",
      "Matching for 901\n",
      "Matching for 902\n",
      "Matching for 903\n",
      "Matching for 904\n",
      "Matching for 905\n",
      "Matching for 906\n",
      "Matching for 907\n",
      "Matching for 908\n",
      "Matching for 909\n",
      "Matching for 910\n",
      "Matching for 911\n",
      "Matching for 912\n",
      "Matching for 913\n",
      "Matching for 914\n",
      "Matching for 915\n",
      "Matching for 916\n",
      "Matching for 917\n",
      "Matching for 918\n",
      "Matching for 926\n",
      "Matching for 927\n",
      "Matching for 928\n",
      "Matching for 929\n",
      "Matching for 930\n",
      "Matching for 931\n",
      "Matching for 932\n",
      "Matching for 933\n",
      "Matching for 934\n",
      "Matching for 936\n",
      "Matching for 937\n",
      "Matching for 938\n",
      "Matching for 939\n",
      "Matching for 940\n",
      "Matching for 941\n",
      "Matching for 942\n",
      "Matching for 943\n",
      "Matching for 944\n",
      "Matching for 945\n",
      "Matching for 948\n",
      "Matching for 949\n",
      "Matching for 950\n",
      "Matching for 951\n",
      "Matching for 952\n",
      "Matching for 953\n",
      "Matching for 954\n",
      "Matching for 955\n",
      "Matching for 956\n",
      "Matching for 957\n",
      "Matching for 958\n",
      "Matching for 959\n",
      "Matching for 960\n",
      "Matching for 961\n",
      "Matching for 962\n",
      "Matching for 963\n",
      "Matching for 964\n",
      "Matching for 965\n",
      "Matching for 966\n",
      "Matching for 967\n",
      "Matching for 968\n",
      "Matching for 969\n",
      "Matching for 970\n",
      "Matching for 971\n",
      "Matching for 972\n",
      "Matching for 973\n",
      "Matching for 974\n",
      "Matching for 975\n",
      "Matching for 976\n",
      "Matching for 977\n",
      "Matching for 978\n",
      "Matching for 979\n",
      "Matching for 980\n",
      "Matching for 981\n",
      "Matching for 982\n",
      "Matching for 983\n",
      "Matching for 984\n",
      "Matching for 985\n",
      "Matching for 986\n",
      "Matching for 987\n",
      "Matching for 988\n",
      "Matching for 989\n",
      "Matched DataFrame shape: (937, 74)\n",
      "Matching for 990\n",
      "Matched DataFrame shape: (997, 74)\n",
      "Matching for 991\n",
      "Matched DataFrame shape: (1115, 74)\n",
      "Matching for 992\n",
      "Matched DataFrame shape: (841, 74)\n",
      "Matching for 993\n",
      "Matched DataFrame shape: (1016, 74)\n",
      "Matching for 994\n",
      "Matching for 995\n",
      "Matched DataFrame shape: (1182, 74)\n",
      "Matching for 996\n",
      "Matched DataFrame shape: (915, 74)\n",
      "Matching for 997\n",
      "Matched DataFrame shape: (1239, 74)\n",
      "Matching for 998\n",
      "Matched DataFrame shape: (898, 74)\n",
      "Matching for 999\n",
      "Matched DataFrame shape: (1250, 74)\n",
      "Matching for 1000\n",
      "Matched DataFrame shape: (1230, 74)\n",
      "Matching for 1001\n",
      "Matched DataFrame shape: (690, 74)\n",
      "Matching for 1002\n",
      "Matched DataFrame shape: (925, 74)\n",
      "Matching for 1003\n",
      "Matched DataFrame shape: (1145, 74)\n",
      "Matching for 1004\n",
      "Matched DataFrame shape: (948, 74)\n",
      "Matching for 1005\n",
      "Matched DataFrame shape: (976, 74)\n",
      "Matching for 1006\n",
      "Matched DataFrame shape: (931, 74)\n",
      "Matching for 1007\n",
      "Matched DataFrame shape: (1242, 74)\n",
      "Matching for 1008\n",
      "Matched DataFrame shape: (1318, 74)\n",
      "Matching for 1009\n",
      "Matched DataFrame shape: (938, 74)\n",
      "Matching for 1010\n",
      "Matching for 1011\n",
      "Matched DataFrame shape: (1081, 74)\n",
      "Matching for 1012\n",
      "Matched DataFrame shape: (1046, 74)\n",
      "Matching for 1013\n",
      "Matched DataFrame shape: (947, 74)\n",
      "Matching for 1014\n",
      "Matched DataFrame shape: (1274, 74)\n",
      "Matching for 1015\n",
      "Matched DataFrame shape: (1489, 74)\n",
      "Matching for 1016\n",
      "Matched DataFrame shape: (1310, 74)\n",
      "Matching for 1017\n",
      "Matched DataFrame shape: (1036, 74)\n",
      "Matching for 1018\n",
      "Matched DataFrame shape: (1401, 74)\n",
      "Matching for 1019\n",
      "Matched DataFrame shape: (913, 74)\n",
      "Matching for 1020\n",
      "Matched DataFrame shape: (1054, 74)\n",
      "Matching for 1021\n",
      "Matched DataFrame shape: (1358, 74)\n",
      "Matching for 1022\n",
      "Matched DataFrame shape: (784, 74)\n",
      "Matching for 1023\n",
      "Matched DataFrame shape: (990, 74)\n",
      "Matching for 1024\n",
      "Matched DataFrame shape: (1162, 74)\n",
      "Matching for 1025\n",
      "Matched DataFrame shape: (900, 74)\n",
      "Matching for 1026\n",
      "Matched DataFrame shape: (986, 74)\n",
      "Matching for 1027\n",
      "Matched DataFrame shape: (1370, 74)\n",
      "Matching for 1028\n",
      "Matched DataFrame shape: (1030, 74)\n",
      "Matching for 1029\n",
      "Matched DataFrame shape: (1381, 74)\n",
      "Matching for 1030\n",
      "Matched DataFrame shape: (1075, 74)\n",
      "Matching for 1031\n",
      "Matched DataFrame shape: (1140, 74)\n",
      "Matching for 1032\n",
      "Matched DataFrame shape: (1226, 74)\n",
      "Matching for 1033\n",
      "Matched DataFrame shape: (1327, 74)\n",
      "Matching for 1034\n",
      "Matched DataFrame shape: (895, 74)\n",
      "Matching for 1035\n",
      "Matched DataFrame shape: (1025, 74)\n",
      "Matching for 1036\n",
      "Matched DataFrame shape: (1274, 74)\n",
      "Matching for 1037\n",
      "Matched DataFrame shape: (766, 74)\n",
      "Matching for 1038\n",
      "Matched DataFrame shape: (924, 74)\n",
      "Matching for 1039\n",
      "Matched DataFrame shape: (778, 74)\n",
      "Matching for 1040\n",
      "Matched DataFrame shape: (946, 74)\n",
      "Matching for 1041\n",
      "Matched DataFrame shape: (1017, 74)\n",
      "Matching for 1042\n",
      "Matched DataFrame shape: (1288, 74)\n",
      "Matching for 1043\n",
      "Matched DataFrame shape: (1128, 74)\n",
      "Matching for 1044\n",
      "Matched DataFrame shape: (1076, 74)\n",
      "Matching for 1045\n",
      "Matched DataFrame shape: (1016, 74)\n",
      "Matching for 1046\n",
      "Matched DataFrame shape: (1531, 74)\n",
      "Matching for 1047\n",
      "Matched DataFrame shape: (1043, 74)\n",
      "Matching for 1051\n",
      "Matched DataFrame shape: (1112, 74)\n",
      "Matching for 1052\n",
      "Matched DataFrame shape: (1026, 74)\n",
      "Matching for 1053\n",
      "Matched DataFrame shape: (1124, 74)\n",
      "Matching for 1054\n",
      "Matched DataFrame shape: (1244, 74)\n",
      "Matching for 1055\n",
      "Matched DataFrame shape: (1246, 74)\n",
      "Matching for 1056\n",
      "Matched DataFrame shape: (1418, 74)\n",
      "Matching for 1057\n",
      "Matched DataFrame shape: (941, 74)\n",
      "Matching for 1058\n",
      "Matched DataFrame shape: (1296, 74)\n",
      "Matching for 1059\n",
      "Matched DataFrame shape: (1051, 74)\n",
      "Matching for 1060\n",
      "Matched DataFrame shape: (1336, 74)\n",
      "Matching for 1061\n",
      "Matched DataFrame shape: (969, 74)\n",
      "Matching for 1062\n",
      "Matched DataFrame shape: (981, 74)\n",
      "Matching for 1063\n",
      "Matched DataFrame shape: (20, 74)\n",
      "Matching for 1064\n",
      "Matched DataFrame shape: (1361, 74)\n",
      "Matching for 1065\n",
      "Matched DataFrame shape: (889, 74)\n",
      "Matching for 1066\n",
      "Matched DataFrame shape: (1025, 74)\n",
      "Matching for 1067\n",
      "Matched DataFrame shape: (1147, 74)\n",
      "Matching for 1069\n",
      "Matched DataFrame shape: (1044, 74)\n",
      "Matching for 1070\n",
      "Matched DataFrame shape: (1259, 74)\n",
      "Matching for 1071\n",
      "Matched DataFrame shape: (1360, 74)\n",
      "Matching for 1072\n",
      "Matched DataFrame shape: (841, 74)\n",
      "Matching for 1073\n",
      "Matched DataFrame shape: (998, 74)\n",
      "Matching for 1074\n",
      "Matched DataFrame shape: (1123, 74)\n",
      "Matching for 1075\n",
      "Matched DataFrame shape: (816, 74)\n",
      "Matching for 1076\n",
      "Matched DataFrame shape: (1042, 74)\n",
      "Matching for 1077\n",
      "Matched DataFrame shape: (1131, 74)\n",
      "Matching for 1078\n",
      "Matched DataFrame shape: (1055, 74)\n",
      "Matching for 1079\n",
      "Matched DataFrame shape: (1230, 74)\n",
      "Matching for 1080\n",
      "Matched DataFrame shape: (1176, 74)\n",
      "Matching for 1081\n",
      "Matched DataFrame shape: (889, 74)\n",
      "Matching for 1082\n",
      "Matched DataFrame shape: (1262, 74)\n",
      "Matching for 1083\n",
      "Matched DataFrame shape: (811, 74)\n",
      "Matching for 1084\n",
      "Matched DataFrame shape: (1323, 74)\n",
      "Matching for 1085\n",
      "Matched DataFrame shape: (953, 74)\n",
      "Matching for 1086\n",
      "Matched DataFrame shape: (1382, 74)\n",
      "Matching for 1087\n",
      "Matched DataFrame shape: (790, 74)\n",
      "Matching for 1088\n",
      "Matched DataFrame shape: (1391, 74)\n",
      "Matching for 1089\n",
      "Matched DataFrame shape: (969, 74)\n",
      "Matching for 1091\n",
      "Matched DataFrame shape: (941, 74)\n",
      "Matching for 1092\n",
      "Matched DataFrame shape: (504, 74)\n",
      "Matching for 1093\n",
      "Matched DataFrame shape: (990, 74)\n",
      "Matching for 1094\n",
      "Matched DataFrame shape: (1378, 74)\n",
      "Matching for 1095\n",
      "Matched DataFrame shape: (1256, 74)\n",
      "Matching for 1096\n",
      "Matched DataFrame shape: (1117, 74)\n",
      "Matching for 1098\n",
      "Matched DataFrame shape: (1055, 74)\n",
      "Matching for 1099\n",
      "Matched DataFrame shape: (942, 74)\n",
      "Matching for 1100\n",
      "Matched DataFrame shape: (995, 74)\n",
      "Matching for 1101\n",
      "Matched DataFrame shape: (961, 74)\n",
      "Matching for 1102\n",
      "Matched DataFrame shape: (1138, 74)\n",
      "Matching for 1104\n",
      "Matched DataFrame shape: (1513, 74)\n",
      "Matching for 1105\n",
      "Matched DataFrame shape: (1312, 74)\n",
      "Matching for 1106\n",
      "Matched DataFrame shape: (1315, 74)\n",
      "Matching for 1107\n",
      "Matched DataFrame shape: (1353, 74)\n",
      "Matching for 1108\n",
      "Matched DataFrame shape: (970, 74)\n",
      "Matching for 1109\n",
      "Matched DataFrame shape: (1252, 74)\n",
      "Matching for 1110\n",
      "Matched DataFrame shape: (815, 74)\n",
      "Matching for 1111\n",
      "Matched DataFrame shape: (1341, 74)\n",
      "Matching for 1112\n",
      "Matched DataFrame shape: (955, 74)\n",
      "Matching for 1113\n",
      "Matched DataFrame shape: (1084, 74)\n",
      "Matching for 1114\n",
      "Matched DataFrame shape: (880, 74)\n",
      "Matching for 1115\n",
      "Matched DataFrame shape: (1005, 74)\n",
      "Matching for 1116\n",
      "Matched DataFrame shape: (1014, 74)\n",
      "Matching for 1117\n",
      "Matched DataFrame shape: (1280, 74)\n",
      "Matching for 1118\n",
      "Matched DataFrame shape: (1106, 74)\n",
      "Matching for 1119\n",
      "Matched DataFrame shape: (943, 74)\n",
      "Matching for 1120\n",
      "Matched DataFrame shape: (1157, 74)\n",
      "Matching for 1121\n",
      "Matching for 1122\n",
      "Matching for 1123\n",
      "Matching for 1124\n",
      "Matching for 1125\n",
      "Matching for 1126\n",
      "Matching for 1127\n",
      "Matching for 1128\n",
      "Matching for 1129\n",
      "Matching for 1130\n",
      "Matching for 1131\n",
      "Matching for 1132\n",
      "Matching for 1133\n",
      "Matching for 1134\n",
      "Matching for 1135\n",
      "Matching for 1136\n",
      "Matching for 1137\n",
      "Matching for 1138\n",
      "Matching for 1139\n",
      "Matching for 1140\n",
      "Matching for 1141\n",
      "(586171, 76)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_19862/4217053995.py:681: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['TrackStatus'].fillna(1, inplace=True)  # 1 = regular racing status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NaN counts in required columns:\n",
      "tire_age: 133323 NaN values (22.74% of rows)\n",
      "fuel_load: 133323 NaN values (22.74% of rows)\n",
      "tire_compound: 11101 NaN values (1.89% of rows)\n",
      "(451356, 88)\n",
      "(451356, 58)\n",
      "Enhanced laps DataFrame saved to 'enhanced_laps_before_training.csv'\n",
      "Epoch 1/100:\n",
      "Train Loss: 31112518.0000, Val Loss: 340683.9375, Train MAE: 4373.87 ms, Val MAE: 583.68 ms\n",
      "Epoch 2/100:\n",
      "Train Loss: 31255954.0000, Val Loss: 989.1229, Train MAE: 4651.10 ms, Val MAE: 31.44 ms\n",
      "Epoch 3/100:\n",
      "Train Loss: 31863176.0000, Val Loss: 214527.6562, Train MAE: 4250.26 ms, Val MAE: 463.17 ms\n",
      "Epoch 4/100:\n",
      "Train Loss: 32338770.0000, Val Loss: 423561.0000, Train MAE: 4153.52 ms, Val MAE: 650.82 ms\n",
      "Epoch 5/100:\n",
      "Train Loss: 18023134.0000, Val Loss: 487934.0625, Train MAE: 3355.22 ms, Val MAE: 698.52 ms\n",
      "Epoch 6/100:\n",
      "Train Loss: 28255366.0000, Val Loss: 488731.0625, Train MAE: 3987.97 ms, Val MAE: 699.09 ms\n",
      "Epoch 7/100:\n",
      "Train Loss: 12530812.0000, Val Loss: 509798.0938, Train MAE: 3015.23 ms, Val MAE: 714.00 ms\n",
      "Epoch 8/100:\n",
      "Train Loss: 24614868.0000, Val Loss: 414044.0625, Train MAE: 4031.68 ms, Val MAE: 643.46 ms\n",
      "Epoch 9/100:\n",
      "Train Loss: 18485592.0000, Val Loss: 283292.3125, Train MAE: 3762.90 ms, Val MAE: 532.25 ms\n",
      "Epoch 10/100:\n",
      "Train Loss: 14825837.0000, Val Loss: 244462.8281, Train MAE: 3160.74 ms, Val MAE: 494.43 ms\n",
      "Epoch 11/100:\n",
      "Train Loss: 15021337.0000, Val Loss: 173351.5781, Train MAE: 3055.02 ms, Val MAE: 416.35 ms\n",
      "Epoch 12/100:\n",
      "Train Loss: 12414559.0000, Val Loss: 107562.5469, Train MAE: 2899.26 ms, Val MAE: 327.97 ms\n",
      "Epoch 13/100:\n",
      "Train Loss: 25762060.0000, Val Loss: 49012.8320, Train MAE: 3723.56 ms, Val MAE: 221.39 ms\n",
      "Epoch 14/100:\n",
      "Train Loss: 8050493.0000, Val Loss: 12582.5605, Train MAE: 2579.59 ms, Val MAE: 112.17 ms\n",
      "Epoch 15/100:\n",
      "Train Loss: 7906188.5000, Val Loss: 146.8114, Train MAE: 2512.27 ms, Val MAE: 12.10 ms\n",
      "Epoch 16/100:\n",
      "Train Loss: 8305307.0000, Val Loss: 4811.0859, Train MAE: 2314.63 ms, Val MAE: 69.36 ms\n",
      "Epoch 17/100:\n",
      "Train Loss: 8762935.0000, Val Loss: 20010.6504, Train MAE: 2333.06 ms, Val MAE: 141.46 ms\n",
      "Epoch 18/100:\n",
      "Train Loss: 7853251.0000, Val Loss: 40947.1992, Train MAE: 2127.50 ms, Val MAE: 202.35 ms\n",
      "Epoch 19/100:\n",
      "Train Loss: 5367063.0000, Val Loss: 60646.8125, Train MAE: 1749.90 ms, Val MAE: 246.26 ms\n",
      "Epoch 20/100:\n",
      "Train Loss: 6160734.5000, Val Loss: 67223.7969, Train MAE: 1962.55 ms, Val MAE: 259.27 ms\n",
      "Epoch 21/100:\n",
      "Train Loss: 3960945.5000, Val Loss: 74210.8125, Train MAE: 1610.00 ms, Val MAE: 272.42 ms\n",
      "Epoch 22/100:\n",
      "Train Loss: 5010088.5000, Val Loss: 79684.5859, Train MAE: 1771.34 ms, Val MAE: 282.28 ms\n",
      "Epoch 23/100:\n",
      "Train Loss: 4921338.5000, Val Loss: 82985.7266, Train MAE: 1595.69 ms, Val MAE: 288.07 ms\n",
      "Epoch 24/100:\n",
      "Train Loss: 5178517.0000, Val Loss: 88511.5859, Train MAE: 1996.17 ms, Val MAE: 297.51 ms\n",
      "Epoch 25/100:\n",
      "Train Loss: 7907702.5000, Val Loss: 86688.6016, Train MAE: 2386.54 ms, Val MAE: 294.43 ms\n",
      "Epoch 26/100:\n",
      "Train Loss: 2344008.2500, Val Loss: 86735.0781, Train MAE: 1166.91 ms, Val MAE: 294.51 ms\n",
      "Epoch 27/100:\n",
      "Train Loss: 4065902.2500, Val Loss: 88122.2656, Train MAE: 1551.99 ms, Val MAE: 296.85 ms\n",
      "Epoch 28/100:\n",
      "Train Loss: 5894464.5000, Val Loss: 90034.8828, Train MAE: 1882.10 ms, Val MAE: 300.06 ms\n",
      "Epoch 29/100:\n",
      "Train Loss: 6233032.0000, Val Loss: 96789.0000, Train MAE: 1738.28 ms, Val MAE: 311.11 ms\n",
      "Epoch 30/100:\n",
      "Train Loss: 6455159.5000, Val Loss: 88581.7344, Train MAE: 1817.80 ms, Val MAE: 297.63 ms\n",
      "Epoch 31/100:\n",
      "Train Loss: 3435874.2500, Val Loss: 80239.8594, Train MAE: 1522.08 ms, Val MAE: 283.27 ms\n",
      "Epoch 32/100:\n",
      "Train Loss: 2367359.5000, Val Loss: 72179.5781, Train MAE: 1130.65 ms, Val MAE: 268.66 ms\n",
      "Epoch 33/100:\n",
      "Train Loss: 5802166.0000, Val Loss: 64135.0547, Train MAE: 1917.45 ms, Val MAE: 253.25 ms\n",
      "Epoch 34/100:\n",
      "Train Loss: 3023018.0000, Val Loss: 55484.1250, Train MAE: 1332.40 ms, Val MAE: 235.55 ms\n",
      "Epoch 35/100:\n",
      "Train Loss: 1636386.3750, Val Loss: 48848.7070, Train MAE: 945.43 ms, Val MAE: 221.02 ms\n",
      "Epoch 36/100:\n",
      "Train Loss: 3180252.2500, Val Loss: 42155.8828, Train MAE: 1406.79 ms, Val MAE: 205.32 ms\n",
      "Epoch 37/100:\n",
      "Train Loss: 3532735.7500, Val Loss: 35275.5547, Train MAE: 1483.46 ms, Val MAE: 187.82 ms\n",
      "Epoch 38/100:\n",
      "Train Loss: 3647198.5000, Val Loss: 32436.4180, Train MAE: 1608.94 ms, Val MAE: 180.10 ms\n",
      "Epoch 39/100:\n",
      "Train Loss: 2549922.2500, Val Loss: 29577.3594, Train MAE: 1336.07 ms, Val MAE: 171.98 ms\n",
      "Epoch 40/100:\n",
      "Train Loss: 1225115.2500, Val Loss: 26473.4844, Train MAE: 960.46 ms, Val MAE: 162.71 ms\n",
      "Epoch 41/100:\n",
      "Train Loss: 1670262.8750, Val Loss: 23419.8750, Train MAE: 1004.13 ms, Val MAE: 153.03 ms\n",
      "Epoch 42/100:\n",
      "Train Loss: 3235027.0000, Val Loss: 19670.1484, Train MAE: 1244.69 ms, Val MAE: 140.25 ms\n",
      "Epoch 43/100:\n",
      "Train Loss: 2461369.0000, Val Loss: 14548.8926, Train MAE: 1199.57 ms, Val MAE: 120.62 ms\n",
      "Epoch 44/100:\n",
      "Train Loss: 1496807.5000, Val Loss: 9600.2588, Train MAE: 839.72 ms, Val MAE: 97.98 ms\n",
      "Epoch 45/100:\n",
      "Train Loss: 2901569.0000, Val Loss: 4970.2798, Train MAE: 1342.99 ms, Val MAE: 70.50 ms\n",
      "Epoch 46/100:\n",
      "Train Loss: 2011894.8750, Val Loss: 2188.8484, Train MAE: 1085.86 ms, Val MAE: 46.78 ms\n",
      "Epoch 47/100:\n",
      "Train Loss: 1177034.0000, Val Loss: 744.9193, Train MAE: 896.77 ms, Val MAE: 27.29 ms\n",
      "Epoch 48/100:\n",
      "Train Loss: 2065365.5000, Val Loss: 105.3867, Train MAE: 1012.64 ms, Val MAE: 10.25 ms\n",
      "Epoch 49/100:\n",
      "Train Loss: 1293349.8750, Val Loss: 7.6068, Train MAE: 876.19 ms, Val MAE: 2.69 ms\n",
      "Epoch 50/100:\n",
      "Train Loss: 1283380.3750, Val Loss: 255.3989, Train MAE: 917.30 ms, Val MAE: 15.97 ms\n",
      "Epoch 51/100:\n",
      "Train Loss: 2529089.5000, Val Loss: 581.2509, Train MAE: 1074.35 ms, Val MAE: 24.10 ms\n",
      "Epoch 52/100:\n",
      "Train Loss: 2745933.0000, Val Loss: 711.1366, Train MAE: 1245.73 ms, Val MAE: 26.66 ms\n",
      "Epoch 53/100:\n",
      "Train Loss: 1364955.0000, Val Loss: 600.1024, Train MAE: 986.31 ms, Val MAE: 24.49 ms\n",
      "Epoch 54/100:\n",
      "Train Loss: 3512676.2500, Val Loss: 337.8091, Train MAE: 1278.68 ms, Val MAE: 18.37 ms\n",
      "Epoch 55/100:\n",
      "Train Loss: 2009242.8750, Val Loss: 129.4994, Train MAE: 1158.65 ms, Val MAE: 11.36 ms\n",
      "Epoch 56/100:\n",
      "Train Loss: 1844326.8750, Val Loss: 18.2003, Train MAE: 1067.35 ms, Val MAE: 4.22 ms\n",
      "Epoch 57/100:\n",
      "Train Loss: 1773846.7500, Val Loss: 10.5281, Train MAE: 1072.99 ms, Val MAE: 3.18 ms\n",
      "Epoch 58/100:\n",
      "Train Loss: 1254806.1250, Val Loss: 133.2330, Train MAE: 820.88 ms, Val MAE: 11.53 ms\n",
      "Epoch 59/100:\n",
      "Train Loss: 735414.4375, Val Loss: 409.3034, Train MAE: 675.91 ms, Val MAE: 20.22 ms\n",
      "Epoch 60/100:\n",
      "Train Loss: 1607163.8750, Val Loss: 904.1246, Train MAE: 966.41 ms, Val MAE: 30.06 ms\n",
      "Epoch 61/100:\n",
      "Train Loss: 1590769.5000, Val Loss: 1343.2762, Train MAE: 923.25 ms, Val MAE: 36.65 ms\n",
      "Epoch 62/100:\n",
      "Train Loss: 1152060.8750, Val Loss: 1766.0934, Train MAE: 814.20 ms, Val MAE: 42.02 ms\n",
      "Epoch 63/100:\n",
      "Train Loss: 2360859.7500, Val Loss: 2247.9705, Train MAE: 1148.76 ms, Val MAE: 47.41 ms\n",
      "Epoch 64/100:\n",
      "Train Loss: 1558738.7500, Val Loss: 2542.6240, Train MAE: 853.24 ms, Val MAE: 50.42 ms\n",
      "Epoch 65/100:\n",
      "Train Loss: 1010354.1875, Val Loss: 2740.3650, Train MAE: 827.75 ms, Val MAE: 52.34 ms\n",
      "Epoch 66/100:\n",
      "Train Loss: 914589.0625, Val Loss: 2975.2593, Train MAE: 799.87 ms, Val MAE: 54.54 ms\n",
      "Epoch 67/100:\n",
      "Train Loss: 1091597.7500, Val Loss: 3143.6514, Train MAE: 722.75 ms, Val MAE: 56.06 ms\n",
      "Epoch 68/100:\n",
      "Train Loss: 1399280.8750, Val Loss: 3419.8333, Train MAE: 883.68 ms, Val MAE: 58.48 ms\n",
      "Epoch 69/100:\n",
      "Train Loss: 1364676.8750, Val Loss: 3785.6978, Train MAE: 823.74 ms, Val MAE: 61.52 ms\n",
      "Epoch 70/100:\n",
      "Train Loss: 845173.7500, Val Loss: 4081.8137, Train MAE: 756.06 ms, Val MAE: 63.89 ms\n",
      "Epoch 71/100:\n",
      "Train Loss: 1646548.7500, Val Loss: 4466.6553, Train MAE: 1013.56 ms, Val MAE: 66.83 ms\n",
      "Epoch 72/100:\n",
      "Train Loss: 1000373.3750, Val Loss: 4667.5947, Train MAE: 790.07 ms, Val MAE: 68.32 ms\n",
      "Epoch 73/100:\n",
      "Train Loss: 732455.2500, Val Loss: 4858.5967, Train MAE: 737.24 ms, Val MAE: 69.70 ms\n",
      "Epoch 74/100:\n",
      "Train Loss: 1610231.2500, Val Loss: 4962.6377, Train MAE: 868.90 ms, Val MAE: 70.44 ms\n",
      "Epoch 75/100:\n",
      "Train Loss: 1316736.1250, Val Loss: 4954.5854, Train MAE: 867.78 ms, Val MAE: 70.39 ms\n",
      "Epoch 76/100:\n",
      "Train Loss: 494793.1875, Val Loss: 4965.9766, Train MAE: 567.35 ms, Val MAE: 70.47 ms\n",
      "Epoch 77/100:\n",
      "Train Loss: 876827.6250, Val Loss: 4809.7627, Train MAE: 713.08 ms, Val MAE: 69.35 ms\n",
      "Epoch 78/100:\n",
      "Train Loss: 1005681.0000, Val Loss: 4356.9922, Train MAE: 737.27 ms, Val MAE: 66.00 ms\n",
      "Epoch 79/100:\n",
      "Train Loss: 2144418.2500, Val Loss: 4146.3257, Train MAE: 1231.63 ms, Val MAE: 64.39 ms\n",
      "Epoch 80/100:\n",
      "Train Loss: 1293049.8750, Val Loss: 3808.3618, Train MAE: 921.16 ms, Val MAE: 61.71 ms\n",
      "Epoch 81/100:\n",
      "Train Loss: 1101962.5000, Val Loss: 3488.0444, Train MAE: 880.40 ms, Val MAE: 59.06 ms\n",
      "Epoch 82/100:\n",
      "Train Loss: 1043072.6250, Val Loss: 3106.1575, Train MAE: 829.07 ms, Val MAE: 55.73 ms\n",
      "Epoch 83/100:\n",
      "Train Loss: 859852.0000, Val Loss: 2745.5657, Train MAE: 767.36 ms, Val MAE: 52.39 ms\n",
      "Epoch 84/100:\n",
      "Train Loss: 576869.9375, Val Loss: 2551.4907, Train MAE: 584.24 ms, Val MAE: 50.51 ms\n",
      "Epoch 85/100:\n",
      "Train Loss: 1209248.7500, Val Loss: 2159.1025, Train MAE: 887.04 ms, Val MAE: 46.46 ms\n",
      "Epoch 86/100:\n",
      "Train Loss: 1108028.3750, Val Loss: 1798.4281, Train MAE: 816.24 ms, Val MAE: 42.40 ms\n",
      "Epoch 87/100:\n",
      "Train Loss: 684596.3750, Val Loss: 1542.0210, Train MAE: 570.36 ms, Val MAE: 39.26 ms\n",
      "Epoch 88/100:\n",
      "Train Loss: 1150960.0000, Val Loss: 1320.3679, Train MAE: 838.09 ms, Val MAE: 36.33 ms\n",
      "Epoch 89/100:\n",
      "Train Loss: 832545.0000, Val Loss: 1128.6777, Train MAE: 677.69 ms, Val MAE: 33.59 ms\n",
      "Epoch 90/100:\n",
      "Train Loss: 695682.6875, Val Loss: 1015.3244, Train MAE: 748.82 ms, Val MAE: 31.86 ms\n",
      "Epoch 91/100:\n",
      "Train Loss: 612939.6250, Val Loss: 878.3830, Train MAE: 651.96 ms, Val MAE: 29.63 ms\n",
      "Epoch 92/100:\n",
      "Train Loss: 1152378.8750, Val Loss: 705.7872, Train MAE: 833.89 ms, Val MAE: 26.56 ms\n",
      "Epoch 93/100:\n",
      "Train Loss: 1217076.8750, Val Loss: 551.6841, Train MAE: 875.95 ms, Val MAE: 23.48 ms\n",
      "Epoch 94/100:\n",
      "Train Loss: 1077517.1250, Val Loss: 439.3060, Train MAE: 821.92 ms, Val MAE: 20.95 ms\n",
      "Epoch 95/100:\n",
      "Train Loss: 308754.6875, Val Loss: 352.2325, Train MAE: 485.10 ms, Val MAE: 18.76 ms\n",
      "Epoch 96/100:\n",
      "Train Loss: 734893.7500, Val Loss: 270.0546, Train MAE: 625.37 ms, Val MAE: 16.42 ms\n",
      "Epoch 97/100:\n",
      "Train Loss: 703894.4375, Val Loss: 198.7696, Train MAE: 693.23 ms, Val MAE: 14.08 ms\n",
      "Epoch 98/100:\n",
      "Train Loss: 641505.5625, Val Loss: 156.9701, Train MAE: 659.39 ms, Val MAE: 12.51 ms\n",
      "Epoch 99/100:\n",
      "Train Loss: 750445.5625, Val Loss: 102.5389, Train MAE: 737.55 ms, Val MAE: 10.11 ms\n",
      "Epoch 100/100:\n",
      "Train Loss: 983330.4375, Val Loss: 53.3151, Train MAE: 765.65 ms, Val MAE: 7.27 ms\n",
      "Model and preprocessor saved to f1_prediction_model.pth\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Define the RaceFeatures dataclass\n",
    "@dataclass\n",
    "class RaceFeatures:\n",
    "    \"\"\"Data structure for race features\"\"\"\n",
    "    static_features: List[str] = field(default_factory=lambda: [\n",
    "        'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "        'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "        'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time'\n",
    "    ])\n",
    "    \n",
    "    dynamic_features: List[str] = field(default_factory=lambda: [\n",
    "        'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "        'air_temp', 'humidity', 'tire_compound', 'TrackStatus', 'is_pit_lap'\n",
    "    ])\n",
    "    \n",
    "    target: str = 'milliseconds'\n",
    "\n",
    "# Define the F1Dataset class\n",
    "class F1Dataset(Dataset):\n",
    "    def __init__(self, sequences, static_features, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.static_features = torch.FloatTensor(static_features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.sequences[idx],\n",
    "            'static': self.static_features[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "\n",
    "# Define the F1DataPreprocessor class\n",
    "class F1DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.static_scaler = StandardScaler()\n",
    "        self.dynamic_scaler = StandardScaler()\n",
    "        self.lap_time_scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_sequence_data(self, df: pd.DataFrame, window_size: int = 3) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare sequential data with sliding window and apply scaling\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        static_features = []\n",
    "        targets = []\n",
    "        \n",
    "        # Instantiate RaceFeatures\n",
    "        race_features = RaceFeatures()\n",
    "        \n",
    "        # Sort the dataframe to ensure consistent ordering\n",
    "        df = df.sort_values(['raceId', 'driverId', 'lap'])\n",
    "        \n",
    "        # Group by race and driver\n",
    "        for (race_id, driver_id), group in df.groupby(['raceId', 'driverId']):\n",
    "            group = group.sort_values('lap')\n",
    "            \n",
    "            # Extract static features (assumed to be constant per driver per race)\n",
    "            static = group[race_features.static_features].iloc[0].values\n",
    "            static_features.append(static)\n",
    "            \n",
    "            # Extract dynamic features and target\n",
    "            lap_times = group[race_features.target].values.reshape(-1, 1)  # Shape: (num_laps, 1)\n",
    "            dynamic = group[race_features.dynamic_features].values  # Shape: (num_laps, num_dynamic_features)\n",
    "            \n",
    "            # Apply scaling\n",
    "            # Note: Scalers should be fitted on the training data to prevent data leakage.\n",
    "            # Here, for simplicity, we're fitting on the entire dataset. For a real-world scenario,\n",
    "            # consider splitting the data first before fitting the scalers.\n",
    "            dynamic_features_to_scale = [col for col in race_features.dynamic_features if col != 'tire_compound']\n",
    "            tire_compounds = dynamic[:, race_features.dynamic_features.index('tire_compound')].reshape(-1, 1)\n",
    "            other_dynamic = dynamic[:, [race_features.dynamic_features.index(col) for col in dynamic_features_to_scale]]\n",
    "            \n",
    "            lap_times_scaled = self.lap_time_scaler.fit_transform(lap_times).flatten()\n",
    "            other_dynamic_scaled = self.dynamic_scaler.fit_transform(other_dynamic)\n",
    "            static_scaled = self.static_scaler.fit_transform(static.reshape(1, -1)).flatten()\n",
    "            \n",
    "            dynamic_scaled = np.hstack((tire_compounds, other_dynamic_scaled))\n",
    "            \n",
    "            # Create sequences\n",
    "            # Create sequences\n",
    "        for i in range(len(lap_times_scaled) - window_size):\n",
    "            sequence_lap_times = lap_times_scaled[i:i+window_size].reshape(-1, 1)  # Shape: (window_size, 1)\n",
    "            sequence_dynamic = dynamic_scaled[i:i+window_size]  # Shape: (window_size, num_dynamic_features)\n",
    "            sequence = np.hstack((sequence_lap_times, sequence_dynamic))  # Shape: (window_size, 1 + num_dynamic_features)\n",
    "            sequences.append(sequence)\n",
    "            static_features.append(static_scaled)\n",
    "            targets.append(lap_times_scaled[i + window_size])\n",
    "        \n",
    "        return (np.array(sequences), \n",
    "                np.array(static_features), \n",
    "                np.array(targets))\n",
    "\n",
    "    \n",
    "    def create_train_val_loaders(\n",
    "        self, \n",
    "        sequences: np.ndarray, \n",
    "        static_features: np.ndarray, \n",
    "        targets: np.ndarray,\n",
    "        batch_size: int = 32,\n",
    "        val_split: float = 0.2\n",
    "    ) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create train and validation dataloaders with given split ratio\n",
    "        \"\"\"\n",
    "        dataset = F1Dataset(sequences, static_features, targets)\n",
    "        \n",
    "        # Calculate lengths for split\n",
    "        val_size = int(len(dataset) * val_split)\n",
    "        train_size = len(dataset) - val_size\n",
    "        \n",
    "        # Split dataset\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "class F1PredictionModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 sequence_dim: int,\n",
    "                 static_dim: int,\n",
    "                 hidden_dim: int = 64,\n",
    "                 num_layers: int = 2,\n",
    "                 dropout_prob: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTM for sequential features with dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=sequence_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Static features processing with dropout\n",
    "        self.static_network = nn.Sequential(\n",
    "            nn.Linear(static_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Combine everything\n",
    "        self.final_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence, static):\n",
    "        # Process sequence through LSTM\n",
    "        lstm_out, _ = self.lstm(sequence)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Output of the last time step\n",
    "        \n",
    "        # Process static features\n",
    "        static_out = self.static_network(static)\n",
    "        \n",
    "        # Combine LSTM output and static features\n",
    "        combined = torch.cat([lstm_out, static_out], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.final_network(combined)\n",
    "        \n",
    "        return prediction.squeeze()\n",
    "\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                epochs: int = 10,\n",
    "                learning_rate: float = 0.001,\n",
    "                lap_time_scaler: StandardScaler = None,  # Pass the lap time scaler\n",
    "                device: Optional[str] = None) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train the model and return training history including MAE in milliseconds\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_maes = []\n",
    "        for batch in train_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            static = batch['static'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(sequences, static)\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Calculate MAE in normalized scale\n",
    "            mae = torch.mean(torch.abs(predictions - targets)).item()\n",
    "            train_maes.append(mae)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_maes = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch['sequence'].to(device)\n",
    "                static = batch['static'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                predictions = model(sequences, static)\n",
    "                loss = criterion(predictions, targets)\n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Calculate MAE in normalized scale\n",
    "                mae_normalized = torch.mean(torch.abs(predictions - targets)).item()\n",
    "                val_maes.append(mae_normalized)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        train_mae_normalized = np.mean(train_maes)\n",
    "        val_mae_normalized = np.mean(val_maes)\n",
    "        \n",
    "        # Convert MAE back to milliseconds using the inverse scaler\n",
    "        if lap_time_scaler:\n",
    "            train_mae_ms = lap_time_scaler.inverse_transform([[train_mae_normalized]])[0][0]\n",
    "            val_mae_ms = lap_time_scaler.inverse_transform([[val_mae_normalized]])[0][0]\n",
    "        else:\n",
    "            train_mae_ms, val_mae_ms = train_mae_normalized, val_mae_normalized\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mae'].append(train_mae_ms)\n",
    "        history['val_mae'].append(val_mae_ms)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train MAE: {train_mae_ms:.2f} ms, Val MAE: {val_mae_ms:.2f} ms')\n",
    "    \n",
    "    return history\n",
    "\n",
    "def predict_with_uncertainty(model, inputs, n_samples=100):\n",
    "    model.train()  # Enable dropout layers\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            prediction = model(**inputs).cpu().numpy()\n",
    "            predictions.append(prediction)\n",
    "    predictions = np.array(predictions)\n",
    "    mean_prediction = predictions.mean(axis=0)\n",
    "    std_prediction = predictions.std(axis=0)\n",
    "    return mean_prediction, std_prediction\n",
    "\n",
    "# Define a function to save the model\n",
    "def save_model_with_preprocessor(model, preprocessor, sequence_dim, static_dim, path: str):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'lap_time_scaler': preprocessor.lap_time_scaler,\n",
    "        'dynamic_scaler': preprocessor.dynamic_scaler, \n",
    "        'static_scaler': preprocessor.static_scaler,\n",
    "        'sequence_dim': sequence_dim,\n",
    "        'static_dim': static_dim\n",
    "    }, path)\n",
    "    print(f\"Model and preprocessor saved to {path}\")\n",
    "\n",
    "\n",
    "# Now, integrate your code snippets into data preprocessing\n",
    "def load_and_preprocess_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from CSV files and preprocess it to create the enhanced_laps DataFrame.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    na_values = ['\\\\N', 'NaN', '']\n",
    "    lap_times = pd.read_csv('../../data/raw_data/lap_times.csv', na_values=na_values)\n",
    "    drivers = pd.read_csv('../../data/raw_data/drivers.csv', na_values=na_values)\n",
    "    races = pd.read_csv('../../data/raw_data/races.csv', na_values=na_values)\n",
    "    circuits = pd.read_csv('../../data/raw_data/circuits.csv', na_values=na_values)\n",
    "    pit_stops = pd.read_csv('../../data/raw_data/pit_stops.csv', na_values=na_values)\n",
    "    pit_stops.rename(columns={'milliseconds' : 'pitstop_milliseconds'}, inplace=True)\n",
    "    results = pd.read_csv('../../data/raw_data/results.csv', na_values=na_values)\n",
    "    results.rename(columns={'milliseconds' : 'racetime_milliseconds'}, inplace=True)\n",
    "\n",
    "    qualifying = pd.read_csv('../../data/raw_data/qualifying.csv', na_values=na_values)\n",
    "    status = pd.read_csv('../../data/raw_data/status.csv', na_values=na_values)\n",
    "    weather_data = pd.read_csv('../../data/raw_data/ff1_weather.csv', na_values=na_values)\n",
    "    practice_sessions = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "    # Load the tire data\n",
    "    tire_data = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "\n",
    "    # Convert date columns to datetime\n",
    "    races['date'] = pd.to_datetime(races['date'])\n",
    "    results['date'] = results['raceId'].map(races.set_index('raceId')['date'])\n",
    "    lap_times['date'] = lap_times['raceId'].map(races.set_index('raceId')['date'])\n",
    "    \n",
    "    # Merge dataframes\n",
    "    laps = lap_times.merge(drivers, on='driverId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(races, on='raceId', how='left', suffixes=('', '_race'))\n",
    "    laps.rename(columns={'quali_time' : 'quali_date_time'}, inplace=True)\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(circuits, on='circuitId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(results[['raceId', 'driverId', 'positionOrder', 'grid', 'racetime_milliseconds', 'fastestLap', 'statusId']], on=['raceId', 'driverId'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(status, on='statusId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(pit_stops[['raceId', 'driverId', 'lap', 'pitstop_milliseconds']], on=['raceId', 'driverId', 'lap'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps['pitstop_milliseconds'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Add weather information\n",
    "    # Filter weather data to include only the Race session\n",
    "    weather_data = weather_data[weather_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge weather data with races to get raceId\n",
    "    weather_data = weather_data.merge(\n",
    "        races[['raceId', 'year', 'name']], \n",
    "        left_on=['EventName', 'Year'],\n",
    "        right_on=['name', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Compute cumulative time from the start of the race for each driver\n",
    "    laps.sort_values(['raceId', 'driverId', 'lap'], inplace=True)\n",
    "    laps['cumulative_milliseconds'] = laps.groupby(['raceId', 'driverId'])['milliseconds'].cumsum()\n",
    "    laps['seconds_from_start'] = laps['cumulative_milliseconds'] / 1000\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Use 'Time' in weather_data as 'seconds_from_start'\n",
    "    weather_data['seconds_from_start'] = weather_data['Time']\n",
    "    \n",
    "    # Standardize text data\n",
    "    tire_data['Compound'] = tire_data['Compound'].str.upper()\n",
    "    tire_data['EventName'] = tire_data['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Filter for race sessions only\n",
    "    tire_data = tire_data[tire_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge with races to get raceId\n",
    "    tire_data = tire_data.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    tire_data['Driver'] = tire_data['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    tire_data['driverId'] = tire_data['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Rename 'LapNumber' to 'lap' and ensure integer type\n",
    "    tire_data.rename(columns={'LapNumber': 'lap'}, inplace=True)\n",
    "    tire_data['lap'] = tire_data['lap'].astype(int)\n",
    "    laps['lap'] = laps['lap'].astype(int)\n",
    "    \n",
    "    # Create compound mapping (ordered from hardest to softest)\n",
    "    compound_mapping = {\n",
    "        'UNKNOWN': 0,\n",
    "        'HARD': 1,\n",
    "        'MEDIUM': 2,\n",
    "        'SOFT': 3,\n",
    "        'INTERMEDIATE': 4,\n",
    "        'WET': 5\n",
    "    }\n",
    "    \n",
    "    # Merge tire_data with laps\n",
    "    laps = laps.merge(\n",
    "        tire_data[['raceId', 'driverId', 'lap', 'Compound', 'TrackStatus']],\n",
    "        on=['raceId', 'driverId', 'lap'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Handle missing compounds and apply numeric encoding\n",
    "    laps['Compound'].fillna('UNKNOWN', inplace=True)\n",
    "    laps['tire_compound'] = laps['Compound'].map(compound_mapping)\n",
    "    \n",
    "    # Drop the original Compound column if desired\n",
    "    laps.drop('Compound', axis=1, inplace=True)\n",
    "    \n",
    "    # Standardize names\n",
    "    practice_sessions['EventName'] = practice_sessions['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Merge practice_sessions with races to get raceId\n",
    "    practice_sessions = practice_sessions.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    practice_sessions['Driver'] = practice_sessions['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    practice_sessions['driverId'] = practice_sessions['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Convert LapTime to milliseconds\n",
    "    practice_sessions['LapTime_ms'] = practice_sessions['LapTime'].apply(lambda x: pd.to_timedelta(x).total_seconds() * 1000)\n",
    "    \n",
    "    # Calculate median lap times for each driver in each session\n",
    "    session_medians = practice_sessions.groupby(['raceId', 'driverId', 'SessionName'])['LapTime_ms'].median().reset_index()\n",
    "    \n",
    "    # Pivot the data to have sessions as columns\n",
    "    session_medians_pivot = session_medians.pivot_table(\n",
    "        index=['raceId', 'driverId'],\n",
    "        columns='SessionName',\n",
    "        values='LapTime_ms'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    session_medians_pivot.rename(columns={\n",
    "        'FP1': 'fp1_median_time',\n",
    "        'FP2': 'fp2_median_time',\n",
    "        'FP3': 'fp3_median_time',\n",
    "        'Q': 'quali_time'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    laps = laps.merge(\n",
    "    session_medians_pivot,\n",
    "    on=['raceId', 'driverId'],\n",
    "    how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing practice times with global median or a placeholder value\n",
    "    global_median_fp1 = laps['fp1_median_time'].median()\n",
    "    laps['fp1_median_time'].fillna(global_median_fp1, inplace=True)\n",
    "    \n",
    "    # Repeat for other sessions\n",
    "    global_median_fp2 = laps['fp2_median_time'].median()\n",
    "    laps['fp2_median_time'].fillna(global_median_fp2, inplace=True)\n",
    "    \n",
    "    global_median_fp3 = laps['fp3_median_time'].median()\n",
    "    laps['fp3_median_time'].fillna(global_median_fp3, inplace=True)\n",
    "    \n",
    "    global_median_quali = laps['quali_time'].median()\n",
    "    laps['quali_time'].fillna(global_median_quali, inplace=True)\n",
    "\n",
    "    \n",
    "    # Create a binary indicator for pit stops\n",
    "    laps['is_pit_lap'] = laps['pitstop_milliseconds'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    \n",
    "    # Define a function to match weather data to laps\n",
    "    def match_weather_to_lap(race_laps, race_weather):\n",
    "        \"\"\"\n",
    "        For each lap, find the closest weather measurement in time\n",
    "        \"\"\"\n",
    "        race_laps = race_laps.sort_values('seconds_from_start')\n",
    "        race_weather = race_weather.sort_values('seconds_from_start')\n",
    "        merged = pd.merge_asof(\n",
    "            race_laps,\n",
    "            race_weather,\n",
    "            on='seconds_from_start',\n",
    "            direction='nearest'\n",
    "        )\n",
    "        return merged\n",
    "\n",
    "    # Apply matching per race\n",
    "    matched_laps_list = []\n",
    "    for race_id in laps['raceId'].unique():\n",
    "        print(f'Matching for {race_id}')\n",
    "        race_laps = laps[laps['raceId'] == race_id]\n",
    "        race_weather = weather_data[weather_data['raceId'] == race_id]\n",
    "        \n",
    "        if not race_weather.empty:\n",
    "            matched = match_weather_to_lap(race_laps, race_weather)\n",
    "            print(f\"Matched DataFrame shape: {matched.shape}\")\n",
    "            matched_laps_list.append(matched)\n",
    "        else:\n",
    "            matched_laps_list.append(race_laps)  # No weather data for this race\n",
    "\n",
    "    # Concatenate all matched laps\n",
    "    laps = pd.concat(matched_laps_list, ignore_index=True)\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Fill missing weather data with default values\n",
    "    laps['track_temp'] = laps['TrackTemp'].fillna(25.0)\n",
    "    laps['air_temp'] = laps['AirTemp'].fillna(20.0)\n",
    "    laps['humidity'] = laps['Humidity'].fillna(50.0)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    # Create driver names\n",
    "    drivers['driver_name'] = drivers['forename'] + ' ' + drivers['surname']\n",
    "    driver_mapping = drivers[['driverId', 'driver_name']].copy()\n",
    "    driver_mapping.set_index('driverId', inplace=True)\n",
    "    driver_names = driver_mapping['driver_name'].to_dict()\n",
    "    \n",
    "    # Map statusId to status descriptions\n",
    "    status_dict = status.set_index('statusId')['status'].to_dict()\n",
    "    results['status'] = results['statusId'].map(status_dict)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    def calculate_aggression(driver_results):\n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default aggression for new drivers\n",
    "        \n",
    "        # Only consider recent races for more current behavior\n",
    "        recent_results = driver_results.sort_values('date', ascending=False).head(20)\n",
    "        \n",
    "        # Calculate overtaking metrics\n",
    "        positions_gained = recent_results['grid'] - recent_results['positionOrder']\n",
    "        \n",
    "        # Calculate risk metrics\n",
    "        dnf_rate = (recent_results['status'] != 'Finished').mean()\n",
    "        incidents = (recent_results['statusId'].isin([\n",
    "            4,  # Collision\n",
    "            5,  # Spun off\n",
    "            6,  # Accident\n",
    "            20, # Collision damage\n",
    "            82, # Collision with another driver\n",
    "        ])).mean()\n",
    "        \n",
    "        # Calculate overtaking success rate (normalized between 0-1)\n",
    "        positive_overtakes = (positions_gained > 0).sum()\n",
    "        negative_overtakes = (positions_gained < 0).sum()\n",
    "        total_overtake_attempts = positive_overtakes + negative_overtakes\n",
    "        overtake_success_rate = positive_overtakes / total_overtake_attempts if total_overtake_attempts > 0 else 0.5\n",
    "        \n",
    "        # Normalize average positions gained (0-1)\n",
    "        avg_positions_gained = positions_gained[positions_gained > 0].mean() if len(positions_gained[positions_gained > 0]) > 0 else 0\n",
    "        max_possible_gain = 20  # Maximum grid positions that could be gained\n",
    "        normalized_gains = np.clip(avg_positions_gained / max_possible_gain, 0, 1)\n",
    "        \n",
    "        # Normalize risk factors (0-1)\n",
    "        normalized_dnf = np.clip(dnf_rate, 0, 1)\n",
    "        normalized_incidents = np.clip(incidents, 0, 1)\n",
    "        \n",
    "        # Calculate component scores (each between 0-1)\n",
    "        overtaking_component = (normalized_gains * 0.6 + overtake_success_rate * 0.4)\n",
    "        risk_component = (normalized_dnf * 0.5 + normalized_incidents * 0.5)\n",
    "        \n",
    "        # Combine components with weights (ensuring sum of weights = 1)\n",
    "        weights = {\n",
    "            'overtaking': 0.4,  # Aggressive overtaking\n",
    "            'risk': 0.5,       # Risk-taking behavior\n",
    "            'baseline': 0.1    # Baseline aggression\n",
    "        }\n",
    "        \n",
    "        aggression = (\n",
    "            overtaking_component * weights['overtaking'] +\n",
    "            risk_component * weights['risk'] +\n",
    "            0.5 * weights['baseline']  # Baseline aggression factor\n",
    "        )\n",
    "        \n",
    "        # Add small random variation while maintaining 0-1 bounds\n",
    "        variation = np.random.normal(0, 0.02)\n",
    "        aggression = np.clip(aggression + variation, 0, 1)\n",
    "        \n",
    "        return aggression\n",
    "    \n",
    "    def calculate_skill(driver_data, results_data, circuit_id):\n",
    "        driver_results = results_data[\n",
    "            (results_data['driverId'] == driver_data['driverId']) & \n",
    "            (results_data['circuitId'] == circuit_id)\n",
    "        ].sort_values('date', ascending=False).head(10)  # Use last 10 races at circuit\n",
    "        \n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default skill\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        avg_finish_pos = driver_results['positionOrder'].mean()\n",
    "        avg_quali_pos = driver_results['grid'].mean()\n",
    "        points_per_race = driver_results['points'].mean()\n",
    "        fastest_laps = (driver_results['rank'] == 1).mean()  # Add fastest lap consideration\n",
    "        \n",
    "        # Improved normalization (exponential decay for positions)\n",
    "        normalized_finish_pos = np.exp(-avg_finish_pos/5) # Better spread of values\n",
    "        normalized_quali_pos = np.exp(-avg_quali_pos/5)\n",
    "        \n",
    "        # Points normalization with improved scaling\n",
    "        max_points_per_race = 26  # Maximum possible points (25 + 1 fastest lap)\n",
    "        normalized_points = points_per_race / max_points_per_race\n",
    "        \n",
    "        # Weighted combination with more factors\n",
    "        weights = {\n",
    "            'finish': 0.35,\n",
    "            'quali': 0.25,\n",
    "            'points': 0.25,\n",
    "            'fastest_laps': 0.15\n",
    "        }\n",
    "        \n",
    "        skill = (\n",
    "            weights['finish'] * normalized_finish_pos +\n",
    "            weights['quali'] * normalized_quali_pos +\n",
    "            weights['points'] * normalized_points +\n",
    "            weights['fastest_laps'] * fastest_laps\n",
    "        )\n",
    "        \n",
    "        # Add random variation to prevent identical skills\n",
    "        skill = np.clip(skill + np.random.normal(0, 0.05), 0.1, 1.0)\n",
    "        \n",
    "        return skill\n",
    "    \n",
    "    # First merge results with races to get circuitId\n",
    "    results = results.merge(\n",
    "        races[['raceId', 'circuitId']], \n",
    "        on='raceId',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Now calculate driver aggression and skill\n",
    "    driver_aggression = {}\n",
    "    driver_skill = {}\n",
    "    for driver_id in drivers['driverId'].unique():\n",
    "        driver_results = results[results['driverId'] == driver_id]\n",
    "        aggression = calculate_aggression(driver_results)\n",
    "        driver_aggression[driver_id] = aggression\n",
    "        \n",
    "        # Now we have circuit_id from the merge\n",
    "        recent_race = driver_results.sort_values('date', ascending=False).head(1)\n",
    "        if not recent_race.empty:\n",
    "            circuit_id = recent_race['circuitId'].iloc[0]\n",
    "            skill = calculate_skill({'driverId': driver_id}, results, circuit_id)\n",
    "            driver_skill[driver_id] = skill\n",
    "        else:\n",
    "            driver_skill[driver_id] = 0.5  # Default skill for new drivers\n",
    "    \n",
    "    # Map calculated aggression and skill back to laps DataFrame\n",
    "    laps['driver_aggression'] = laps['driverId'].map(driver_aggression)\n",
    "    laps['driver_overall_skill'] = laps['driverId'].map(driver_skill)\n",
    "    laps['driver_circuit_skill'] = laps['driver_overall_skill']  # For simplicity, using overall skill\n",
    "    laps['driver_consistency'] = 0.5  # Placeholder\n",
    "    laps['driver_reliability'] = 0.5  # Placeholder\n",
    "    laps['driver_risk_taking'] = laps['driver_aggression']  # Assuming similar to aggression\n",
    "    \n",
    "    # Dynamic features\n",
    "    laps['tire_age'] = laps.groupby(['raceId', 'driverId'])['lap'].cumcount()\n",
    "    laps['fuel_load'] = laps.groupby(['raceId', 'driverId'])['lap'].transform(lambda x: x.max() - x + 1)\n",
    "    laps['track_position'] = laps['position']  # Assuming 'position' is available in laps data\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    # Create an instance of RaceFeatures\n",
    "    race_features = RaceFeatures()\n",
    "\n",
    "    \n",
    "    laps['TrackStatus'].fillna(1, inplace=True)  # 1 = regular racing status\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    required_columns = race_features.static_features + race_features.dynamic_features\n",
    "    # Before dropping NaN values\n",
    "    print(\"\\nNaN counts in required columns:\")\n",
    "    for col in required_columns:\n",
    "        nan_count = laps[col].isna().sum()\n",
    "        total_rows = len(laps)\n",
    "        if nan_count > 0:\n",
    "            print(f\"{col}: {nan_count} NaN values ({(nan_count/total_rows*100):.2f}% of rows)\")\n",
    "    missing_columns = set(required_columns) - set(laps.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Drop rows with missing values in required columns\n",
    "    laps.to_csv('laps_withNan.csv', index=False)\n",
    "    laps = laps.dropna(subset=required_columns)\n",
    "    \n",
    "    print(laps.shape)\n",
    "    \n",
    "    return laps\n",
    "\n",
    "# Update the main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    enhanced_laps = load_and_preprocess_data()\n",
    "    \n",
    "    enhanced_laps.drop(columns=['position', 'time', 'driverRef', 'number', 'R', 'S', 'code', 'forename', 'surname', 'url_x', 'url_race', 'name_x', 'circuitRef', 'name_y', 'location', 'country', 'url_y', 'positionOrder', 'fastestLap', 'cumulative_milliseconds', 'seconds_from_start', 'raceId_x', 'year_x', 'Time', 'TrackTemp', 'AirTemp', 'Humidity', 'name', 'year_y', 'raceId_y'], inplace=True)\n",
    "    \n",
    "    # Save the preprocessed laps DataFrame for inspection\n",
    "    enhanced_laps.to_csv('enhanced_laps_before_training.csv', index=False)\n",
    "    print(enhanced_laps.shape)\n",
    "    \n",
    "    print(\"Enhanced laps DataFrame saved to 'enhanced_laps_before_training.csv'\")\n",
    "    \n",
    "    preprocessor = F1DataPreprocessor()\n",
    "    sequences, static, targets = preprocessor.prepare_sequence_data(enhanced_laps, window_size=3)\n",
    "    \n",
    "    # Create train and validation loaders\n",
    "    train_loader, val_loader = preprocessor.create_train_val_loaders(\n",
    "        sequences, \n",
    "        static, \n",
    "        targets,\n",
    "        batch_size=32,\n",
    "        val_split=0.2\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = F1PredictionModel(\n",
    "        sequence_dim=sequences.shape[2],\n",
    "        static_dim=static.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = train_model(model, train_loader, val_loader, epochs=100, learning_rate=0.001)\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model_with_preprocessor(model, preprocessor, sequences.shape[2], static.shape[1], 'f1_prediction_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b610d07e42b0b7",
   "metadata": {},
   "source": [
    "# Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a3b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceSimulator:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.race_features = RaceFeatures()\n",
    "        \n",
    "    def simulate_lap(self, current_state):\n",
    "        \"\"\"\n",
    "        Predict lap time and uncertainty for a single lap.\n",
    "        \"\"\"\n",
    "        # Prepare input data\n",
    "        sequence = current_state['sequence']\n",
    "        static = current_state['static']\n",
    "        \n",
    "        # Convert to tensors\n",
    "        sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0)  # Add batch dimension\n",
    "        static_tensor = torch.FloatTensor(static).unsqueeze(0)\n",
    "        \n",
    "        # Predict with uncertainty\n",
    "        mean_pred, std_pred = predict_with_uncertainty(\n",
    "            self.model, \n",
    "            {'sequence': sequence_tensor, 'static': static_tensor},\n",
    "            n_samples=50\n",
    "        )\n",
    "        \n",
    "        # Inverse transform the prediction\n",
    "        lap_time = self.preprocessor.lap_time_scaler.inverse_transform([[mean_pred]])[0][0]\n",
    "        uncertainty = self.preprocessor.lap_time_scaler.inverse_transform([[std_pred]])[0][0]\n",
    "        \n",
    "        return lap_time, uncertainty\n",
    "    \n",
    "    def simulate_full_race(self, initial_state, strategy):\n",
    "        \"\"\"\n",
    "        Simulate the entire race lap by lap based on the provided strategy.\n",
    "        \"\"\"\n",
    "        lap_times = []\n",
    "        uncertainties = []\n",
    "        current_state = initial_state.copy()\n",
    "        \n",
    "        total_laps = strategy.total_laps\n",
    "        for lap in range(1, total_laps + 1):\n",
    "            # Update dynamic features based on strategy and lap number\n",
    "            current_state = self.update_state(current_state, lap, strategy)\n",
    "            \n",
    "            # Simulate lap\n",
    "            lap_time, uncertainty = self.simulate_lap(current_state)\n",
    "            lap_times.append(lap_time)\n",
    "            uncertainties.append(uncertainty)\n",
    "            \n",
    "            # Update sequence data for the next lap\n",
    "            current_state['sequence'] = self.update_sequence(\n",
    "                current_state['sequence'], lap_time, current_state['dynamic']\n",
    "            )\n",
    "        \n",
    "        return lap_times, uncertainties\n",
    "    \n",
    "    def update_state(self, state, lap, strategy):\n",
    "        \"\"\"\n",
    "        Update the state for the next lap based on the strategy.\n",
    "        \"\"\"\n",
    "        # Update tire age\n",
    "        state['dynamic']['tire_age'] += 1\n",
    "        \n",
    "        # Check for pit stops\n",
    "        if lap in strategy.pit_stop_laps:\n",
    "            # Reset tire age and update tire compound\n",
    "            state['dynamic']['tire_age'] = 0\n",
    "            state['dynamic']['tire_compound'] = strategy.pit_stop_compounds[lap]\n",
    "        \n",
    "        # Update fuel load\n",
    "        state['dynamic']['fuel_load'] -= strategy.fuel_consumption_per_lap\n",
    "        \n",
    "        # Update other dynamic features as needed\n",
    "        # ...\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def update_sequence(self, sequence, new_lap_time, dynamic_features):\n",
    "        \"\"\"\n",
    "        Update the sequence data with the latest lap information.\n",
    "        \"\"\"\n",
    "        # Remove the oldest lap data\n",
    "        sequence = sequence[1:]\n",
    "        # Append the new lap data\n",
    "        new_sequence_entry = np.hstack((\n",
    "            [new_lap_time],\n",
    "            [dynamic_features[feature] for feature in self.race_features.dynamic_features]\n",
    "        ))\n",
    "        sequence = np.vstack([sequence, new_sequence_entry])\n",
    "        return sequence\n",
    "    \n",
    "    # Additional helper methods as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6274a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceStrategy:\n",
    "    def __init__(self, total_laps, pit_stops):\n",
    "        \"\"\"\n",
    "        pit_stops: List of dictionaries with keys 'lap' and 'compound'\n",
    "        Example: [{'lap': 15, 'compound': 'MEDIUM'}, {'lap': 30, 'compound': 'SOFT'}]\n",
    "        \"\"\"\n",
    "        self.total_laps = total_laps\n",
    "        self.pit_stops = pit_stops  # List of pit stop events\n",
    "        self.pit_stop_laps = [stop['lap'] for stop in pit_stops]\n",
    "        self.pit_stop_compounds = {stop['lap']: stop['compound'] for stop in pit_stops}\n",
    "        self.fuel_consumption_per_lap = 1.5  # Example value, adjust as needed\n",
    "    \n",
    "    def evaluate(self, simulator, initial_state):\n",
    "        \"\"\"\n",
    "        Simulate the race using this strategy and return total race time.\n",
    "        \"\"\"\n",
    "        lap_times, uncertainties = simulator.simulate_full_race(initial_state, self)\n",
    "        total_time = sum(lap_times)\n",
    "        return total_time, lap_times, uncertainties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d1c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceStrategyOptimizer:\n",
    "    def __init__(self, simulator):\n",
    "        self.simulator = simulator\n",
    "    \n",
    "    def optimize(self, initial_conditions, constraints):\n",
    "        \"\"\"\n",
    "        Find the optimal strategy given initial conditions and constraints.\n",
    "        \"\"\"\n",
    "        best_strategy = None\n",
    "        best_time = float('inf')\n",
    "        \n",
    "        # Generate possible strategies within constraints\n",
    "        possible_strategies = self.generate_strategies(constraints)\n",
    "        \n",
    "        # Evaluate each strategy\n",
    "        for strategy in possible_strategies:\n",
    "            total_time, _, _ = strategy.evaluate(self.simulator, initial_conditions)\n",
    "            if total_time < best_time:\n",
    "                best_time = total_time\n",
    "                best_strategy = strategy\n",
    "        \n",
    "        return best_strategy\n",
    "    \n",
    "    def generate_strategies(self, constraints):\n",
    "        \"\"\"\n",
    "        Generate possible strategies based on constraints.\n",
    "        \"\"\"\n",
    "        min_pit_stops = constraints.get('min_pit_stops', 1)\n",
    "        max_pit_stops = constraints.get('max_pit_stops', 3)\n",
    "        available_compounds = constraints.get('available_compounds', ['SOFT', 'MEDIUM', 'HARD'])\n",
    "        total_laps = constraints.get('total_laps', 50)\n",
    "        \n",
    "        strategies = []\n",
    "        \n",
    "        # Example: Generate strategies with different pit stop laps and compounds\n",
    "        for num_pit_stops in range(min_pit_stops, max_pit_stops + 1):\n",
    "            pit_stop_laps_options = self.get_pit_stop_lap_combinations(total_laps, num_pit_stops)\n",
    "            for pit_stop_laps in pit_stop_laps_options:\n",
    "                for compounds in self.get_compound_combinations(available_compounds, num_pit_stops):\n",
    "                    pit_stops = [{'lap': lap, 'compound': comp} for lap, comp in zip(pit_stop_laps, compounds)]\n",
    "                    strategy = RaceStrategy(total_laps, pit_stops)\n",
    "                    strategies.append(strategy)\n",
    "        return strategies\n",
    "    \n",
    "    def get_pit_stop_lap_combinations(self, total_laps, num_pit_stops):\n",
    "        \"\"\"\n",
    "        Generate possible pit stop lap combinations.\n",
    "        \"\"\"\n",
    "        from itertools import combinations\n",
    "        lap_numbers = range(5, total_laps - 5)  # Avoid pitting too early or too late\n",
    "        return combinations(lap_numbers, num_pit_stops)\n",
    "    \n",
    "    def get_compound_combinations(self, compounds, num_pit_stops):\n",
    "        \"\"\"\n",
    "        Generate possible combinations of compounds for pit stops.\n",
    "        \"\"\"\n",
    "        from itertools import product\n",
    "        return product(compounds, repeat=num_pit_stops)\n",
    "    \n",
    "    def load_model_with_preprocessor(path: str, sequence_dim: int, static_dim: int, hidden_dim: int = 64, num_layers: int = 2):\n",
    "        \"\"\"\n",
    "        Load the model and preprocessor from a saved file.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "        \n",
    "        # Recreate the model architecture\n",
    "        model = F1PredictionModel(\n",
    "            sequence_dim=sequence_dim,\n",
    "            static_dim=static_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        # Recreate the preprocessor\n",
    "        preprocessor = F1DataPreprocessor()\n",
    "        preprocessor.lap_time_scaler = checkpoint['lap_time_scaler']\n",
    "        preprocessor.dynamic_scaler = checkpoint['dynamic_scaler']\n",
    "        preprocessor.static_scaler = checkpoint['static_scaler']\n",
    "        \n",
    "        print(f\"Model and preprocessor loaded from {path}\")\n",
    "        return model, preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf71ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_with_preprocessor(path: str) -> Tuple[F1PredictionModel, F1DataPreprocessor]:\n",
    "   \"\"\"Load saved model and preprocessor\"\"\"\n",
    "   checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "   \n",
    "   model = F1PredictionModel(\n",
    "       sequence_dim=checkpoint['sequence_dim'], \n",
    "       static_dim=checkpoint['static_dim']\n",
    "   )\n",
    "   model.load_state_dict(checkpoint['model_state_dict'])\n",
    "   \n",
    "   preprocessor = F1DataPreprocessor()\n",
    "   preprocessor.lap_time_scaler = checkpoint['lap_time_scaler']\n",
    "   preprocessor.dynamic_scaler = checkpoint['dynamic_scaler']\n",
    "   preprocessor.static_scaler = checkpoint['static_scaler']\n",
    "   \n",
    "   return model, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17ec175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/96l18xtx7fz9rgy3dxg2crdm0000gn/T/ipykernel_19862/785214940.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 115\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Create optimizer and find the best strategy\u001b[39;00m\n\u001b[1;32m    114\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m RaceStrategyOptimizer(simulator)\n\u001b[0;32m--> 115\u001b[0m best_strategy \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Evaluate the best strategy\u001b[39;00m\n\u001b[1;32m    118\u001b[0m total_time, lap_times, uncertainties \u001b[38;5;241m=\u001b[39m best_strategy\u001b[38;5;241m.\u001b[39mevaluate(simulator, initial_state)\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mRaceStrategyOptimizer.optimize\u001b[0;34m(self, initial_conditions, constraints)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Evaluate each strategy\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m strategy \u001b[38;5;129;01min\u001b[39;00m possible_strategies:\n\u001b[0;32m---> 17\u001b[0m     total_time, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_conditions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m total_time \u001b[38;5;241m<\u001b[39m best_time:\n\u001b[1;32m     19\u001b[0m         best_time \u001b[38;5;241m=\u001b[39m total_time\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mRaceStrategy.evaluate\u001b[0;34m(self, simulator, initial_state)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, simulator, initial_state):\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Simulate the race using this strategy and return total race time.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     lap_times, uncertainties \u001b[38;5;241m=\u001b[39m \u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_full_race\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     total_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(lap_times)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_time, lap_times, uncertainties\n",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m, in \u001b[0;36mRaceSimulator.simulate_full_race\u001b[0;34m(self, initial_state, strategy)\u001b[0m\n\u001b[1;32m     43\u001b[0m current_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_state(current_state, lap, strategy)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Simulate lap\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m lap_time, uncertainty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_lap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m lap_times\u001b[38;5;241m.\u001b[39mappend(lap_time)\n\u001b[1;32m     48\u001b[0m uncertainties\u001b[38;5;241m.\u001b[39mappend(uncertainty)\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mRaceSimulator.simulate_lap\u001b[0;34m(self, current_state)\u001b[0m\n\u001b[1;32m     17\u001b[0m static_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(static)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Predict with uncertainty\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m mean_pred, std_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_with_uncertainty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstatic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_tensor\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Inverse transform the prediction\u001b[39;00m\n\u001b[1;32m     27\u001b[0m lap_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39mlap_time_scaler\u001b[38;5;241m.\u001b[39minverse_transform([[mean_pred]])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 286\u001b[0m, in \u001b[0;36mpredict_with_uncertainty\u001b[0;34m(model, inputs, n_samples)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_samples):\n\u001b[0;32m--> 286\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    287\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m    288\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(predictions)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[7], line 195\u001b[0m, in \u001b[0;36mF1PredictionModel.forward\u001b[0;34m(self, sequence, static)\u001b[0m\n\u001b[1;32m    192\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([lstm_out, static_out], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Final prediction\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[1;32m   1426\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/torch/_VF.py:27\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_VariableFunctions\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf, name)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The model and preprocessor are now ready for simulation or further training\n",
    "model, preprocessor = load_model_with_preprocessor('f1_prediction_model.pth')\n",
    "\n",
    "# Assume 'model' and 'preprocessor' have been loaded or trained\n",
    "simulator = RaceSimulator(model, preprocessor)\n",
    "\n",
    "# Step 1: Define static features\n",
    "static_features_list = [\n",
    "    'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "    'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "    'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time'\n",
    "]\n",
    "\n",
    "static_features = np.array([\n",
    "    0.8,   # driver_overall_skill\n",
    "    0.75,  # driver_circuit_skill\n",
    "    0.7,   # driver_consistency\n",
    "    0.9,   # driver_reliability\n",
    "    0.6,   # driver_aggression\n",
    "    0.5,   # driver_risk_taking\n",
    "    88000, # fp1_median_time\n",
    "    87500, # fp2_median_time\n",
    "    87000, # fp3_median_time\n",
    "    86000  # quali_time\n",
    "])\n",
    "\n",
    "# Step 2: Define initial dynamic features for previous laps\n",
    "dynamic_features_list = [\n",
    "    'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "    'air_temp', 'humidity', 'TrackStatus', 'is_pit_lap'\n",
    "]\n",
    "\n",
    "# Common values\n",
    "track_temp = 35.0\n",
    "air_temp = 25.0\n",
    "humidity = 50.0\n",
    "TrackStatus = 1\n",
    "is_pit_lap = 0\n",
    "\n",
    "dynamic_features = np.array([\n",
    "    [0, 100, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap],  # Lap 1\n",
    "    [1, 98.5, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap],  # Lap 2\n",
    "    [2, 97, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap]     # Lap 3\n",
    "])\n",
    "\n",
    "# Tire compound (not scaled)\n",
    "tire_compound = 0  # Example value for 'HARD' tires\n",
    "tire_compound_column = np.full((dynamic_features.shape[0], 1), tire_compound)\n",
    "\n",
    "# Lap times in milliseconds\n",
    "lap_times = np.array([90000, 89500, 89200])  # Laps 1-3\n",
    "\n",
    "# Step 3: Combine lap times, tire_compound, and dynamic features\n",
    "sequence = np.hstack((\n",
    "    lap_times.reshape(-1, 1),  # Lap times\n",
    "    tire_compound_column,      # Tire compound\n",
    "    dynamic_features           # Dynamic features\n",
    "))\n",
    "\n",
    "# Step 4: Scale the lap times\n",
    "lap_times_scaled = preprocessor.lap_time_scaler.transform(lap_times.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Step 5: Scale the dynamic features (excluding 'tire_compound')\n",
    "dynamic_features_to_scale = sequence[:, 2:]  # Exclude lap times and 'tire_compound'\n",
    "dynamic_scaled = preprocessor.dynamic_scaler.transform(dynamic_features_to_scale)\n",
    "\n",
    "# Step 6: Reconstruct the scaled sequence\n",
    "sequence_scaled = np.hstack((\n",
    "    lap_times_scaled.reshape(-1, 1),  # Scaled lap times\n",
    "    tire_compound_column,             # Tire compound (not scaled)\n",
    "    dynamic_scaled                    # Scaled dynamic features\n",
    "))\n",
    "\n",
    "# Step 7: Scale the static features\n",
    "static_scaled = preprocessor.static_scaler.transform(static_features.reshape(1, -1)).flatten()\n",
    "\n",
    "# Step 8: Prepare current dynamic features\n",
    "current_dynamic_features = {\n",
    "    'tire_age': 3,\n",
    "    'fuel_load': 95.5,\n",
    "    'track_position': 1,\n",
    "    'track_temp': 35.0,\n",
    "    'air_temp': 25.0,\n",
    "    'humidity': 50.0,\n",
    "    'TrackStatus': 1,\n",
    "    'is_pit_lap': 0,\n",
    "    'tire_compound': 0\n",
    "}\n",
    "\n",
    "# Extract and scale dynamic features (excluding 'tire_compound')\n",
    "dynamic_feature_values = np.array([\n",
    "    current_dynamic_features[feature] for feature in dynamic_features_list\n",
    "])\n",
    "\n",
    "dynamic_scaled_current = preprocessor.dynamic_scaler.transform(dynamic_feature_values.reshape(1, -1)).flatten()\n",
    "\n",
    "# Step 9: Update the initial state\n",
    "initial_state = {\n",
    "    'sequence': sequence_scaled,   # Shape: (window_size, sequence_dim)\n",
    "    'static': static_scaled,       # Shape: (static_dim,)\n",
    "    'dynamic': current_dynamic_features\n",
    "}\n",
    "\n",
    "\n",
    "# Define constraints\n",
    "constraints = {\n",
    "    'min_pit_stops': 1,\n",
    "    'max_pit_stops': 2,\n",
    "    'available_compounds': [3, 2, 1],\n",
    "    'total_laps': 50\n",
    "}\n",
    "\n",
    "# Create optimizer and find the best strategy\n",
    "optimizer = RaceStrategyOptimizer(simulator)\n",
    "best_strategy = optimizer.optimize(initial_state, constraints)\n",
    "\n",
    "# Evaluate the best strategy\n",
    "total_time, lap_times, uncertainties = best_strategy.evaluate(simulator, initial_state)\n",
    "print(f\"Optimal total race time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f9b854",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# %% [markdown]\n",
    "# # Phase 1\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Define the RaceFeatures dataclass\n",
    "@dataclass\n",
    "class RaceFeatures:\n",
    "    \"\"\"Data structure for race features\"\"\"\n",
    "    static_features: List[str] = field(default_factory=lambda: [\n",
    "        'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "        'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "        'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time'\n",
    "    ])\n",
    "    \n",
    "    dynamic_features: List[str] = field(default_factory=lambda: [\n",
    "        'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "        'air_temp', 'humidity', 'tire_compound', 'TrackStatus', 'is_pit_lap'\n",
    "    ])\n",
    "    \n",
    "    target: str = 'milliseconds'\n",
    "\n",
    "# Define the F1Dataset class\n",
    "class F1Dataset(Dataset):\n",
    "    def __init__(self, sequences, static_features, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.static_features = torch.FloatTensor(static_features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'sequence': self.sequences[idx],\n",
    "            'static': self.static_features[idx],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "\n",
    "# Define the F1DataPreprocessor class\n",
    "class F1DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.static_scaler = StandardScaler()\n",
    "        self.dynamic_scaler = StandardScaler()\n",
    "        self.lap_time_scaler = StandardScaler()\n",
    "        \n",
    "    def prepare_sequence_data(self, df: pd.DataFrame, window_size: int = 3) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare sequential data with sliding window and apply scaling\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        static_features = []\n",
    "        targets = []\n",
    "        \n",
    "        # Instantiate RaceFeatures\n",
    "        race_features = RaceFeatures()\n",
    "        \n",
    "        # Sort the dataframe to ensure consistent ordering\n",
    "        df = df.sort_values(['raceId', 'driverId', 'lap'])\n",
    "        \n",
    "        # Group by race and driver\n",
    "        for (race_id, driver_id), group in df.groupby(['raceId', 'driverId']):\n",
    "            group = group.sort_values('lap')\n",
    "            \n",
    "            # Extract static features (assumed to be constant per driver per race)\n",
    "            static = group[race_features.static_features].iloc[0].values\n",
    "            static_features.append(static)\n",
    "            \n",
    "            # Extract dynamic features and target\n",
    "            lap_times = group[race_features.target].values.reshape(-1, 1)  # Shape: (num_laps, 1)\n",
    "            dynamic = group[race_features.dynamic_features].values  # Shape: (num_laps, num_dynamic_features)\n",
    "            \n",
    "            # Apply scaling\n",
    "            # Note: Scalers should be fitted on the training data to prevent data leakage.\n",
    "            # Here, for simplicity, we're fitting on the entire dataset. For a real-world scenario,\n",
    "            # consider splitting the data first before fitting the scalers.\n",
    "            dynamic_features_to_scale = [col for col in race_features.dynamic_features if col != 'tire_compound']\n",
    "            tire_compounds = dynamic[:, race_features.dynamic_features.index('tire_compound')].reshape(-1, 1)\n",
    "            other_dynamic = dynamic[:, [race_features.dynamic_features.index(col) for col in dynamic_features_to_scale]]\n",
    "            \n",
    "            lap_times_scaled = self.lap_time_scaler.fit_transform(lap_times).flatten()\n",
    "            other_dynamic_scaled = self.dynamic_scaler.fit_transform(other_dynamic)\n",
    "            static_scaled = self.static_scaler.fit_transform(static.reshape(1, -1)).flatten()\n",
    "            \n",
    "            dynamic_scaled = np.hstack((tire_compounds, other_dynamic_scaled))\n",
    "            \n",
    "            # Create sequences\n",
    "            # Create sequences\n",
    "        for i in range(len(lap_times_scaled) - window_size):\n",
    "            sequence_lap_times = lap_times_scaled[i:i+window_size].reshape(-1, 1)  # Shape: (window_size, 1)\n",
    "            sequence_dynamic = dynamic_scaled[i:i+window_size]  # Shape: (window_size, num_dynamic_features)\n",
    "            sequence = np.hstack((sequence_lap_times, sequence_dynamic))  # Shape: (window_size, 1 + num_dynamic_features)\n",
    "            sequences.append(sequence)\n",
    "            static_features.append(static_scaled)\n",
    "            targets.append(lap_times_scaled[i + window_size])\n",
    "        \n",
    "        return (np.array(sequences), \n",
    "                np.array(static_features), \n",
    "                np.array(targets))\n",
    "\n",
    "    \n",
    "    def create_train_val_loaders(\n",
    "        self, \n",
    "        sequences: np.ndarray, \n",
    "        static_features: np.ndarray, \n",
    "        targets: np.ndarray,\n",
    "        batch_size: int = 32,\n",
    "        val_split: float = 0.2\n",
    "    ) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"\n",
    "        Create train and validation dataloaders with given split ratio\n",
    "        \"\"\"\n",
    "        dataset = F1Dataset(sequences, static_features, targets)\n",
    "        \n",
    "        # Calculate lengths for split\n",
    "        val_size = int(len(dataset) * val_split)\n",
    "        train_size = len(dataset) - val_size\n",
    "        \n",
    "        # Split dataset\n",
    "        train_dataset, val_dataset = random_split(\n",
    "            dataset, \n",
    "            [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "class F1PredictionModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 sequence_dim: int,\n",
    "                 static_dim: int,\n",
    "                 hidden_dim: int = 64,\n",
    "                 num_layers: int = 2,\n",
    "                 dropout_prob: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LSTM for sequential features with dropout\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=sequence_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob\n",
    "        )\n",
    "        \n",
    "        # Static features processing with dropout\n",
    "        self.static_network = nn.Sequential(\n",
    "            nn.Linear(static_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Combine everything\n",
    "        self.final_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, sequence, static):\n",
    "        # Process sequence through LSTM\n",
    "        lstm_out, _ = self.lstm(sequence)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Output of the last time step\n",
    "        \n",
    "        # Process static features\n",
    "        static_out = self.static_network(static)\n",
    "        \n",
    "        # Combine LSTM output and static features\n",
    "        combined = torch.cat([lstm_out, static_out], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.final_network(combined)\n",
    "        \n",
    "        return prediction.squeeze()\n",
    "\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model: nn.Module, \n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                epochs: int = 10,\n",
    "                learning_rate: float = 0.001,\n",
    "                lap_time_scaler: StandardScaler = None,  # Pass the lap time scaler\n",
    "                device: Optional[str] = None) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train the model and return training history including MAE in milliseconds\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_maes = []\n",
    "        for batch in train_loader:\n",
    "            sequences = batch['sequence'].to(device)\n",
    "            static = batch['static'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(sequences, static)\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Calculate MAE in normalized scale\n",
    "            mae = torch.mean(torch.abs(predictions - targets)).item()\n",
    "            train_maes.append(mae)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_maes = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                sequences = batch['sequence'].to(device)\n",
    "                static = batch['static'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                predictions = model(sequences, static)\n",
    "                loss = criterion(predictions, targets)\n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Calculate MAE in normalized scale\n",
    "                mae_normalized = torch.mean(torch.abs(predictions - targets)).item()\n",
    "                val_maes.append(mae_normalized)\n",
    "        \n",
    "        # Record metrics\n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        train_mae_normalized = np.mean(train_maes)\n",
    "        val_mae_normalized = np.mean(val_maes)\n",
    "        \n",
    "        # Convert MAE back to milliseconds using the inverse scaler\n",
    "        if lap_time_scaler:\n",
    "            train_mae_ms = lap_time_scaler.inverse_transform([[train_mae_normalized]])[0][0]\n",
    "            val_mae_ms = lap_time_scaler.inverse_transform([[val_mae_normalized]])[0][0]\n",
    "        else:\n",
    "            train_mae_ms, val_mae_ms = train_mae_normalized, val_mae_normalized\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mae'].append(train_mae_ms)\n",
    "        history['val_mae'].append(val_mae_ms)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train MAE: {train_mae_ms:.2f} ms, Val MAE: {val_mae_ms:.2f} ms')\n",
    "    \n",
    "    return history\n",
    "\n",
    "def predict_with_uncertainty(model, inputs, n_samples=100):\n",
    "    model.train()  # Enable dropout layers\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            prediction = model(**inputs).cpu().numpy()\n",
    "            predictions.append(prediction)\n",
    "    predictions = np.array(predictions)\n",
    "    mean_prediction = predictions.mean(axis=0)\n",
    "    std_prediction = predictions.std(axis=0)\n",
    "    return mean_prediction, std_prediction\n",
    "\n",
    "# Define a function to save the model\n",
    "def save_model_with_preprocessor(model, preprocessor, sequence_dim, static_dim, path: str):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'lap_time_scaler': preprocessor.lap_time_scaler,\n",
    "        'dynamic_scaler': preprocessor.dynamic_scaler, \n",
    "        'static_scaler': preprocessor.static_scaler,\n",
    "        'sequence_dim': sequence_dim,\n",
    "        'static_dim': static_dim\n",
    "    }, path)\n",
    "    print(f\"Model and preprocessor saved to {path}\")\n",
    "\n",
    "\n",
    "# Now, integrate your code snippets into data preprocessing\n",
    "def load_and_preprocess_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from CSV files and preprocess it to create the enhanced_laps DataFrame.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    na_values = ['\\\\N', 'NaN', '']\n",
    "    lap_times = pd.read_csv('../../data/raw_data/lap_times.csv', na_values=na_values)\n",
    "    drivers = pd.read_csv('../../data/raw_data/drivers.csv', na_values=na_values)\n",
    "    races = pd.read_csv('../../data/raw_data/races.csv', na_values=na_values)\n",
    "    circuits = pd.read_csv('../../data/raw_data/circuits.csv', na_values=na_values)\n",
    "    pit_stops = pd.read_csv('../../data/raw_data/pit_stops.csv', na_values=na_values)\n",
    "    pit_stops.rename(columns={'milliseconds' : 'pitstop_milliseconds'}, inplace=True)\n",
    "    results = pd.read_csv('../../data/raw_data/results.csv', na_values=na_values)\n",
    "    results.rename(columns={'milliseconds' : 'racetime_milliseconds'}, inplace=True)\n",
    "\n",
    "    qualifying = pd.read_csv('../../data/raw_data/qualifying.csv', na_values=na_values)\n",
    "    status = pd.read_csv('../../data/raw_data/status.csv', na_values=na_values)\n",
    "    weather_data = pd.read_csv('../../data/raw_data/ff1_weather.csv', na_values=na_values)\n",
    "    practice_sessions = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "    # Load the tire data\n",
    "    tire_data = pd.read_csv('../../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
    "\n",
    "    # Convert date columns to datetime\n",
    "    races['date'] = pd.to_datetime(races['date'])\n",
    "    results['date'] = results['raceId'].map(races.set_index('raceId')['date'])\n",
    "    lap_times['date'] = lap_times['raceId'].map(races.set_index('raceId')['date'])\n",
    "    \n",
    "    # Merge dataframes\n",
    "    laps = lap_times.merge(drivers, on='driverId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(races, on='raceId', how='left', suffixes=('', '_race'))\n",
    "    laps.rename(columns={'quali_time' : 'quali_date_time'}, inplace=True)\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(circuits, on='circuitId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(results[['raceId', 'driverId', 'positionOrder', 'grid', 'racetime_milliseconds', 'fastestLap', 'statusId']], on=['raceId', 'driverId'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(status, on='statusId', how='left')\n",
    "    print(laps.shape)\n",
    "    laps = laps.merge(pit_stops[['raceId', 'driverId', 'lap', 'pitstop_milliseconds']], on=['raceId', 'driverId', 'lap'], how='left')\n",
    "    print(laps.shape)\n",
    "    laps['pitstop_milliseconds'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Add weather information\n",
    "    # Filter weather data to include only the Race session\n",
    "    weather_data = weather_data[weather_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge weather data with races to get raceId\n",
    "    weather_data = weather_data.merge(\n",
    "        races[['raceId', 'year', 'name']], \n",
    "        left_on=['EventName', 'Year'],\n",
    "        right_on=['name', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Compute cumulative time from the start of the race for each driver\n",
    "    laps.sort_values(['raceId', 'driverId', 'lap'], inplace=True)\n",
    "    laps['cumulative_milliseconds'] = laps.groupby(['raceId', 'driverId'])['milliseconds'].cumsum()\n",
    "    laps['seconds_from_start'] = laps['cumulative_milliseconds'] / 1000\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Use 'Time' in weather_data as 'seconds_from_start'\n",
    "    weather_data['seconds_from_start'] = weather_data['Time']\n",
    "    \n",
    "    # Standardize text data\n",
    "    tire_data['Compound'] = tire_data['Compound'].str.upper()\n",
    "    tire_data['EventName'] = tire_data['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Filter for race sessions only\n",
    "    tire_data = tire_data[tire_data['SessionName'] == 'R']\n",
    "    \n",
    "    # Merge with races to get raceId\n",
    "    tire_data = tire_data.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    tire_data['Driver'] = tire_data['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    tire_data['driverId'] = tire_data['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Rename 'LapNumber' to 'lap' and ensure integer type\n",
    "    tire_data.rename(columns={'LapNumber': 'lap'}, inplace=True)\n",
    "    tire_data['lap'] = tire_data['lap'].astype(int)\n",
    "    laps['lap'] = laps['lap'].astype(int)\n",
    "    \n",
    "    # Create compound mapping (ordered from hardest to softest)\n",
    "    compound_mapping = {\n",
    "        'UNKNOWN': 0,\n",
    "        'HARD': 1,\n",
    "        'MEDIUM': 2,\n",
    "        'SOFT': 3,\n",
    "        'INTERMEDIATE': 4,\n",
    "        'WET': 5\n",
    "    }\n",
    "    \n",
    "    # Merge tire_data with laps\n",
    "    laps = laps.merge(\n",
    "        tire_data[['raceId', 'driverId', 'lap', 'Compound', 'TrackStatus']],\n",
    "        on=['raceId', 'driverId', 'lap'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Handle missing compounds and apply numeric encoding\n",
    "    laps['Compound'].fillna('UNKNOWN', inplace=True)\n",
    "    laps['tire_compound'] = laps['Compound'].map(compound_mapping)\n",
    "    \n",
    "    # Drop the original Compound column if desired\n",
    "    laps.drop('Compound', axis=1, inplace=True)\n",
    "    \n",
    "    # Standardize names\n",
    "    practice_sessions['EventName'] = practice_sessions['EventName'].str.strip().str.upper()\n",
    "    races['name'] = races['name'].str.strip().str.upper()\n",
    "    \n",
    "    # Merge practice_sessions with races to get raceId\n",
    "    practice_sessions = practice_sessions.merge(\n",
    "        races[['raceId', 'year', 'name']],\n",
    "        left_on=['Year', 'EventName'],\n",
    "        right_on=['year', 'name'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Map driver codes to driverId\n",
    "    practice_sessions['Driver'] = practice_sessions['Driver'].str.strip().str.upper()\n",
    "    drivers['code'] = drivers['code'].str.strip().str.upper()\n",
    "    driver_code_to_id = drivers.set_index('code')['driverId'].to_dict()\n",
    "    practice_sessions['driverId'] = practice_sessions['Driver'].map(driver_code_to_id)\n",
    "    \n",
    "    # Convert LapTime to milliseconds\n",
    "    practice_sessions['LapTime_ms'] = practice_sessions['LapTime'].apply(lambda x: pd.to_timedelta(x).total_seconds() * 1000)\n",
    "    \n",
    "    # Calculate median lap times for each driver in each session\n",
    "    session_medians = practice_sessions.groupby(['raceId', 'driverId', 'SessionName'])['LapTime_ms'].median().reset_index()\n",
    "    \n",
    "    # Pivot the data to have sessions as columns\n",
    "    session_medians_pivot = session_medians.pivot_table(\n",
    "        index=['raceId', 'driverId'],\n",
    "        columns='SessionName',\n",
    "        values='LapTime_ms'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    session_medians_pivot.rename(columns={\n",
    "        'FP1': 'fp1_median_time',\n",
    "        'FP2': 'fp2_median_time',\n",
    "        'FP3': 'fp3_median_time',\n",
    "        'Q': 'quali_time'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    laps = laps.merge(\n",
    "    session_medians_pivot,\n",
    "    on=['raceId', 'driverId'],\n",
    "    how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing practice times with global median or a placeholder value\n",
    "    global_median_fp1 = laps['fp1_median_time'].median()\n",
    "    laps['fp1_median_time'].fillna(global_median_fp1, inplace=True)\n",
    "    \n",
    "    # Repeat for other sessions\n",
    "    global_median_fp2 = laps['fp2_median_time'].median()\n",
    "    laps['fp2_median_time'].fillna(global_median_fp2, inplace=True)\n",
    "    \n",
    "    global_median_fp3 = laps['fp3_median_time'].median()\n",
    "    laps['fp3_median_time'].fillna(global_median_fp3, inplace=True)\n",
    "    \n",
    "    global_median_quali = laps['quali_time'].median()\n",
    "    laps['quali_time'].fillna(global_median_quali, inplace=True)\n",
    "\n",
    "    \n",
    "    # Create a binary indicator for pit stops\n",
    "    laps['is_pit_lap'] = laps['pitstop_milliseconds'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    \n",
    "    # Define a function to match weather data to laps\n",
    "    def match_weather_to_lap(race_laps, race_weather):\n",
    "        \"\"\"\n",
    "        For each lap, find the closest weather measurement in time\n",
    "        \"\"\"\n",
    "        race_laps = race_laps.sort_values('seconds_from_start')\n",
    "        race_weather = race_weather.sort_values('seconds_from_start')\n",
    "        merged = pd.merge_asof(\n",
    "            race_laps,\n",
    "            race_weather,\n",
    "            on='seconds_from_start',\n",
    "            direction='nearest'\n",
    "        )\n",
    "        return merged\n",
    "\n",
    "    # Apply matching per race\n",
    "    matched_laps_list = []\n",
    "    for race_id in laps['raceId'].unique():\n",
    "        print(f'Matching for {race_id}')\n",
    "        race_laps = laps[laps['raceId'] == race_id]\n",
    "        race_weather = weather_data[weather_data['raceId'] == race_id]\n",
    "        \n",
    "        if not race_weather.empty:\n",
    "            matched = match_weather_to_lap(race_laps, race_weather)\n",
    "            print(f\"Matched DataFrame shape: {matched.shape}\")\n",
    "            matched_laps_list.append(matched)\n",
    "        else:\n",
    "            matched_laps_list.append(race_laps)  # No weather data for this race\n",
    "\n",
    "    # Concatenate all matched laps\n",
    "    laps = pd.concat(matched_laps_list, ignore_index=True)\n",
    "    print(laps.shape)\n",
    "    \n",
    "    # Fill missing weather data with default values\n",
    "    laps['track_temp'] = laps['TrackTemp'].fillna(25.0)\n",
    "    laps['air_temp'] = laps['AirTemp'].fillna(20.0)\n",
    "    laps['humidity'] = laps['Humidity'].fillna(50.0)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    # Create driver names\n",
    "    drivers['driver_name'] = drivers['forename'] + ' ' + drivers['surname']\n",
    "    driver_mapping = drivers[['driverId', 'driver_name']].copy()\n",
    "    driver_mapping.set_index('driverId', inplace=True)\n",
    "    driver_names = driver_mapping['driver_name'].to_dict()\n",
    "    \n",
    "    # Map statusId to status descriptions\n",
    "    status_dict = status.set_index('statusId')['status'].to_dict()\n",
    "    results['status'] = results['statusId'].map(status_dict)\n",
    "    \n",
    "    # Calculate driver aggression and skill\n",
    "    def calculate_aggression(driver_results):\n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default aggression for new drivers\n",
    "        \n",
    "        # Only consider recent races for more current behavior\n",
    "        recent_results = driver_results.sort_values('date', ascending=False).head(20)\n",
    "        \n",
    "        # Calculate overtaking metrics\n",
    "        positions_gained = recent_results['grid'] - recent_results['positionOrder']\n",
    "        \n",
    "        # Calculate risk metrics\n",
    "        dnf_rate = (recent_results['status'] != 'Finished').mean()\n",
    "        incidents = (recent_results['statusId'].isin([\n",
    "            4,  # Collision\n",
    "            5,  # Spun off\n",
    "            6,  # Accident\n",
    "            20, # Collision damage\n",
    "            82, # Collision with another driver\n",
    "        ])).mean()\n",
    "        \n",
    "        # Calculate overtaking success rate (normalized between 0-1)\n",
    "        positive_overtakes = (positions_gained > 0).sum()\n",
    "        negative_overtakes = (positions_gained < 0).sum()\n",
    "        total_overtake_attempts = positive_overtakes + negative_overtakes\n",
    "        overtake_success_rate = positive_overtakes / total_overtake_attempts if total_overtake_attempts > 0 else 0.5\n",
    "        \n",
    "        # Normalize average positions gained (0-1)\n",
    "        avg_positions_gained = positions_gained[positions_gained > 0].mean() if len(positions_gained[positions_gained > 0]) > 0 else 0\n",
    "        max_possible_gain = 20  # Maximum grid positions that could be gained\n",
    "        normalized_gains = np.clip(avg_positions_gained / max_possible_gain, 0, 1)\n",
    "        \n",
    "        # Normalize risk factors (0-1)\n",
    "        normalized_dnf = np.clip(dnf_rate, 0, 1)\n",
    "        normalized_incidents = np.clip(incidents, 0, 1)\n",
    "        \n",
    "        # Calculate component scores (each between 0-1)\n",
    "        overtaking_component = (normalized_gains * 0.6 + overtake_success_rate * 0.4)\n",
    "        risk_component = (normalized_dnf * 0.5 + normalized_incidents * 0.5)\n",
    "        \n",
    "        # Combine components with weights (ensuring sum of weights = 1)\n",
    "        weights = {\n",
    "            'overtaking': 0.4,  # Aggressive overtaking\n",
    "            'risk': 0.5,       # Risk-taking behavior\n",
    "            'baseline': 0.1    # Baseline aggression\n",
    "        }\n",
    "        \n",
    "        aggression = (\n",
    "            overtaking_component * weights['overtaking'] +\n",
    "            risk_component * weights['risk'] +\n",
    "            0.5 * weights['baseline']  # Baseline aggression factor\n",
    "        )\n",
    "        \n",
    "        # Add small random variation while maintaining 0-1 bounds\n",
    "        variation = np.random.normal(0, 0.02)\n",
    "        aggression = np.clip(aggression + variation, 0, 1)\n",
    "        \n",
    "        return aggression\n",
    "    \n",
    "    def calculate_skill(driver_data, results_data, circuit_id):\n",
    "        driver_results = results_data[\n",
    "            (results_data['driverId'] == driver_data['driverId']) & \n",
    "            (results_data['circuitId'] == circuit_id)\n",
    "        ].sort_values('date', ascending=False).head(10)  # Use last 10 races at circuit\n",
    "        \n",
    "        if len(driver_results) == 0:\n",
    "            return 0.5  # Default skill\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        avg_finish_pos = driver_results['positionOrder'].mean()\n",
    "        avg_quali_pos = driver_results['grid'].mean()\n",
    "        points_per_race = driver_results['points'].mean()\n",
    "        fastest_laps = (driver_results['rank'] == 1).mean()  # Add fastest lap consideration\n",
    "        \n",
    "        # Improved normalization (exponential decay for positions)\n",
    "        normalized_finish_pos = np.exp(-avg_finish_pos/5) # Better spread of values\n",
    "        normalized_quali_pos = np.exp(-avg_quali_pos/5)\n",
    "        \n",
    "        # Points normalization with improved scaling\n",
    "        max_points_per_race = 26  # Maximum possible points (25 + 1 fastest lap)\n",
    "        normalized_points = points_per_race / max_points_per_race\n",
    "        \n",
    "        # Weighted combination with more factors\n",
    "        weights = {\n",
    "            'finish': 0.35,\n",
    "            'quali': 0.25,\n",
    "            'points': 0.25,\n",
    "            'fastest_laps': 0.15\n",
    "        }\n",
    "        \n",
    "        skill = (\n",
    "            weights['finish'] * normalized_finish_pos +\n",
    "            weights['quali'] * normalized_quali_pos +\n",
    "            weights['points'] * normalized_points +\n",
    "            weights['fastest_laps'] * fastest_laps\n",
    "        )\n",
    "        \n",
    "        # Add random variation to prevent identical skills\n",
    "        skill = np.clip(skill + np.random.normal(0, 0.05), 0.1, 1.0)\n",
    "        \n",
    "        return skill\n",
    "    \n",
    "    # First merge results with races to get circuitId\n",
    "    results = results.merge(\n",
    "        races[['raceId', 'circuitId']], \n",
    "        on='raceId',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Now calculate driver aggression and skill\n",
    "    driver_aggression = {}\n",
    "    driver_skill = {}\n",
    "    for driver_id in drivers['driverId'].unique():\n",
    "        driver_results = results[results['driverId'] == driver_id]\n",
    "        aggression = calculate_aggression(driver_results)\n",
    "        driver_aggression[driver_id] = aggression\n",
    "        \n",
    "        # Now we have circuit_id from the merge\n",
    "        recent_race = driver_results.sort_values('date', ascending=False).head(1)\n",
    "        if not recent_race.empty:\n",
    "            circuit_id = recent_race['circuitId'].iloc[0]\n",
    "            skill = calculate_skill({'driverId': driver_id}, results, circuit_id)\n",
    "            driver_skill[driver_id] = skill\n",
    "        else:\n",
    "            driver_skill[driver_id] = 0.5  # Default skill for new drivers\n",
    "    \n",
    "    # Map calculated aggression and skill back to laps DataFrame\n",
    "    laps['driver_aggression'] = laps['driverId'].map(driver_aggression)\n",
    "    laps['driver_overall_skill'] = laps['driverId'].map(driver_skill)\n",
    "    laps['driver_circuit_skill'] = laps['driver_overall_skill']  # For simplicity, using overall skill\n",
    "    laps['driver_consistency'] = 0.5  # Placeholder\n",
    "    laps['driver_reliability'] = 0.5  # Placeholder\n",
    "    laps['driver_risk_taking'] = laps['driver_aggression']  # Assuming similar to aggression\n",
    "    \n",
    "    # Dynamic features\n",
    "    laps['tire_age'] = laps.groupby(['raceId', 'driverId'])['lap'].cumcount()\n",
    "    laps['fuel_load'] = laps.groupby(['raceId', 'driverId'])['lap'].transform(lambda x: x.max() - x + 1)\n",
    "    laps['track_position'] = laps['position']  # Assuming 'position' is available in laps data\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    # Create an instance of RaceFeatures\n",
    "    race_features = RaceFeatures()\n",
    "\n",
    "    \n",
    "    laps['TrackStatus'].fillna(1, inplace=True)  # 1 = regular racing status\n",
    "    \n",
    "    # Ensure that all required columns are present\n",
    "    required_columns = race_features.static_features + race_features.dynamic_features\n",
    "    # Before dropping NaN values\n",
    "    print(\"\\nNaN counts in required columns:\")\n",
    "    for col in required_columns:\n",
    "        nan_count = laps[col].isna().sum()\n",
    "        total_rows = len(laps)\n",
    "        if nan_count > 0:\n",
    "            print(f\"{col}: {nan_count} NaN values ({(nan_count/total_rows*100):.2f}% of rows)\")\n",
    "    missing_columns = set(required_columns) - set(laps.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Drop rows with missing values in required columns\n",
    "    laps.to_csv('laps_withNan.csv', index=False)\n",
    "    laps = laps.dropna(subset=required_columns)\n",
    "    \n",
    "    print(laps.shape)\n",
    "    \n",
    "    return laps\n",
    "\n",
    "# Update the main function\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    enhanced_laps = load_and_preprocess_data()\n",
    "    \n",
    "    enhanced_laps.drop(columns=['position', 'time', 'driverRef', 'number', 'R', 'S', 'code', 'forename', 'surname', 'url_x', 'url_race', 'name_x', 'circuitRef', 'name_y', 'location', 'country', 'url_y', 'positionOrder', 'fastestLap', 'cumulative_milliseconds', 'seconds_from_start', 'raceId_x', 'year_x', 'Time', 'TrackTemp', 'AirTemp', 'Humidity', 'name', 'year_y', 'raceId_y'], inplace=True)\n",
    "    \n",
    "    # Save the preprocessed laps DataFrame for inspection\n",
    "    enhanced_laps.to_csv('enhanced_laps_before_training.csv', index=False)\n",
    "    print(enhanced_laps.shape)\n",
    "    \n",
    "    print(\"Enhanced laps DataFrame saved to 'enhanced_laps_before_training.csv'\")\n",
    "    \n",
    "    preprocessor = F1DataPreprocessor()\n",
    "    sequences, static, targets = preprocessor.prepare_sequence_data(enhanced_laps, window_size=3)\n",
    "    \n",
    "    # Create train and validation loaders\n",
    "    train_loader, val_loader = preprocessor.create_train_val_loaders(\n",
    "        sequences, \n",
    "        static, \n",
    "        targets,\n",
    "        batch_size=32,\n",
    "        val_split=0.2\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = F1PredictionModel(\n",
    "        sequence_dim=sequences.shape[2],\n",
    "        static_dim=static.shape[1]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = train_model(model, train_loader, val_loader, epochs=100, learning_rate=0.001)\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model_with_preprocessor(model, preprocessor, sequences.shape[2], static.shape[1], 'f1_prediction_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# %% [markdown]\n",
    "# # Phase 2\n",
    "\n",
    "# %%\n",
    "class RaceSimulator:\n",
    "    def __init__(self, model, preprocessor):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.race_features = RaceFeatures()\n",
    "        \n",
    "    def simulate_lap(self, current_state):\n",
    "        \"\"\"\n",
    "        Predict lap time and uncertainty for a single lap.\n",
    "        \"\"\"\n",
    "        # Prepare input data\n",
    "        sequence = current_state['sequence']\n",
    "        static = current_state['static']\n",
    "        \n",
    "        # Convert to tensors\n",
    "        sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0)  # Add batch dimension\n",
    "        static_tensor = torch.FloatTensor(static).unsqueeze(0)\n",
    "        \n",
    "        # Predict with uncertainty\n",
    "        mean_pred, std_pred = predict_with_uncertainty(\n",
    "            self.model, \n",
    "            {'sequence': sequence_tensor, 'static': static_tensor},\n",
    "            n_samples=50\n",
    "        )\n",
    "        \n",
    "        # Inverse transform the prediction\n",
    "        lap_time = self.preprocessor.lap_time_scaler.inverse_transform([[mean_pred]])[0][0]\n",
    "        uncertainty = self.preprocessor.lap_time_scaler.inverse_transform([[std_pred]])[0][0]\n",
    "        \n",
    "        return lap_time, uncertainty\n",
    "    \n",
    "    def simulate_full_race(self, initial_state, strategy):\n",
    "        \"\"\"\n",
    "        Simulate the entire race lap by lap based on the provided strategy.\n",
    "        \"\"\"\n",
    "        lap_times = []\n",
    "        uncertainties = []\n",
    "        current_state = initial_state.copy()\n",
    "        \n",
    "        total_laps = strategy.total_laps\n",
    "        for lap in range(1, total_laps + 1):\n",
    "            # Update dynamic features based on strategy and lap number\n",
    "            current_state = self.update_state(current_state, lap, strategy)\n",
    "            \n",
    "            # Simulate lap\n",
    "            lap_time, uncertainty = self.simulate_lap(current_state)\n",
    "            lap_times.append(lap_time)\n",
    "            uncertainties.append(uncertainty)\n",
    "            \n",
    "            # Update sequence data for the next lap\n",
    "            current_state['sequence'] = self.update_sequence(\n",
    "                current_state['sequence'], lap_time, current_state['dynamic']\n",
    "            )\n",
    "        \n",
    "        return lap_times, uncertainties\n",
    "    \n",
    "    def update_state(self, state, lap, strategy):\n",
    "        \"\"\"\n",
    "        Update the state for the next lap based on the strategy.\n",
    "        \"\"\"\n",
    "        # Update tire age\n",
    "        state['dynamic']['tire_age'] += 1\n",
    "        \n",
    "        # Check for pit stops\n",
    "        if lap in strategy.pit_stop_laps:\n",
    "            # Reset tire age and update tire compound\n",
    "            state['dynamic']['tire_age'] = 0\n",
    "            state['dynamic']['tire_compound'] = strategy.pit_stop_compounds[lap]\n",
    "        \n",
    "        # Update fuel load\n",
    "        state['dynamic']['fuel_load'] -= strategy.fuel_consumption_per_lap\n",
    "        \n",
    "        # Update other dynamic features as needed\n",
    "        # ...\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def update_sequence(self, sequence, new_lap_time, dynamic_features):\n",
    "        \"\"\"\n",
    "        Update the sequence data with the latest lap information.\n",
    "        \"\"\"\n",
    "        # Remove the oldest lap data\n",
    "        sequence = sequence[1:]\n",
    "        # Append the new lap data\n",
    "        new_sequence_entry = np.hstack((\n",
    "            [new_lap_time],\n",
    "            [dynamic_features[feature] for feature in self.race_features.dynamic_features]\n",
    "        ))\n",
    "        sequence = np.vstack([sequence, new_sequence_entry])\n",
    "        return sequence\n",
    "    \n",
    "    # Additional helper methods as needed\n",
    "\n",
    "\n",
    "# %%\n",
    "class RaceStrategy:\n",
    "    def __init__(self, total_laps, pit_stops):\n",
    "        \"\"\"\n",
    "        pit_stops: List of dictionaries with keys 'lap' and 'compound'\n",
    "        Example: [{'lap': 15, 'compound': 'MEDIUM'}, {'lap': 30, 'compound': 'SOFT'}]\n",
    "        \"\"\"\n",
    "        self.total_laps = total_laps\n",
    "        self.pit_stops = pit_stops  # List of pit stop events\n",
    "        self.pit_stop_laps = [stop['lap'] for stop in pit_stops]\n",
    "        self.pit_stop_compounds = {stop['lap']: stop['compound'] for stop in pit_stops}\n",
    "        self.fuel_consumption_per_lap = 1.5  # Example value, adjust as needed\n",
    "    \n",
    "    def evaluate(self, simulator, initial_state):\n",
    "        \"\"\"\n",
    "        Simulate the race using this strategy and return total race time.\n",
    "        \"\"\"\n",
    "        lap_times, uncertainties = simulator.simulate_full_race(initial_state, self)\n",
    "        total_time = sum(lap_times)\n",
    "        return total_time, lap_times, uncertainties\n",
    "\n",
    "\n",
    "# %%\n",
    "class RaceStrategyOptimizer:\n",
    "    def __init__(self, simulator):\n",
    "        self.simulator = simulator\n",
    "    \n",
    "    def optimize(self, initial_conditions, constraints):\n",
    "        \"\"\"\n",
    "        Find the optimal strategy given initial conditions and constraints.\n",
    "        \"\"\"\n",
    "        best_strategy = None\n",
    "        best_time = float('inf')\n",
    "        \n",
    "        # Generate possible strategies within constraints\n",
    "        possible_strategies = self.generate_strategies(constraints)\n",
    "        \n",
    "        # Evaluate each strategy\n",
    "        for strategy in possible_strategies:\n",
    "            total_time, _, _ = strategy.evaluate(self.simulator, initial_conditions)\n",
    "            if total_time < best_time:\n",
    "                best_time = total_time\n",
    "                best_strategy = strategy\n",
    "        \n",
    "        return best_strategy\n",
    "    \n",
    "    def generate_strategies(self, constraintas):\n",
    "        \"\"\"\n",
    "        Generate possible strategies based on constraints.\n",
    "        \"\"\"\n",
    "        min_pit_stops = constraints.get('min_pit_stops', 1)\n",
    "        max_pit_stops = constraints.get('max_pit_stops', 3)\n",
    "        available_compounds = constraints.get('available_compounds', ['SOFT', 'MEDIUM', 'HARD'])\n",
    "        total_laps = constraints.get('total_laps', 50)\n",
    "        \n",
    "        strategies = []\n",
    "        \n",
    "        # Example: Generate strategies with different pit stop laps and compounds\n",
    "        for num_pit_stops in range(min_pit_stops, max_pit_stops + 1):\n",
    "            pit_stop_laps_options = self.get_pit_stop_lap_combinations(total_laps, num_pit_stops)\n",
    "            for pit_stop_laps in pit_stop_laps_options:\n",
    "                for compounds in self.get_compound_combinations(available_compounds, num_pit_stops):\n",
    "                    pit_stops = [{'lap': lap, 'compound': comp} for lap, comp in zip(pit_stop_laps, compounds)]\n",
    "                    strategy = RaceStrategy(total_laps, pit_stops)\n",
    "                    strategies.append(strategy)\n",
    "        return strategies\n",
    "    \n",
    "    def get_pit_stop_lap_combinations(self, total_laps, num_pit_stops):\n",
    "        \"\"\"\n",
    "        Generate possible pit stop lap combinations.\n",
    "        \"\"\"\n",
    "        from itertools import combinations\n",
    "        lap_numbers = range(5, total_laps - 5)  # Avoid pitting too early or too late\n",
    "        return combinations(lap_numbers, num_pit_stops)\n",
    "    \n",
    "    def get_compound_combinations(self, compounds, num_pit_stops):\n",
    "        \"\"\"\n",
    "        Generate possible combinations of compounds for pit stops.\n",
    "        \"\"\"\n",
    "        from itertools import product\n",
    "        return product(compounds, repeat=num_pit_stops)\n",
    "    \n",
    "    def load_model_with_preprocessor(path: str, sequence_dim: int, static_dim: int, hidden_dim: int = 64, num_layers: int = 2):\n",
    "        \"\"\"\n",
    "        Load the model and preprocessor from a saved file.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "        \n",
    "        # Recreate the model architecture\n",
    "        model = F1PredictionModel(\n",
    "            sequence_dim=sequence_dim,\n",
    "            static_dim=static_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        # Recreate the preprocessor\n",
    "        preprocessor = F1DataPreprocessor()\n",
    "        preprocessor.lap_time_scaler = checkpoint['lap_time_scaler']\n",
    "        preprocessor.dynamic_scaler = checkpoint['dynamic_scaler']\n",
    "        preprocessor.static_scaler = checkpoint['static_scaler']\n",
    "        \n",
    "        print(f\"Model and preprocessor loaded from {path}\")\n",
    "        return model, preprocessor\n",
    "\n",
    "\n",
    "# %%\n",
    "def load_model_with_preprocessor(path: str) -> Tuple[F1PredictionModel, F1DataPreprocessor]:\n",
    "   \"\"\"Load saved model and preprocessor\"\"\"\n",
    "   checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "   \n",
    "   model = F1PredictionModel(\n",
    "       sequence_dim=checkpoint['sequence_dim'], \n",
    "       static_dim=checkpoint['static_dim']\n",
    "   )\n",
    "   model.load_state_dict(checkpoint['model_state_dict'])\n",
    "   \n",
    "   preprocessor = F1DataPreprocessor()\n",
    "   preprocessor.lap_time_scaler = checkpoint['lap_time_scaler']\n",
    "   preprocessor.dynamic_scaler = checkpoint['dynamic_scaler']\n",
    "   preprocessor.static_scaler = checkpoint['static_scaler']\n",
    "   \n",
    "   return model, preprocessor\n",
    "\n",
    "# %%\n",
    "# The model and preprocessor are now ready for simulation or further training\n",
    "model, preprocessor = load_model_with_preprocessor('f1_prediction_model.pth')\n",
    "\n",
    "# Assume 'model' and 'preprocessor' have been loaded or trained\n",
    "simulator = RaceSimulator(model, preprocessor)\n",
    "\n",
    "# Step 1: Define static features\n",
    "static_features_list = [\n",
    "    'driver_overall_skill', 'driver_circuit_skill', 'driver_consistency',\n",
    "    'driver_reliability', 'driver_aggression', 'driver_risk_taking',\n",
    "    'fp1_median_time', 'fp2_median_time', 'fp3_median_time', 'quali_time'\n",
    "]\n",
    "\n",
    "static_features = np.array([\n",
    "    0.8,   # driver_overall_skill\n",
    "    0.75,  # driver_circuit_skill\n",
    "    0.7,   # driver_consistency\n",
    "    0.9,   # driver_reliability\n",
    "    0.6,   # driver_aggression\n",
    "    0.5,   # driver_risk_taking\n",
    "    88000, # fp1_median_time\n",
    "    87500, # fp2_median_time\n",
    "    87000, # fp3_median_time\n",
    "    86000  # quali_time\n",
    "])\n",
    "\n",
    "# Step 2: Define initial dynamic features for previous laps\n",
    "dynamic_features_list = [\n",
    "    'tire_age', 'fuel_load', 'track_position', 'track_temp',\n",
    "    'air_temp', 'humidity', 'TrackStatus', 'is_pit_lap'\n",
    "]\n",
    "\n",
    "# Common values\n",
    "track_temp = 35.0\n",
    "air_temp = 25.0\n",
    "humidity = 50.0\n",
    "TrackStatus = 1\n",
    "is_pit_lap = 0\n",
    "\n",
    "dynamic_features = np.array([\n",
    "    [0, 100, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap],  # Lap 1\n",
    "    [1, 98.5, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap],  # Lap 2\n",
    "    [2, 97, 1, track_temp, air_temp, humidity, TrackStatus, is_pit_lap]     # Lap 3\n",
    "])\n",
    "\n",
    "# Tire compound (not scaled)\n",
    "tire_compound = 0  # Example value for 'HARD' tires\n",
    "tire_compound_column = np.full((dynamic_features.shape[0], 1), tire_compound)\n",
    "\n",
    "# Lap times in milliseconds\n",
    "lap_times = np.array([90000, 89500, 89200])  # Laps 1-3\n",
    "\n",
    "# Step 3: Combine lap times, tire_compound, and dynamic features\n",
    "sequence = np.hstack((\n",
    "    lap_times.reshape(-1, 1),  # Lap times\n",
    "    tire_compound_column,      # Tire compound\n",
    "    dynamic_features           # Dynamic features\n",
    "))\n",
    "\n",
    "# Step 4: Scale the lap times\n",
    "lap_times_scaled = preprocessor.lap_time_scaler.transform(lap_times.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Step 5: Scale the dynamic features (excluding 'tire_compound')\n",
    "dynamic_features_to_scale = sequence[:, 2:]  # Exclude lap times and 'tire_compound'\n",
    "dynamic_scaled = preprocessor.dynamic_scaler.transform(dynamic_features_to_scale)\n",
    "\n",
    "# Step 6: Reconstruct the scaled sequence\n",
    "sequence_scaled = np.hstack((\n",
    "    lap_times_scaled.reshape(-1, 1),  # Scaled lap times\n",
    "    tire_compound_column,             # Tire compound (not scaled)\n",
    "    dynamic_scaled                    # Scaled dynamic features\n",
    "))\n",
    "\n",
    "# Step 7: Scale the static features\n",
    "static_scaled = preprocessor.static_scaler.transform(static_features.reshape(1, -1)).flatten()\n",
    "\n",
    "# Step 8: Prepare current dynamic features\n",
    "current_dynamic_features = {\n",
    "    'tire_age': 3,\n",
    "    'fuel_load': 95.5,\n",
    "    'track_position': 1,\n",
    "    'track_temp': 35.0,\n",
    "    'air_temp': 25.0,\n",
    "    'humidity': 50.0,\n",
    "    'TrackStatus': 1,\n",
    "    'is_pit_lap': 0,\n",
    "    'tire_compound': 0\n",
    "}\n",
    "\n",
    "# Extract and scale dynamic features (excluding 'tire_compound')\n",
    "dynamic_feature_values = np.array([\n",
    "    current_dynamic_features[feature] for feature in dynamic_features_list\n",
    "])\n",
    "\n",
    "dynamic_scaled_current = preprocessor.dynamic_scaler.transform(dynamic_feature_values.reshape(1, -1)).flatten()\n",
    "\n",
    "# Step 9: Update the initial state\n",
    "initial_state = {\n",
    "    'sequence': sequence_scaled,   # Shape: (window_size, sequence_dim)\n",
    "    'static': static_scaled,       # Shape: (static_dim,)\n",
    "    'dynamic': current_dynamic_features\n",
    "}\n",
    "\n",
    "\n",
    "# Define constraints\n",
    "constraints = {\n",
    "    'min_pit_stops': 1,\n",
    "    'max_pit_stops': 2,\n",
    "    'available_compounds': [3, 2, 1],\n",
    "    'total_laps': 50\n",
    "}\n",
    "\n",
    "# Create optimizer and find the best strategy\n",
    "optimizer = RaceStrategyOptimizer(simulator)\n",
    "best_strategy = optimizer.optimize(initial_state, constraints)\n",
    "\n",
    "# Evaluate the best strategy\n",
    "total_time, lap_times, uncertainties = best_strategy.evaluate(simulator, initial_state)\n",
    "print(f\"Optimal total race time: {total_time:.2f} seconds\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "First of all, I need somekind of logging or process bar during the simulations.\n",
    "\n",
    "Second, I think if the model is working so far (we can try out other models later)\n",
    "\n",
    "The goal now would be to not only have a tool to find out the best strategy, but also to predict whole races. I image it like this:\n",
    "Inputs:\n",
    "-data from before, free practice etc.\n",
    "-circuitId to tell the model which circuit the race is on\n",
    "-weatherforecast: in our case we could just do a train test split with the races (including all their laps) and since we know the weather data, we can just assume that that is the forecast\n",
    "-which drivers will drive for which constructor through driverId (maybe each driver as an agent with (skill. agression etc.?)\n",
    "-\n",
    "-Pitstop, I assume optimizing to get the best strategy for each driver would take a while, for now lets just say we predefined one strategy for all of them (when to stop and what compounds)\n",
    "\n",
    "When we introduce these new variables we probably need to add them the model as well?\n",
    "\n",
    "Third, I think we need to introduce some like Global Race Events i.e Retirements, Safetey car etc. One could probably train models for this as well, e.g. which lap a safety car is most likely, how likely and when a driver / car will retire. But for now we maybe can also just predefined those things.\n",
    "structure all the ideas I just outlined and think about whats missing and in which steps to implement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b608e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
