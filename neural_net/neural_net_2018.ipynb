{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network implementation for Racetime prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pytorch_lightning.loggers import Logger\n",
    "from pytorch_lightning.utilities.rank_zero import rank_zero_only\n",
    "\n",
    "class F1NeuralNetworkExperimentsLogger2018(Logger):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "        self.logs = {}\n",
    "        self._version = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"F1NeuralNetworkExperimentsLogger2018\"\n",
    "\n",
    "    @property\n",
    "    def version(self):\n",
    "        return self._version\n",
    "    \n",
    "    @rank_zero_only\n",
    "    def log_model_architecture(self, model):\n",
    "        architecture = str(model)\n",
    "        self.logs['model_architecture'] = architecture\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_hyperparams(self, params):\n",
    "        self.logs['hyperparameters'] = {k: str(v) for k, v in params.items()}\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_metrics(self, metrics, step):\n",
    "        self.metrics.append((step, metrics))\n",
    "        if 'metrics' not in self.logs:\n",
    "            self.logs['metrics'] = []\n",
    "        self.logs['metrics'].append({'step': step, 'metrics': {k: float(v) for k, v in metrics.items()}})\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_overall_test_loss(self, test_loss):\n",
    "        self.logs['overall_test_loss'] = test_loss\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_used_features(self, features):\n",
    "        self.logs['used_features'] = features\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_optimization_strategy(self, optimizer, scheduler):\n",
    "        optimizer_str = str(optimizer)\n",
    "        scheduler_str = str(scheduler)\n",
    "        self.logs['optimizer'] = optimizer_str\n",
    "        self.logs['scheduler'] = scheduler_str\n",
    "\n",
    "    @rank_zero_only\n",
    "    def save(self):\n",
    "        directory = os.path.join(self.name, self.version)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open(os.path.join(directory, \"logs.json\"), \"w\") as f:\n",
    "            json.dump(self.logs, f, indent=4)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def finalize(self, status):\n",
    "        self.save()\n",
    "\n",
    "# Example usage\n",
    "logger = F1NeuralNetworkExperimentsLogger2018()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select which features to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "features_filepath = os.path.join(DATA_DIR, \"train/train.csv\")\n",
    "labels_filepath = os.path.join(DATA_DIR, \"train/train_labels.csv\")\n",
    "features = pd.read_csv(features_filepath)\n",
    "labels = pd.read_csv(labels_filepath)\n",
    "\n",
    "# Filter data for entries where the year is greater than 2018\n",
    "features_filtered = features[features[\"year\"] > 2018]\n",
    "\n",
    "# Ensure all data is numeric and handle missing values\n",
    "USED_FEATURES = features_filtered.columns.drop(['year'])\n",
    "x = features_filtered[USED_FEATURES].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# Create a DataFrame from the scaled data\n",
    "df_scaled = pd.DataFrame(x_scaled, columns=USED_FEATURES)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_scaled.corr()\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used features\n",
    "Based on the heatmaps, I decided to use the following features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USED_FEATURES = [\n",
    "    \"fp3_avg_is_on_fresh_tyres\",\n",
    "    \"fp1_avg_sector_2\",\n",
    "    \"fp1_avg_lap_time\",\n",
    "    \"fp1_avg_speedST\",\n",
    "    \"fp2_avg_sector_1\",\n",
    "    \"fp2_avg_sector_2\",\n",
    "    \"fp2_avg_lap_time\",\n",
    "    \"fp2_avg_speedST\",\n",
    "    \"fp3_avg_sector_1\",\n",
    "    \"fp3_avg_sector_2\",\n",
    "    \"fp3_avg_lap_time\",\n",
    "    \"fp3_avg_speedST\",\n",
    "    \"q1_time\",\n",
    "    \"q2_time\",\n",
    "    \"q3_time\",\n",
    "    \"q_position\",\n",
    "    \"min_pit_stop_duration\",\n",
    "    \"avg_pit_stop_duration\",\n",
    "    \"rolling_avg_pit_stop_driver\",\n",
    "    \"rolling_avg_pit_stop_team\",\n",
    "    \"race_lat\",\n",
    "    \"race_lng\",\n",
    "    \"race_alt\",\n",
    "    \"race_date\",\n",
    "    \"year\",\n",
    "    \"round\",\n",
    "    \"driver_nationality\",\n",
    "    \"constructorId\"\n",
    "]\n",
    "logger.log_used_features(USED_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class F1Dataset(Dataset):\n",
    "    def __init__(self, features_filepath, labels_filepath, year_from):\n",
    "        super().__init__()\n",
    "        self.year_from = year_from\n",
    "\n",
    "        # Load the data\n",
    "        features = pd.read_csv(features_filepath)\n",
    "        labels = pd.read_csv(labels_filepath)\n",
    "\n",
    "        # Filter data for entries where the year is greater than year_from\n",
    "        features_filtered = features[features[\"year\"] > self.year_from]\n",
    "        labels_filtered = labels[features[\"year\"] > self.year_from]\n",
    "\n",
    "        # Ensure all data is numeric and handle missing values\n",
    "        x = features_filtered[USED_FEATURES].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "        # Normalize the data\n",
    "        scaler = StandardScaler()\n",
    "        self.x = scaler.fit_transform(x)\n",
    "        self.y = labels_filtered[\"positionOrder\"].apply(pd.to_numeric, errors='coerce').fillna(0).to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.x[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = F1Dataset(os.path.join(DATA_DIR, \"train/train.csv\"), os.path.join(DATA_DIR, \"train/train_labels.csv\"), year_from=2018)\n",
    "test_dataset = F1Dataset(os.path.join(DATA_DIR, \"test/test.csv\"), os.path.join(DATA_DIR, \"test/test_labels.csv\"), year_from=2018)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "data_2018_onwards = F1Dataset(\n",
    "    os.path.join(DATA_DIR, \"train/train.csv\"),\n",
    "    os.path.join(DATA_DIR, \"train/train_labels.csv\"),\n",
    "    2018,\n",
    ")\n",
    "\n",
    "data_2018_onwards = F1Dataset(os.path.join(DATA_DIR, \"train/train.csv\"), os.path.join(DATA_DIR, \"train/train_labels.csv\"), 2018)\n",
    "data_2018_onwards.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PositionPredictionModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim=len(USED_FEATURES), hidden_dim=64, output_dim=1, lr=1e-3, loss_fn=nn.L1Loss):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=True)\n",
    "        self.predictions = []\n",
    "        self.actuals = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.test_losses = []\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
    "            nn.BatchNorm1d(hidden_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "        logger.log_model_architecture(self.model)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.loss_fn = loss_fn()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x).squeeze(-1)  # Remove the extra dimension\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x).squeeze(-1)  # Remove the extra dimension\n",
    "        test_loss_fn = nn.MSELoss()\n",
    "        loss = torch.sqrt(test_loss_fn(y_hat, y))\n",
    "        self.predictions.extend(y_hat.cpu().numpy())\n",
    "        self.actuals.extend(y.cpu().numpy())\n",
    "        self.test_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        # Calculate and log the average test loss\n",
    "        avg_test_loss = sum(self.test_losses) / len(self.test_losses)\n",
    "        print(f\"Average Test Loss: {avg_test_loss}\")\n",
    "        logger.log_overall_test_loss(avg_test_loss)\n",
    "        \n",
    "        # Plot the correlation between actual and predicted values\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(self.actuals, self.predictions, alpha=0.6, edgecolor='k')\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title('Correlation between Actual and Predicted Values')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot the loss curve\n",
    "        self.plot_loss_curve()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters())\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.lr,\n",
    "            steps_per_epoch=100,\n",
    "            epochs=10,\n",
    "            anneal_strategy='cos',\n",
    "        )\n",
    "        logger.log_optimization_strategy(str(optimizer), str(scheduler))\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "\n",
    "    def plot_loss_curve(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "model = PositionPredictionModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=50, logger=logger)\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
