{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "sys.path.append('../')\n",
    "import datetime\n",
    "from models.xgboost.xgboost import F1XGBoostPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-08 11:43:23,324] A new study created in memory with name: no-name-9c18c3ea-7bdc-4234-b1a5-759a3ac24bc1\n",
      "[I 2024-12-08 11:43:25,181] Trial 0 finished with value: 6909.208211597503 and parameters: {'max_depth': 4, 'learning_rate': 0.0015485890196237778, 'n_estimators': 578, 'min_child_weight': 1, 'subsample': 0.689507634685872, 'colsample_bytree': 0.8775055431411992, 'reg_alpha': 0.005009544984592522, 'reg_lambda': 4.800552887262388e-06}. Best is trial 0 with value: 6909.208211597503.\n",
      "[I 2024-12-08 11:43:27,059] Trial 1 finished with value: 4671.194747361591 and parameters: {'max_depth': 3, 'learning_rate': 0.025981542680047296, 'n_estimators': 777, 'min_child_weight': 2, 'subsample': 0.8754294109962378, 'colsample_bytree': 0.9081774592003278, 'reg_alpha': 6.422989460626791e-05, 'reg_lambda': 1.7001814628149523e-07}. Best is trial 1 with value: 4671.194747361591.\n",
      "[I 2024-12-08 11:43:28,580] Trial 2 finished with value: 6649.309586137278 and parameters: {'max_depth': 4, 'learning_rate': 0.001894195533016858, 'n_estimators': 498, 'min_child_weight': 4, 'subsample': 0.8717102061378756, 'colsample_bytree': 0.7114090875510541, 'reg_alpha': 0.0023330927030687444, 'reg_lambda': 0.009017556188086489}. Best is trial 1 with value: 4671.194747361591.\n",
      "[I 2024-12-08 11:43:29,523] Trial 3 finished with value: 4608.416695541294 and parameters: {'max_depth': 4, 'learning_rate': 0.07800884417612376, 'n_estimators': 338, 'min_child_weight': 4, 'subsample': 0.9995544743256066, 'colsample_bytree': 0.6766304466638392, 'reg_alpha': 0.014398142800011763, 'reg_lambda': 0.0019981823566170244}. Best is trial 3 with value: 4608.416695541294.\n",
      "[I 2024-12-08 11:43:30,487] Trial 4 finished with value: 8362.19460686579 and parameters: {'max_depth': 7, 'learning_rate': 0.002210393054495413, 'n_estimators': 197, 'min_child_weight': 7, 'subsample': 0.7134074234267944, 'colsample_bytree': 0.7666780403539094, 'reg_alpha': 0.04480867403137816, 'reg_lambda': 0.007524234129846315}. Best is trial 3 with value: 4608.416695541294.\n",
      "[I 2024-12-08 11:43:31,267] Trial 5 finished with value: 6727.807100628286 and parameters: {'max_depth': 7, 'learning_rate': 0.005328863769405303, 'n_estimators': 160, 'min_child_weight': 4, 'subsample': 0.7809437690122929, 'colsample_bytree': 0.681154720300021, 'reg_alpha': 3.961586511047889e-06, 'reg_lambda': 1.9182533185637785e-06}. Best is trial 3 with value: 4608.416695541294.\n",
      "[I 2024-12-08 11:43:32,450] Trial 6 finished with value: 5605.450729212671 and parameters: {'max_depth': 7, 'learning_rate': 0.006126694722000775, 'n_estimators': 233, 'min_child_weight': 7, 'subsample': 0.7420800695791169, 'colsample_bytree': 0.8057733488552218, 'reg_alpha': 0.011709164980192055, 'reg_lambda': 0.00035829047955806425}. Best is trial 3 with value: 4608.416695541294.\n",
      "[I 2024-12-08 11:43:41,148] Trial 7 finished with value: 7477.245587655169 and parameters: {'max_depth': 12, 'learning_rate': 0.00102852576955777, 'n_estimators': 596, 'min_child_weight': 2, 'subsample': 0.8864237677630138, 'colsample_bytree': 0.6352564754506466, 'reg_alpha': 0.0022292900978089425, 'reg_lambda': 0.014456385638578652}. Best is trial 3 with value: 4608.416695541294.\n",
      "[I 2024-12-08 11:43:43,139] Trial 8 finished with value: 5791.733879521454 and parameters: {'max_depth': 10, 'learning_rate': 0.0058472476470469645, 'n_estimators': 200, 'min_child_weight': 3, 'subsample': 0.6568757609265373, 'colsample_bytree': 0.7480994880130906, 'reg_alpha': 3.4230601340059825e-06, 'reg_lambda': 1.5283200019157866e-07}. Best is trial 3 with value: 4608.416695541294.\n",
      "[I 2024-12-08 11:43:44,504] Trial 9 finished with value: 5103.783680572256 and parameters: {'max_depth': 4, 'learning_rate': 0.005584741108957918, 'n_estimators': 377, 'min_child_weight': 1, 'subsample': 0.7433544537422984, 'colsample_bytree': 0.6559402893934528, 'reg_alpha': 0.02404179981062521, 'reg_lambda': 0.22896252354082644}. Best is trial 3 with value: 4608.416695541294.\n"
     ]
    }
   ],
   "source": [
    "# Load your processed data\n",
    "with open('processed_race_data.pkl', 'rb') as f:\n",
    "    processed_data = pickle.load(f)\n",
    "\n",
    "# Initialize and optimize\n",
    "predictor = F1XGBoostPredictor(processed_data)\n",
    "study = predictor.optimize(n_trials=250)\n",
    "\n",
    "# Train the model with best parameters\n",
    "predictor.train(study)\n",
    "\n",
    "# Evaluate performance\n",
    "metrics = predictor.evaluate()\n",
    "print(f\"Test RMSE: {metrics['rmse']:.2f} ms\")\n",
    "print(f\"Test MAE: {metrics['mae']:.2f} ms\")\n",
    "\n",
    "# Save the model\n",
    "predictor.save_model('f1_xgboost_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def analyze_feature_importance(predictor, processed_data: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive feature importance analysis using multiple methods.\n",
    "    \n",
    "    Args:\n",
    "        predictor: Trained XGBoost predictor\n",
    "        processed_data: Dictionary containing processed feature data\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing various feature importance metrics\n",
    "    \"\"\"\n",
    "    # Get feature names\n",
    "    feature_names = predictor.static_features + predictor.dynamic_features\n",
    "    \n",
    "    # Get feature importance from XGBoost\n",
    "    xgb_importance = predictor.model.feature_importances_\n",
    "    \n",
    "    # Calculate correlation with target\n",
    "    features = processed_data['train']['features']\n",
    "    targets = processed_data['train']['targets']\n",
    "    \n",
    "    # Calculate Spearman correlation for each feature\n",
    "    correlations = []\n",
    "    for i in range(features.shape[1]):\n",
    "        correlation, _ = spearmanr(features[:, i], targets)\n",
    "        correlations.append(abs(correlation))  # Use absolute correlation\n",
    "    \n",
    "    # Calculate feature stability (variance across different subsets)\n",
    "    n_splits = 5\n",
    "    split_size = len(features) // n_splits\n",
    "    stability_scores = []\n",
    "    \n",
    "    for i in range(features.shape[1]):\n",
    "        importances = []\n",
    "        for j in range(n_splits):\n",
    "            start_idx = j * split_size\n",
    "            end_idx = (j + 1) * split_size\n",
    "            subset_features = features[start_idx:end_idx]\n",
    "            subset_targets = targets[start_idx:end_idx]\n",
    "            \n",
    "            # Calculate correlation for this subset\n",
    "            corr, _ = spearmanr(subset_features[:, i], subset_targets)\n",
    "            importances.append(abs(corr))\n",
    "        \n",
    "        stability_scores.append(1 - np.std(importances))\n",
    "    \n",
    "    # Combine all metrics\n",
    "    feature_metrics = []\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_metrics.append({\n",
    "            'feature': feature_name,\n",
    "            'xgb_importance': xgb_importance[i],\n",
    "            'correlation': correlations[i],\n",
    "            'stability': stability_scores[i],\n",
    "            # Combined score giving equal weight to all metrics\n",
    "            'combined_score': (\n",
    "                0.4 * xgb_importance[i] + \n",
    "                0.4 * correlations[i] + \n",
    "                0.2 * stability_scores[i]\n",
    "            )\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(feature_metrics).sort_values('combined_score', ascending=False)\n",
    "\n",
    "def plot_feature_importance(importance_df: pd.DataFrame, top_n: int = 20):\n",
    "    \"\"\"\n",
    "    Create visualizations for feature importance analysis.\n",
    "    \n",
    "    Args:\n",
    "        importance_df: DataFrame containing feature importance metrics\n",
    "        top_n: Number of top features to display\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot top features by combined score\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    sns.barplot(data=top_features, x='combined_score', y='feature')\n",
    "    plt.title(f'Top {top_n} Features by Combined Importance Score')\n",
    "    plt.xlabel('Combined Importance Score')\n",
    "    \n",
    "    # Plot correlation between different importance metrics\n",
    "    plt.subplot(2, 1, 2)\n",
    "    importance_metrics = ['xgb_importance', 'correlation', 'stability']\n",
    "    correlation_matrix = importance_df[importance_metrics].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation between Importance Metrics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def get_feature_recommendations(importance_df: pd.DataFrame, \n",
    "                              threshold: float = 0.01) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Provide recommendations for feature selection.\n",
    "    \n",
    "    Args:\n",
    "        importance_df: DataFrame containing feature importance metrics\n",
    "        threshold: Minimum importance threshold for keeping features\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing (features to keep, features to consider removing)\n",
    "    \"\"\"\n",
    "    # Features to definitely keep (high importance or correlation)\n",
    "    keep_features = importance_df[\n",
    "        (importance_df['combined_score'] > threshold) |\n",
    "        (importance_df['correlation'] > threshold * 2)\n",
    "    ]['feature'].tolist()\n",
    "    \n",
    "    # Features to consider removing\n",
    "    remove_features = importance_df[\n",
    "        (importance_df['combined_score'] <= threshold) &\n",
    "        (importance_df['correlation'] <= threshold * 2)\n",
    "    ]['feature'].tolist()\n",
    "    \n",
    "    return keep_features, remove_features\n",
    "\n",
    "# Example usage after optimization:\n",
    "\n",
    "# Run analysis\n",
    "importance_df = analyze_feature_importance(predictor, processed_data)\n",
    "\n",
    "# Plot results\n",
    "plot_feature_importance(importance_df)\n",
    "\n",
    "# Get recommendations\n",
    "keep_features, remove_features = get_feature_recommendations(importance_df)\n",
    "\n",
    "print(\"\\nRecommended features to keep:\")\n",
    "print(\"\\n\".join(f\"- {feature}\" for feature in keep_features))\n",
    "\n",
    "print(\"\\nFeatures to consider removing:\")\n",
    "print(\"\\n\".join(f\"- {feature}\" for feature in remove_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_physical_patterns(predictions, actual, metadata):\n",
    "    \"\"\"Validate if predictions follow expected F1 physics patterns.\"\"\"\n",
    "    # Group by race and driver\n",
    "    races = metadata.groupby(['raceId', 'driverId'])\n",
    "    \n",
    "    patterns = {\n",
    "        'fuel_effect': [],\n",
    "        'tire_effect': [],\n",
    "        'combined_effect': []\n",
    "    }\n",
    "    \n",
    "    for (race_id, driver_id), group in races:\n",
    "        # Calculate lap time trends\n",
    "        early_laps = predictions[group.index[:10]].mean()\n",
    "        late_laps = predictions[group.index[-10:]].mean()\n",
    "        \n",
    "        # Net effect should be faster laps (negative delta)\n",
    "        patterns['combined_effect'].append(late_laps - early_laps)\n",
    "    \n",
    "    return pd.DataFrame(patterns).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lap_sequences(predictions, actual, metadata):\n",
    "    \"\"\"Analyze if predicted lap time sequences match F1 patterns.\"\"\"\n",
    "    # Group by stint (periods between pit stops)\n",
    "    stints = metadata.groupby(['raceId', 'driverId', 'stint'])\n",
    "    \n",
    "    stint_patterns = []\n",
    "    for _, stint in stints:\n",
    "        # Calculate trend lines\n",
    "        actual_trend = np.polyfit(range(len(stint)), actual[stint.index], 1)[0]\n",
    "        pred_trend = np.polyfit(range(len(stint)), predictions[stint.index], 1)[0]\n",
    "        \n",
    "        stint_patterns.append({\n",
    "            'actual_trend': actual_trend,\n",
    "            'predicted_trend': pred_trend,\n",
    "            'stint_length': len(stint)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(stint_patterns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ie500-data-mining-group7-LKR-OXJO-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
