{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import custom modules\n",
    "from data_preparation import load_and_preprocess_data, prepare_sequence_data, split_data_by_race, save_data_splits, prepare_regression_data\n",
    "from features import RaceFeatures\n",
    "from lstm import F1PredictionModel, F1Dataset, F1DataPreprocessor, train_model, save_model_with_preprocessor\n",
    "from evaluation import evaluate_model, plot_predictions\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "df = load_and_preprocess_data()\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# One hot encoding for the column \"code\"\n",
    "\n",
    "\n",
    "X = df.drop(columns=[\"cumulative_milliseconds\", \"positionOrder\", \"date\", \"driverRef\", \"number\", \"date_race\", \"time_race\", \"time\", \"forename\", \"surname\", \"dob\", \"url_race\", \"location\", \"circuitRef\"])\n",
    "\n",
    "\n",
    "X = X.drop(columns=[\"milliseconds\"])\n",
    "y = df[\"milliseconds\"]\n",
    "\n",
    "X.head()\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(\n",
    "    X, columns=[\"code\", \"nationality\", \"status\", \"circuit_type\", \"country\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "X_normalized = pd.DataFrame(X_normalized, columns=X.columns)\n",
    "\n",
    "# Display the first few rows of the normalized data\n",
    "X_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train_scaled is already scaled data from StandardScaler\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=None)\n",
    "dfx_pca = pca.fit(X_normalized)\n",
    "\n",
    "# Retrieve eigenvectors (components/loadings)\n",
    "eigenvectors = pca.components_  # Shape: (n_components, n_features)\n",
    "\n",
    "# Retrieve eigenvalues (explained variance)\n",
    "eigenvalues = pca.explained_variance_\n",
    "\n",
    "\n",
    "# Explained variance ratio (proportion of variance explained by each component)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a DataFrame for loadings\n",
    "loadings = pd.DataFrame(\n",
    "    eigenvectors.T,  # Transpose to align features as rows and PCs as columns\n",
    "    columns=[f\"PC{i+1}\" for i in range(eigenvectors.shape[0])],\n",
    "    index=X.columns,  # Assuming columns are the feature names\n",
    ")\n",
    "\n",
    "# Display the loadings\n",
    "print(\"PCA Loadings (Feature Contributions):\")\n",
    "print(loadings)\n",
    "\n",
    "# Explained variance\n",
    "print(\"\\nExplained Variance Ratio:\")\n",
    "print(\n",
    "    pd.Series(\n",
    "        explained_variance_ratio,\n",
    "        index=[f\"PC{i+1}\" for i in range(len(explained_variance_ratio))],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Most important features for PC1\n",
    "important_features_pc1 = loadings[\"PC1\"].abs().sort_values(ascending=False)\n",
    "print(\"\\nMost Important Features for PC1:\")\n",
    "print(important_features_pc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bar plot for feature importance in PC1\n",
    "plt.figure(figsize=(20, 6))\n",
    "important_features_pc1.plot(kind=\"bar\", color=\"skyblue\")  # Bar color\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Feature Importance for PC1\", fontsize=16, color=\"white\")  # Title in white\n",
    "plt.ylabel(\n",
    "    \"Loading Score (Absolute Value)\", fontsize=12, color=\"white\"\n",
    ")  # Y-axis label in white\n",
    "plt.xlabel(\"Features\", fontsize=12, color=\"white\")  # X-axis label in white\n",
    "\n",
    "# Change tick colors to white\n",
    "plt.xticks(rotation=90, fontsize=10, color=\"white\")  # X-ticks in white\n",
    "plt.yticks(fontsize=10, color=\"white\")  # Y-ticks in white\n",
    "\n",
    "# Change background color\n",
    "plt.gca().set_facecolor(\"black\")  # Axes background color\n",
    "plt.gcf().set_facecolor(\"black\")  # Figure background color\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(\n",
    "    range(1, len(eigenvalues) + 1), eigenvalues, marker=\"o\", linestyle=\"--\", color=\"r\"\n",
    ")\n",
    "plt.title(\"Scree Plot\", color=\"white\")\n",
    "plt.xlabel(\"Principal Component Index\", color=\"white\")\n",
    "plt.ylabel(\"Eigenvalue\", color=\"white\")\n",
    "plt.xticks(range(1, len(eigenvalues) + 1), color=\"white\")\n",
    "plt.yticks(color=\"white\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.gca().set_facecolor(\"black\")  # Set the background color to black\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform initial train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Further split the training set into training and validation sets\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.25,\n",
    "    random_state=42,  # 0.25 * 0.8 = 0.2 of the total data\n",
    ")\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_validation shape:\", X_validation.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_validation shape:\", y_validation.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define a range of n_components to try\n",
    "n_components_range = range(1, 165)  # Example range from 1 to 10\n",
    "\n",
    "# Initialize a dictionary to store RMSE for each n_components\n",
    "rmse_dict = {}\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    # Apply PCA with the current number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_validation_pca = pca.transform(X_validation)\n",
    "    \n",
    "    # Initialize the linear regression model\n",
    "    log_reg_pca = LinearRegression()\n",
    "    \n",
    "    # Train the model\n",
    "    log_reg_pca.fit(X_train_pca, y_train)\n",
    "    \n",
    "    # Make predictions on the validation set\n",
    "    y_pred_pca = log_reg_pca.predict(X_validation_pca)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = root_mean_squared_error(y_validation, y_pred_pca)\n",
    "    rmse_dict[n_components] = rmse\n",
    "    print(f\"n_components: {n_components}, RMSE: {rmse}\")\n",
    "\n",
    "# Find the best n_components with the lowest RMSE\n",
    "best_n_components = min(rmse_dict, key=rmse_dict.get)\n",
    "print(f\"Best n_components: {best_n_components}, RMSE: {rmse_dict[best_n_components]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a .pth file\n",
    "#from linear_regression_utils import save\n",
    "\n",
    "#torch.save(log_reg_pca, 'models/linear_regression_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_rf = rf_regressor.predict(X_validation)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_rf = root_mean_squared_error(y_validation, y_pred_rf)\n",
    "print(f\"Random Forest RMSE: {rmse_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=200, random_state=42, )\n",
    "\n",
    "# Train the model\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_xgb = xgb_regressor.predict(X_validation)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_xgb = root_mean_squared_error(y_validation, y_pred_xgb)\n",
    "print(f\"XGBoost RMSE: {rmse_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download [houses dataset](https://www.openml.org/d/537) from OpenML. The task is to predict median price of the house in the region based on demographic composition and a state of housing market in the region.\n",
    "from flaml import AutoML\n",
    "\n",
    "automl = AutoML()\n",
    "settings = {\n",
    "    \"time_budget\": 200,  # total running time in seconds\n",
    "    \"metric\": \"mse\",  # primary metrics for regression can be chosen from: ['mae','mse','r2']\n",
    "    \"estimator_list\": [\n",
    "        \"xgboost\"\n",
    "    ],  # list of ML learners; we tune XGBoost in this example\n",
    "    \"task\": \"regression\",  # task type\n",
    "    \"log_file_name\": \"f1regression.log\",  # flaml log file\n",
    "    \"seed\": 7654321,  # random seed\n",
    "}\n",
    "automl.fit(X_train=X_train, y_train=y_train, X_val=X_validation, y_val=y_validation, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparmeter config:\", automl.best_config)\n",
    "print(\"Best r2 on validation data: {0:.4g}\".format(1 - automl.best_loss))\n",
    "print(\"Training duration of best run: {0:.4g} s\".format(automl.best_config_train_time))\n",
    "\n",
    "model = automl.model.estimator\n",
    "\n",
    "y_pred_flaml = automl.predict(X_test)\n",
    "print(\"Predicted labels\", y_pred_flaml)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_flaml = root_mean_squared_error(y_validation, y_pred_flaml)\n",
    "print(f\"FLAML RMSE: {rmse_flaml}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 20))\n",
    "plt.barh(automl.feature_names_in_, automl.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Make predictions on the test set for Linear Regression with PCA\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "y_pred_pca_test = log_reg_pca.predict(X_test_pca)\n",
    "\n",
    "# Make predictions on the test set for Random Forest\n",
    "y_pred_rf_test = rf_regressor.predict(X_test_normalized)\n",
    "\n",
    "# Make predictions on the test set for XGBoost\n",
    "y_pred_xgb_test = xgb_regressor.predict(X_test_normalized)\n",
    "\n",
    "# Make predictions on the test set for FLAML\n",
    "y_pred_flaml_test = automl.predict(X_test_normalized)\n",
    "\n",
    "# Calculate RMSE for Linear Regression with PCA\n",
    "rmse_linear_pca = np.sqrt(mean_squared_error(y_test, y_pred_pca_test))\n",
    "print(f\"Linear Regression with PCA RMSE: {rmse_linear_pca}\")\n",
    "\n",
    "# Calculate RMSE for Random Forest\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf_test))\n",
    "print(f\"Random Forest RMSE: {rmse_rf}\")\n",
    "\n",
    "# Calculate RMSE for XGBoost\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb_test))\n",
    "print(f\"XGBoost RMSE: {rmse_xgb}\")\n",
    "\n",
    "# Calculate RMSE for FLAML\n",
    "rmse_flaml = np.sqrt(mean_squared_error(y_test, y_pred_flaml_test))\n",
    "print(f\"FLAML RMSE: {rmse_flaml}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plot_predictions(y_test, y_pred_flaml_test, model_name=\"XGBoost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ie500-data-mining-group7-PnEMH2vj-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
