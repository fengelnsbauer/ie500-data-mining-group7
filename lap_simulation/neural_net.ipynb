{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for racetime prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a logger for logging experiments configurations and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pytorch_lightning.loggers import Logger\n",
    "from pytorch_lightning.utilities.rank_zero import rank_zero_only\n",
    "\n",
    "class F1NeuralNetworkExperimentsLogger(Logger):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "        self.logs = {}\n",
    "        self._version = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"F1NeuralNetworkExperimentsLogger\"\n",
    "\n",
    "    @property\n",
    "    def version(self):\n",
    "        return self._version\n",
    "    \n",
    "    @rank_zero_only\n",
    "    def log_model_architecture(self, model):\n",
    "        def layer_to_dict(layer):\n",
    "            \"\"\"Convert a PyTorch layer to a structured dictionary.\"\"\"\n",
    "            return {\n",
    "                'type': layer.__class__.__name__,\n",
    "                'parameters': {\n",
    "                    name: p.shape if hasattr(p, 'shape') else str(p)\n",
    "                    for name, p in layer.named_parameters(recurse=False)\n",
    "                },\n",
    "                'submodules': [layer_to_dict(sub) for sub in layer.children()]\n",
    "            }\n",
    "\n",
    "        architecture_dict = layer_to_dict(model)\n",
    "        self.logs['model_architecture'] = architecture_dict\n",
    "\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_hyperparams(self, params):\n",
    "        self.logs['hyperparameters'] = {k: str(v) for k, v in params.items()}\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_metrics(self, metrics, step):\n",
    "        self.metrics.append((step, metrics))\n",
    "        if 'metrics' not in self.logs:\n",
    "            self.logs['metrics'] = []\n",
    "        self.logs['metrics'].append({'step': step, 'metrics': {k: float(v) for k, v in metrics.items()}})\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_overall_test_loss(self, test_loss):\n",
    "        self.logs['overall_test_loss'] = test_loss\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_used_features(self, features):\n",
    "        self.logs['used_features'] = features\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_optimization_strategy(self, optimizer, scheduler):\n",
    "        optimizer_str = str(optimizer)\n",
    "        scheduler_str = str(scheduler)\n",
    "        self.logs['optimizer'] = optimizer_str\n",
    "        self.logs['scheduler'] = scheduler_str\n",
    "\n",
    "    @rank_zero_only\n",
    "    def save(self):\n",
    "        directory = os.path.join(self.name, self.version)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open(os.path.join(directory, \"logs.json\"), \"w\") as f:\n",
    "            json.dump(self.logs, f, indent=4)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def finalize(self, status):\n",
    "        self.save()\n",
    "\n",
    "logger = F1NeuralNetworkExperimentsLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and load the features and labels for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import load_and_preprocess_data, prepare_regression_data, split_data_by_race\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class F1Dataset(Dataset):\n",
    "    def __init__(self, X, y, scaler=None):\n",
    "        super().__init__()\n",
    "        self.X = X.reset_index(drop=True)\n",
    "        print(self.X.columns)\n",
    "        self.y = y.reset_index(drop=True)\n",
    "        self.scaler = scaler\n",
    "        if self.scaler:\n",
    "            self.scaler.fit(self.X)\n",
    "            self.X = pd.DataFrame(self.scaler.fit_transform(self.X), columns=self.X.columns)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X.iloc[idx].values, dtype=torch.float32)  # Access by iloc\n",
    "        y = torch.tensor(self.y.iloc[idx], dtype=torch.float32)  # Access by iloc\n",
    "        return X, y\n",
    "\n",
    "train_df, test_df = split_data_by_race(load_and_preprocess_data())\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n",
    "\n",
    "X_train, y_train = prepare_regression_data(train_df)\n",
    "X_val, y_val = prepare_regression_data(val_df)\n",
    "X_test, y_test = prepare_regression_data(test_df)\n",
    "\n",
    "N_FEATURES = X_train.shape[1]\n",
    "print(f\"Number of features: {N_FEATURES}\")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_dataset = F1Dataset(X_train, y_train, scaler=scaler)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = F1Dataset(X_val, y_val, scaler=scaler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "test_dataset = F1Dataset(X_test, y_test, scaler=scaler)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LaptimePredicionModel(pl.LightningModule):\n",
    "    def __init__(self, lr=0.005, loss_fn=nn.MSELoss, optimizer=torch.optim.AdamW, layer_config=(2, 64)):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=True)\n",
    "        self.predictions = []\n",
    "        self.actuals = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.test_losses = []\n",
    "        self.lr = lr\n",
    "        self.INPUT_DIM = N_FEATURES\n",
    "        self.OUTPUT_DIM = 1\n",
    "\n",
    "        # Get the number of layers and neurons per layer from layer_config\n",
    "        num_layers, neurons_per_layer = layer_config\n",
    "        \n",
    "        # Build the model dynamically\n",
    "        layers = []\n",
    "        \n",
    "        in_features = self.INPUT_DIM\n",
    "        layers.append(nn.Linear(in_features, neurons_per_layer))\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(neurons_per_layer, neurons_per_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_features = neurons_per_layer\n",
    "        \n",
    "        # Final output layer\n",
    "        layers.append(nn.Linear(neurons_per_layer, self.OUTPUT_DIM))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.loss_fn = loss_fn() if callable(loss_fn) else loss_fn\n",
    "        self.optimizer = optimizer(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x).squeeze(-1)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x).squeeze(-1)\n",
    "        test_loss_fn = nn.MSELoss()\n",
    "        loss = torch.sqrt(test_loss_fn(y_hat, y))\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x).squeeze(-1)\n",
    "        test_loss_fn = nn.MSELoss()\n",
    "        loss = torch.sqrt(test_loss_fn(y_hat, y))\n",
    "        self.predictions.extend(y_hat.cpu().numpy())\n",
    "        self.actuals.extend(y.cpu().numpy())\n",
    "        self.test_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_test_loss = sum(self.test_losses) / len(self.test_losses)\n",
    "        print(f\"Average Test Loss: {avg_test_loss}\")\n",
    "        \n",
    "        # Set the background color to black\n",
    "        plt.figure(figsize=(8, 6), facecolor='black')\n",
    "        ax = plt.gca()  # Get the current axis\n",
    "        ax.set_facecolor('black')  # Set the plot area background color to black\n",
    "\n",
    "        # Scatter plot with blue points and blue edges\n",
    "        plt.scatter(self.actuals, self.predictions, alpha=0.6, color='b', edgecolor='b')  \n",
    "\n",
    "        # Customize axis labels, title, and grid\n",
    "        plt.xlabel('Actual Values', color='w')  # White axis label\n",
    "        plt.ylabel('Predicted Values', color='w')  # White axis label\n",
    "        plt.title('Correlation between Actual and Predicted Values', color='w')  # White title\n",
    "        plt.grid(True, color='w')  # White grid lines\n",
    "        \n",
    "        # Customize the spines (axes borders) to have black background behind labels\n",
    "        ax.spines['top'].set_color('w')\n",
    "        ax.spines['right'].set_color('w')\n",
    "        ax.spines['left'].set_color('w')\n",
    "        ax.spines['bottom'].set_color('w')\n",
    "\n",
    "        # Set the ticks and their labels to white\n",
    "        ax.tick_params(axis='both', colors='w')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        self.plot_loss_curve()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_losses.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.hparams.lr,\n",
    "            steps_per_epoch=100,\n",
    "            epochs=10,\n",
    "            anneal_strategy='cos',\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': self.optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "\n",
    "    def plot_loss_curve(self):\n",
    "        # Set the background color to black\n",
    "        plt.figure(figsize=(10, 5), facecolor='black')\n",
    "        ax = plt.gca()  # Get the current axis\n",
    "        ax.set_facecolor('black')  # Set the plot area background color to black\n",
    "\n",
    "        if self.val_losses:\n",
    "            plt.plot(self.val_losses, label='Validation Loss', color='b')  # Blue line for loss curve\n",
    "\n",
    "        # Customize axis labels, title, and grid\n",
    "        plt.xlabel('Epoch', color='w')  # White axis label\n",
    "        plt.ylabel('Loss', color='w')  # White axis label\n",
    "        plt.title('Loss Curve', color='w')  # White title\n",
    "        plt.legend(frameon=False, loc='best', facecolor='black', edgecolor='w', labelcolor='w')  # White text for legend\n",
    "        plt.grid(True, color='w')  # White grid lines\n",
    "\n",
    "        # Customize the spines (axes borders) to have black background behind labels\n",
    "        ax.spines['top'].set_color('w')\n",
    "        ax.spines['right'].set_color('w')\n",
    "        ax.spines['left'].set_color('w')\n",
    "        ax.spines['bottom'].set_color('w')\n",
    "\n",
    "        # Set the ticks and their labels to white\n",
    "        ax.tick_params(axis='both', colors='w')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure callback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,          \n",
    "    verbose=True,\n",
    "    mode=\"min\"           \n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",     \n",
    "    filename=\"best_model-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,             \n",
    "    mode=\"min\",                  \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine optimal amount of model layers and neurons per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pytorch_lightning as pl\n",
    "import optuna\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "    neurons_per_layer = trial.suggest_int('neurons_per_layer', 32, 128)\n",
    "\n",
    "    # Initialize the model with sampled hyperparameters\n",
    "    model = LaptimePredicionModel(layer_config=(num_layers, neurons_per_layer))\n",
    "\n",
    "    # Trainer setup\n",
    "    trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[early_stopping, checkpoint_callback])\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "    # Calculate the average validation loss\n",
    "    avg_val_loss = sum(model.val_losses) / len(model.val_losses)\n",
    "    print(f\"Test Loss for {num_layers} layers, {neurons_per_layer} neurons per layer: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Log results to CSV\n",
    "    log_complexity_results_to_csv('neural_net_hyperparam_evaluation/complexity_hyperparams.csv', num_layers, neurons_per_layer, avg_val_loss)\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def log_complexity_results_to_csv(filename, num_layers, neurons_per_layer, avg_val_loss):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    # Open the file in append mode\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header only if the file is new\n",
    "        if not file_exists:\n",
    "            writer.writerow(['layers', 'neurons_per_layer', 'loss'])\n",
    "        \n",
    "        # Write the trial's results\n",
    "        writer.writerow([num_layers, neurons_per_layer, avg_val_loss])\n",
    "\n",
    "\n",
    "def optimize_hyperparameters():\n",
    "    # Create a study to minimize the validation loss\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    # Print the best hyperparameters and their corresponding loss\n",
    "    print(\"Best Hyperparameters: \", study.best_params)\n",
    "    print(\"Best Value (Test Loss): \", study.best_value)\n",
    "\n",
    "\n",
    "# Run the optimization process\n",
    "optimize_hyperparameters()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the optimal learning rate, optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import optuna\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Expanded learning rate range\n",
    "    lr = trial.suggest_loguniform('lr', 1e-6, 1)  # Learning rate (log scale, wider range)\n",
    "    \n",
    "    # Choose optimizer: Adam, AdamW, or SGD\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD\n",
    "\n",
    "    # Choose loss function: L1Loss or MSELoss\n",
    "    loss_fn_name = trial.suggest_categorical('loss_fn', ['L1Loss', 'MSELoss'])\n",
    "    if loss_fn_name == 'L1Loss':\n",
    "        loss_fn = nn.L1Loss\n",
    "    else:\n",
    "        loss_fn = nn.MSELoss\n",
    "\n",
    "    # Initialize the model with the fixed architecture and selected loss function\n",
    "    model = LaptimePredicionModel(lr=lr, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[early_stopping, checkpoint_callback])\n",
    "\n",
    "    trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "    # Calculate average test loss\n",
    "    avg_val_loss = sum(model.val_losses) / len(model.val_losses)\n",
    "    print(f\"Test Loss for Optimizer: {optimizer_name}, Loss function: {loss_fn_name}, \"\n",
    "          f\"Learning rate: {lr:.6f}: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Log results to CSV\n",
    "    log_learning_results_to_csv('neural_net_hyperparam_evaluation/learning_hyperparams.csv', lr, optimizer_name, loss_fn_name, avg_val_loss)\n",
    "\n",
    "    return avg_val_loss  # We aim to minimize the test loss\n",
    "\n",
    "\n",
    "# Function to log trial results to a CSV file\n",
    "def log_learning_results_to_csv(filename, lr, optimizer_name, loss_fn_name, avg_val_loss):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    # Open the file in append mode\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header only if the file is new\n",
    "        if not file_exists:\n",
    "            writer.writerow(['learning_rate', 'optimizer', 'loss_fn', 'loss'])\n",
    "        \n",
    "        # Write the trial's results\n",
    "        writer.writerow([lr, optimizer_name, loss_fn_name, avg_val_loss])\n",
    "\n",
    "\n",
    "# Function to run the optimization process\n",
    "def optimize_hyperparameters():\n",
    "    # Create an Optuna study to minimize the test loss\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=10)  # Run 10 trials\n",
    "\n",
    "    # Print the best hyperparameters found by Optuna\n",
    "    print(\"Best Hyperparameters: \", study.best_params)\n",
    "    print(\"Best Test Loss: \", study.best_value)\n",
    "\n",
    "# Start the optimization process\n",
    "optimize_hyperparameters()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Training with best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LaptimePredicionModel()\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    log_every_n_steps=10,\n",
    "    max_epochs=1000,\n",
    ")\n",
    "\n",
    "trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model, dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
