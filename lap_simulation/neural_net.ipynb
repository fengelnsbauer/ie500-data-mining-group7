{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for racetime prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a logger for logging experiments configurations and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pytorch_lightning.loggers import Logger\n",
    "from pytorch_lightning.utilities.rank_zero import rank_zero_only\n",
    "\n",
    "class F1NeuralNetworkExperimentsLogger(Logger):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "        self.logs = {}\n",
    "        self._version = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"F1NeuralNetworkExperimentsLogger\"\n",
    "\n",
    "    @property\n",
    "    def version(self):\n",
    "        return self._version\n",
    "    \n",
    "    @rank_zero_only\n",
    "    def log_model_architecture(self, model):\n",
    "        def layer_to_dict(layer):\n",
    "            \"\"\"Convert a PyTorch layer to a structured dictionary.\"\"\"\n",
    "            return {\n",
    "                'type': layer.__class__.__name__,\n",
    "                'parameters': {\n",
    "                    name: p.shape if hasattr(p, 'shape') else str(p)\n",
    "                    for name, p in layer.named_parameters(recurse=False)\n",
    "                },\n",
    "                'submodules': [layer_to_dict(sub) for sub in layer.children()]\n",
    "            }\n",
    "\n",
    "        architecture_dict = layer_to_dict(model)\n",
    "        self.logs['model_architecture'] = architecture_dict\n",
    "\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_hyperparams(self, params):\n",
    "        self.logs['hyperparameters'] = {k: str(v) for k, v in params.items()}\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_metrics(self, metrics, step):\n",
    "        self.metrics.append((step, metrics))\n",
    "        if 'metrics' not in self.logs:\n",
    "            self.logs['metrics'] = []\n",
    "        self.logs['metrics'].append({'step': step, 'metrics': {k: float(v) for k, v in metrics.items()}})\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_overall_test_loss(self, test_loss):\n",
    "        self.logs['overall_test_loss'] = test_loss\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_used_features(self, features):\n",
    "        self.logs['used_features'] = features\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_optimization_strategy(self, optimizer, scheduler):\n",
    "        optimizer_str = str(optimizer)\n",
    "        scheduler_str = str(scheduler)\n",
    "        self.logs['optimizer'] = optimizer_str\n",
    "        self.logs['scheduler'] = scheduler_str\n",
    "\n",
    "    @rank_zero_only\n",
    "    def save(self):\n",
    "        directory = os.path.join(self.name, self.version)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        with open(os.path.join(directory, \"logs.json\"), \"w\") as f:\n",
    "            json.dump(self.logs, f, indent=4)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def finalize(self, status):\n",
    "        self.save()\n",
    "\n",
    "logger = F1NeuralNetworkExperimentsLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the features and labels for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:25: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  practice_sessions = pd.read_csv('../data/raw_data/ff1_laps.csv', na_values=na_values)\n",
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:26: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tire_data = pd.read_csv('../data/raw_data/ff1_laps.csv', na_values=na_values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586171, 15)\n",
      "(586171, 32)\n",
      "(586171, 40)\n",
      "(586171, 45)\n",
      "(586171, 46)\n",
      "(586171, 47)\n",
      "(586171, 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:79: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['pitstop_milliseconds'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['constructor_points'].fillna(laps['constructor_points'].mean(), inplace=True)\n",
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['constructor_position'].fillna(laps['constructor_position'].max(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586171, 56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:196: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['Compound'].fillna('UNKNOWN', inplace=True)\n",
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:249: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['fp1_median_time'].fillna(global_median_fp1, inplace=True)\n",
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:253: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['fp2_median_time'].fillna(global_median_fp2, inplace=True)\n",
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:256: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['fp3_median_time'].fillna(global_median_fp3, inplace=True)\n",
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:259: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['quali_time'].fillna(global_median_quali, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching for 1\n",
      "Matching for 2\n",
      "Matching for 3\n",
      "Matching for 4\n",
      "Matching for 5\n",
      "Matching for 6\n",
      "Matching for 7\n",
      "Matching for 8\n",
      "Matching for 9\n",
      "Matching for 10\n",
      "Matching for 11\n",
      "Matching for 12\n",
      "Matching for 13\n",
      "Matching for 14\n",
      "Matching for 15\n",
      "Matching for 16\n",
      "Matching for 17\n",
      "Matching for 18\n",
      "Matching for 19\n",
      "Matching for 20\n",
      "Matching for 21\n",
      "Matching for 22\n",
      "Matching for 23\n",
      "Matching for 24\n",
      "Matching for 25\n",
      "Matching for 26\n",
      "Matching for 27\n",
      "Matching for 28\n",
      "Matching for 29\n",
      "Matching for 30\n",
      "Matching for 31\n",
      "Matching for 32\n",
      "Matching for 33\n",
      "Matching for 34\n",
      "Matching for 35\n",
      "Matching for 36\n",
      "Matching for 37\n",
      "Matching for 38\n",
      "Matching for 39\n",
      "Matching for 40\n",
      "Matching for 41\n",
      "Matching for 42\n",
      "Matching for 43\n",
      "Matching for 44\n",
      "Matching for 45\n",
      "Matching for 46\n",
      "Matching for 47\n",
      "Matching for 48\n",
      "Matching for 49\n",
      "Matching for 50\n",
      "Matching for 51\n",
      "Matching for 52\n",
      "Matching for 53\n",
      "Matching for 54\n",
      "Matching for 55\n",
      "Matching for 56\n",
      "Matching for 57\n",
      "Matching for 58\n",
      "Matching for 59\n",
      "Matching for 60\n",
      "Matching for 61\n",
      "Matching for 62\n",
      "Matching for 63\n",
      "Matching for 64\n",
      "Matching for 65\n",
      "Matching for 66\n",
      "Matching for 67\n",
      "Matching for 68\n",
      "Matching for 69\n",
      "Matching for 70\n",
      "Matching for 71\n",
      "Matching for 72\n",
      "Matching for 73\n",
      "Matching for 74\n",
      "Matching for 75\n",
      "Matching for 76\n",
      "Matching for 77\n",
      "Matching for 78\n",
      "Matching for 79\n",
      "Matching for 80\n",
      "Matching for 81\n",
      "Matching for 82\n",
      "Matching for 83\n",
      "Matching for 84\n",
      "Matching for 85\n",
      "Matching for 86\n",
      "Matching for 87\n",
      "Matching for 88\n",
      "Matching for 89\n",
      "Matching for 90\n",
      "Matching for 91\n",
      "Matching for 92\n",
      "Matching for 93\n",
      "Matching for 94\n",
      "Matching for 95\n",
      "Matching for 96\n",
      "Matching for 97\n",
      "Matching for 98\n",
      "Matching for 99\n",
      "Matching for 100\n",
      "Matching for 101\n",
      "Matching for 102\n",
      "Matching for 103\n",
      "Matching for 104\n",
      "Matching for 105\n",
      "Matching for 106\n",
      "Matching for 107\n",
      "Matching for 108\n",
      "Matching for 109\n",
      "Matching for 110\n",
      "Matching for 111\n",
      "Matching for 112\n",
      "Matching for 113\n",
      "Matching for 114\n",
      "Matching for 115\n",
      "Matching for 116\n",
      "Matching for 117\n",
      "Matching for 118\n",
      "Matching for 119\n",
      "Matching for 120\n",
      "Matching for 121\n",
      "Matching for 122\n",
      "Matching for 123\n",
      "Matching for 124\n",
      "Matching for 125\n",
      "Matching for 126\n",
      "Matching for 127\n",
      "Matching for 128\n",
      "Matching for 129\n",
      "Matching for 130\n",
      "Matching for 131\n",
      "Matching for 132\n",
      "Matching for 133\n",
      "Matching for 134\n",
      "Matching for 135\n",
      "Matching for 136\n",
      "Matching for 137\n",
      "Matching for 138\n",
      "Matching for 139\n",
      "Matching for 140\n",
      "Matching for 141\n",
      "Matching for 142\n",
      "Matching for 143\n",
      "Matching for 144\n",
      "Matching for 145\n",
      "Matching for 146\n",
      "Matching for 147\n",
      "Matching for 148\n",
      "Matching for 149\n",
      "Matching for 150\n",
      "Matching for 151\n",
      "Matching for 152\n",
      "Matching for 153\n",
      "Matching for 154\n",
      "Matching for 155\n",
      "Matching for 156\n",
      "Matching for 157\n",
      "Matching for 158\n",
      "Matching for 159\n",
      "Matching for 160\n",
      "Matching for 161\n",
      "Matching for 162\n",
      "Matching for 163\n",
      "Matching for 164\n",
      "Matching for 165\n",
      "Matching for 166\n",
      "Matching for 167\n",
      "Matching for 168\n",
      "Matching for 169\n",
      "Matching for 170\n",
      "Matching for 171\n",
      "Matching for 172\n",
      "Matching for 173\n",
      "Matching for 174\n",
      "Matching for 175\n",
      "Matching for 176\n",
      "Matching for 177\n",
      "Matching for 178\n",
      "Matching for 179\n",
      "Matching for 180\n",
      "Matching for 181\n",
      "Matching for 182\n",
      "Matching for 183\n",
      "Matching for 184\n",
      "Matching for 185\n",
      "Matching for 186\n",
      "Matching for 187\n",
      "Matching for 188\n",
      "Matching for 189\n",
      "Matching for 190\n",
      "Matching for 191\n",
      "Matching for 192\n",
      "Matching for 193\n",
      "Matching for 194\n",
      "Matching for 195\n",
      "Matching for 196\n",
      "Matching for 197\n",
      "Matching for 198\n",
      "Matching for 199\n",
      "Matching for 200\n",
      "Matching for 201\n",
      "Matching for 202\n",
      "Matching for 203\n",
      "Matching for 204\n",
      "Matching for 205\n",
      "Matching for 206\n",
      "Matching for 207\n",
      "Matching for 208\n",
      "Matching for 209\n",
      "Matching for 210\n",
      "Matching for 211\n",
      "Matching for 212\n",
      "Matching for 213\n",
      "Matching for 214\n",
      "Matching for 215\n",
      "Matching for 216\n",
      "Matching for 217\n",
      "Matching for 218\n",
      "Matching for 219\n",
      "Matching for 220\n",
      "Matching for 221\n",
      "Matching for 222\n",
      "Matching for 223\n",
      "Matching for 224\n",
      "Matching for 225\n",
      "Matching for 226\n",
      "Matching for 227\n",
      "Matching for 228\n",
      "Matching for 229\n",
      "Matching for 230\n",
      "Matching for 231\n",
      "Matching for 232\n",
      "Matching for 233\n",
      "Matching for 234\n",
      "Matching for 235\n",
      "Matching for 236\n",
      "Matching for 237\n",
      "Matching for 238\n",
      "Matching for 239\n",
      "Matching for 337\n",
      "Matching for 338\n",
      "Matching for 339\n",
      "Matching for 340\n",
      "Matching for 341\n",
      "Matching for 342\n",
      "Matching for 343\n",
      "Matching for 344\n",
      "Matching for 345\n",
      "Matching for 346\n",
      "Matching for 347\n",
      "Matching for 348\n",
      "Matching for 349\n",
      "Matching for 350\n",
      "Matching for 351\n",
      "Matching for 352\n",
      "Matching for 353\n",
      "Matching for 354\n",
      "Matching for 355\n",
      "Matching for 841\n",
      "Matching for 842\n",
      "Matching for 843\n",
      "Matching for 844\n",
      "Matching for 845\n",
      "Matching for 846\n",
      "Matching for 847\n",
      "Matching for 848\n",
      "Matching for 849\n",
      "Matching for 850\n",
      "Matching for 851\n",
      "Matching for 852\n",
      "Matching for 853\n",
      "Matching for 854\n",
      "Matching for 855\n",
      "Matching for 856\n",
      "Matching for 857\n",
      "Matching for 858\n",
      "Matching for 859\n",
      "Matching for 860\n",
      "Matching for 861\n",
      "Matching for 862\n",
      "Matching for 863\n",
      "Matching for 864\n",
      "Matching for 865\n",
      "Matching for 866\n",
      "Matching for 867\n",
      "Matching for 868\n",
      "Matching for 869\n",
      "Matching for 870\n",
      "Matching for 871\n",
      "Matching for 872\n",
      "Matching for 873\n",
      "Matching for 874\n",
      "Matching for 875\n",
      "Matching for 876\n",
      "Matching for 877\n",
      "Matching for 878\n",
      "Matching for 879\n",
      "Matching for 880\n",
      "Matching for 881\n",
      "Matching for 882\n",
      "Matching for 883\n",
      "Matching for 884\n",
      "Matching for 885\n",
      "Matching for 886\n",
      "Matching for 887\n",
      "Matching for 888\n",
      "Matching for 890\n",
      "Matching for 891\n",
      "Matching for 892\n",
      "Matching for 893\n",
      "Matching for 894\n",
      "Matching for 895\n",
      "Matching for 896\n",
      "Matching for 897\n",
      "Matching for 898\n",
      "Matching for 899\n",
      "Matching for 900\n",
      "Matching for 901\n",
      "Matching for 902\n",
      "Matching for 903\n",
      "Matching for 904\n",
      "Matching for 905\n",
      "Matching for 906\n",
      "Matching for 907\n",
      "Matching for 908\n",
      "Matching for 909\n",
      "Matching for 910\n",
      "Matching for 911\n",
      "Matching for 912\n",
      "Matching for 913\n",
      "Matching for 914\n",
      "Matching for 915\n",
      "Matching for 916\n",
      "Matching for 917\n",
      "Matching for 918\n",
      "Matching for 926\n",
      "Matching for 927\n",
      "Matching for 928\n",
      "Matching for 929\n",
      "Matching for 930\n",
      "Matching for 931\n",
      "Matching for 932\n",
      "Matching for 933\n",
      "Matching for 934\n",
      "Matching for 936\n",
      "Matching for 937\n",
      "Matching for 938\n",
      "Matching for 939\n",
      "Matching for 940\n",
      "Matching for 941\n",
      "Matching for 942\n",
      "Matching for 943\n",
      "Matching for 944\n",
      "Matching for 945\n",
      "Matching for 948\n",
      "Matching for 949\n",
      "Matching for 950\n",
      "Matching for 951\n",
      "Matching for 952\n",
      "Matching for 953\n",
      "Matching for 954\n",
      "Matching for 955\n",
      "Matching for 956\n",
      "Matching for 957\n",
      "Matching for 958\n",
      "Matching for 959\n",
      "Matching for 960\n",
      "Matching for 961\n",
      "Matching for 962\n",
      "Matching for 963\n",
      "Matching for 964\n",
      "Matching for 965\n",
      "Matching for 966\n",
      "Matching for 967\n",
      "Matching for 968\n",
      "Matching for 969\n",
      "Matching for 970\n",
      "Matching for 971\n",
      "Matching for 972\n",
      "Matching for 973\n",
      "Matching for 974\n",
      "Matching for 975\n",
      "Matching for 976\n",
      "Matching for 977\n",
      "Matching for 978\n",
      "Matching for 979\n",
      "Matching for 980\n",
      "Matching for 981\n",
      "Matching for 982\n",
      "Matching for 983\n",
      "Matching for 984\n",
      "Matching for 985\n",
      "Matching for 986\n",
      "Matching for 987\n",
      "Matching for 988\n",
      "Matching for 989\n",
      "Matching for 990\n",
      "Matching for 991\n",
      "Matching for 992\n",
      "Matching for 993\n",
      "Matching for 994\n",
      "Matching for 995\n",
      "Matching for 996\n",
      "Matching for 997\n",
      "Matching for 998\n",
      "Matching for 999\n",
      "Matching for 1000\n",
      "Matching for 1001\n",
      "Matching for 1002\n",
      "Matching for 1003\n",
      "Matching for 1004\n",
      "Matching for 1005\n",
      "Matching for 1006\n",
      "Matching for 1007\n",
      "Matching for 1008\n",
      "Matching for 1009\n",
      "Matching for 1010\n",
      "Matching for 1011\n",
      "Matching for 1012\n",
      "Matching for 1013\n",
      "Matching for 1014\n",
      "Matching for 1015\n",
      "Matching for 1016\n",
      "Matching for 1017\n",
      "Matching for 1018\n",
      "Matching for 1019\n",
      "Matching for 1020\n",
      "Matching for 1021\n",
      "Matching for 1022\n",
      "Matching for 1023\n",
      "Matching for 1024\n",
      "Matching for 1025\n",
      "Matching for 1026\n",
      "Matching for 1027\n",
      "Matching for 1028\n",
      "Matching for 1029\n",
      "Matching for 1030\n",
      "Matching for 1031\n",
      "Matching for 1032\n",
      "Matching for 1033\n",
      "Matching for 1034\n",
      "Matching for 1035\n",
      "Matching for 1036\n",
      "Matching for 1037\n",
      "Matching for 1038\n",
      "Matching for 1039\n",
      "Matching for 1040\n",
      "Matching for 1041\n",
      "Matching for 1042\n",
      "Matching for 1043\n",
      "Matching for 1044\n",
      "Matching for 1045\n",
      "Matching for 1046\n",
      "Matching for 1047\n",
      "Matching for 1051\n",
      "Matching for 1052\n",
      "Matching for 1053\n",
      "Matching for 1054\n",
      "Matching for 1055\n",
      "Matching for 1056\n",
      "Matching for 1057\n",
      "Matching for 1058\n",
      "Matching for 1059\n",
      "Matching for 1060\n",
      "Matching for 1061\n",
      "Matching for 1062\n",
      "Matching for 1063\n",
      "Matching for 1064\n",
      "Matching for 1065\n",
      "Matching for 1066\n",
      "Matching for 1067\n",
      "Matching for 1069\n",
      "Matching for 1070\n",
      "Matching for 1071\n",
      "Matching for 1072\n",
      "Matching for 1073\n",
      "Matching for 1074\n",
      "Matched DataFrame shape: (1123, 81)\n",
      "Matching for 1075\n",
      "Matched DataFrame shape: (816, 81)\n",
      "Matching for 1076\n",
      "Matched DataFrame shape: (1042, 81)\n",
      "Matching for 1077\n",
      "Matched DataFrame shape: (1131, 81)\n",
      "Matching for 1078\n",
      "Matched DataFrame shape: (1055, 81)\n",
      "Matching for 1079\n",
      "Matched DataFrame shape: (1230, 81)\n",
      "Matching for 1080\n",
      "Matched DataFrame shape: (1176, 81)\n",
      "Matching for 1081\n",
      "Matched DataFrame shape: (889, 81)\n",
      "Matching for 1082\n",
      "Matched DataFrame shape: (1262, 81)\n",
      "Matching for 1083\n",
      "Matched DataFrame shape: (811, 81)\n",
      "Matching for 1084\n",
      "Matched DataFrame shape: (1323, 81)\n",
      "Matching for 1085\n",
      "Matched DataFrame shape: (953, 81)\n",
      "Matching for 1086\n",
      "Matched DataFrame shape: (1382, 81)\n",
      "Matching for 1087\n",
      "Matched DataFrame shape: (790, 81)\n",
      "Matching for 1088\n",
      "Matched DataFrame shape: (1391, 81)\n",
      "Matching for 1089\n",
      "Matched DataFrame shape: (969, 81)\n",
      "Matching for 1091\n",
      "Matched DataFrame shape: (941, 81)\n",
      "Matching for 1092\n",
      "Matched DataFrame shape: (504, 81)\n",
      "Matching for 1093\n",
      "Matched DataFrame shape: (990, 81)\n",
      "Matching for 1094\n",
      "Matched DataFrame shape: (1378, 81)\n",
      "Matching for 1095\n",
      "Matched DataFrame shape: (1256, 81)\n",
      "Matching for 1096\n",
      "Matching for 1098\n",
      "Matched DataFrame shape: (1055, 81)\n",
      "Matching for 1099\n",
      "Matched DataFrame shape: (942, 81)\n",
      "Matching for 1100\n",
      "Matched DataFrame shape: (995, 81)\n",
      "Matching for 1101\n",
      "Matched DataFrame shape: (961, 81)\n",
      "Matching for 1102\n",
      "Matched DataFrame shape: (1138, 81)\n",
      "Matching for 1104\n",
      "Matched DataFrame shape: (1513, 81)\n",
      "Matching for 1105\n",
      "Matched DataFrame shape: (1312, 81)\n",
      "Matching for 1106\n",
      "Matched DataFrame shape: (1315, 81)\n",
      "Matching for 1107\n",
      "Matched DataFrame shape: (1353, 81)\n",
      "Matching for 1108\n",
      "Matched DataFrame shape: (970, 81)\n",
      "Matching for 1109\n",
      "Matched DataFrame shape: (1252, 81)\n",
      "Matching for 1110\n",
      "Matched DataFrame shape: (815, 81)\n",
      "Matching for 1111\n",
      "Matched DataFrame shape: (1341, 81)\n",
      "Matching for 1112\n",
      "Matched DataFrame shape: (955, 81)\n",
      "Matching for 1113\n",
      "Matched DataFrame shape: (1084, 81)\n",
      "Matching for 1114\n",
      "Matched DataFrame shape: (880, 81)\n",
      "Matching for 1115\n",
      "Matched DataFrame shape: (1005, 81)\n",
      "Matching for 1116\n",
      "Matched DataFrame shape: (1014, 81)\n",
      "Matching for 1117\n",
      "Matched DataFrame shape: (1280, 81)\n",
      "Matching for 1118\n",
      "Matched DataFrame shape: (1106, 81)\n",
      "Matching for 1119\n",
      "Matched DataFrame shape: (943, 81)\n",
      "Matching for 1120\n",
      "Matching for 1121\n",
      "Matching for 1122\n",
      "Matching for 1123\n",
      "Matching for 1124\n",
      "Matching for 1125\n",
      "Matching for 1126\n",
      "Matching for 1127\n",
      "Matching for 1128\n",
      "Matching for 1129\n",
      "Matching for 1130\n",
      "Matched DataFrame shape: (1310, 81)\n",
      "Matching for 1131\n",
      "Matched DataFrame shape: (1405, 81)\n",
      "Matching for 1132\n",
      "Matched DataFrame shape: (960, 81)\n",
      "Matching for 1133\n",
      "Matched DataFrame shape: (1355, 81)\n",
      "Matching for 1134\n",
      "Matched DataFrame shape: (841, 81)\n",
      "Matching for 1135\n",
      "Matched DataFrame shape: (1426, 81)\n",
      "Matching for 1136\n",
      "Matched DataFrame shape: (1008, 81)\n",
      "Matching for 1137\n",
      "Matched DataFrame shape: (971, 81)\n",
      "Matching for 1138\n",
      "Matched DataFrame shape: (1177, 81)\n",
      "Matching for 1139\n",
      "Matched DataFrame shape: (1058, 81)\n",
      "Matching for 1140\n",
      "Matched DataFrame shape: (1213, 81)\n",
      "Matching for 1141\n",
      "Matched DataFrame shape: (1133, 81)\n",
      "(586171, 83)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/data_preparation.py:492: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['TrackStatus'].fillna(1, inplace=True)  # 1 = regular racing status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before filtering and outlier removal: (586171, 95)\n",
      "Normal racing laps: (528281, 95)\n",
      "Special laps (pit stops, safety car, etc.): (57890, 95)\n",
      "Final shape after outlier removal: (555735, 95)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([4.1873e-01, 4.1873e-01, 5.0000e-01, 5.0000e-01, 2.1115e-01, 2.1115e-01,\n",
       "         7.8018e+04, 8.0018e+04, 8.7219e+04, 8.2268e+04, 2.0000e+01, 5.0000e+01,\n",
       "         5.0000e+00, 2.5000e+01, 2.0000e+01, 5.0000e+01, 1.0000e+00, 1.0000e+00,\n",
       "         0.0000e+00]),\n",
       " tensor(76849.))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_preparation import load_and_preprocess_data, prepare_regression_data, split_data_by_race\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class F1Dataset(Dataset):\n",
    "    def __init__(self, X, y, scaler=None):\n",
    "        super().__init__()\n",
    "        self.X = X.apply(pd.to_numeric, errors='coerce').reset_index(drop=True)\n",
    "        self.y = y.reset_index(drop=True)\n",
    "        self.scaler = scaler\n",
    "        if self.scaler:\n",
    "            self.X = self.scaler.transform(self.X)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.X.iloc[idx].values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.y.iloc[idx], dtype=torch.float32)\n",
    "        return X, y\n",
    "\n",
    "train_df, test_df = split_data_by_race(load_and_preprocess_data())\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\n",
    "\n",
    "X_train, y_train = prepare_regression_data(train_df)\n",
    "X_val, y_val = prepare_regression_data(val_df)\n",
    "X_test, y_test = prepare_regression_data(test_df)\n",
    "\n",
    "N_FEATURES = X_train.shape[1]\n",
    "\n",
    "train_dataset = F1Dataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = F1Dataset(X_val, y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "test_dataset = F1Dataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LaptimePredicionModel(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3, loss_fn=nn.L1Loss, optimizer=torch.optim.AdamW, layer_config=(3, 64)):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=True)\n",
    "        self.predictions = []\n",
    "        self.actuals = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.test_losses = []\n",
    "        self.lr = lr\n",
    "        self.INPUT_DIM = N_FEATURES\n",
    "        self.OUTPUT_DIM = 1\n",
    "\n",
    "        # Get the number of layers and neurons per layer from layer_config\n",
    "        num_layers, neurons_per_layer = layer_config\n",
    "        \n",
    "        # Build the model dynamically\n",
    "        layers = []\n",
    "        \n",
    "        in_features = self.INPUT_DIM\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(in_features, neurons_per_layer))\n",
    "            layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            in_features = neurons_per_layer\n",
    "        \n",
    "        # Final output layer\n",
    "        layers.append(nn.Linear(neurons_per_layer, self.OUTPUT_DIM))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.loss_fn = loss_fn()\n",
    "        self.optimizer = optimizer(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x).squeeze(-1)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x).squeeze(-1)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x).squeeze(-1)\n",
    "        test_loss_fn = nn.MSELoss()\n",
    "        loss = torch.sqrt(test_loss_fn(y_hat, y))\n",
    "        self.predictions.extend(y_hat.cpu().numpy())\n",
    "        self.actuals.extend(y.cpu().numpy())\n",
    "        self.test_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_test_loss = sum(self.test_losses) / len(self.test_losses)\n",
    "        print(f\"Average Test Loss: {avg_test_loss}\")\n",
    "        \n",
    "        # Plot the correlation between actual and predicted values\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(self.actuals, self.predictions, alpha=0.6, edgecolor='k')\n",
    "        plt.xlabel('Actual Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.title('Correlation between Actual and Predicted Values')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        self.plot_loss_curve()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.hparams.lr,\n",
    "            steps_per_epoch=100,\n",
    "            epochs=10,\n",
    "            anneal_strategy='cos',\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': self.optimizer,\n",
    "            'lr_scheduler': scheduler,\n",
    "            'monitor': 'val_loss'\n",
    "        }\n",
    "\n",
    "    def plot_loss_curve(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        if self.val_losses:\n",
    "            plt.plot(self.val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure callback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,          \n",
    "    verbose=True,\n",
    "    mode=\"min\"           \n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",     \n",
    "    filename=\"best_model-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,             \n",
    "    mode=\"min\",                  \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine optimal amount of model layers and neurons per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-30 17:36:33,722] A new study created in memory with name: no-name-ccf43d6f-47e1-44fe-add9-e73978fa8441\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/.venv/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/lap_simulation/F1NeuralNetworkExperimentsLogger/2024-11-30_17-31-07/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | model   | Sequential | 83.3 K | train\n",
      "1 | loss_fn | L1Loss     | 0      | train\n",
      "-----------------------------------------------\n",
      "83.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "83.3 K    Total params\n",
      "0.333     Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ff3e0e9f0447c5aa52cdce08f06201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/i551965/Documents/dev/Uni/DataMining/ie500-data-mining-group7/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39b998460124f12bc2ba1f7e9a16ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062662b53ba545e08599d31774a4b08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 4 records. Best score: 95116.000. Signaling Trainer to stop.\n",
      "Epoch 0, global step 1932: 'val_loss' was not in top 1\n",
      "[I 2024-11-30 17:36:54,993] Trial 0 finished with value: 95161.70421829446 and parameters: {'num_layers': 3, 'neurons_per_layer': 197}. Best is trial 0 with value: 95161.70421829446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss for 3 layers, 197 neurons per layer: 95161.7042\n",
      "Best Hyperparameters:  {'num_layers': 3, 'neurons_per_layer': 197}\n",
      "Best Value (Test Loss):  95161.70421829446\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "    neurons_per_layer = trial.suggest_int('neurons_per_layer', 32, 256)\n",
    "\n",
    "    model = LaptimePredicionModel(layer_config=(num_layers, neurons_per_layer))\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=10, logger=logger, callbacks=[early_stopping, checkpoint_callback])\n",
    "\n",
    "    trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "    avg_val_loss = sum(model.val_losses) / len(model.val_losses)\n",
    "    print(f\"Test Loss for {num_layers} layers, {neurons_per_layer} neurons per layer: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def optimize_hyperparameters():\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    print(\"Best Hyperparameters: \", study.best_params)\n",
    "    print(\"Best Value (Test Loss): \", study.best_value)\n",
    "    \n",
    "optimize_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the optimal learning rate, loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Sample hyperparameters from the search space\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)  # Learning rate (log scale)\n",
    "    \n",
    "    # Choose optimizer: Adam, AdamW, or SGD\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'SGD'])\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD\n",
    "\n",
    "    # Choose loss function: L1Loss or MSELoss\n",
    "    loss_fn_name = trial.suggest_categorical('loss_fn', ['L1Loss', 'MSELoss'])\n",
    "    if loss_fn_name == 'L1Loss':\n",
    "        loss_fn = nn.L1Loss()\n",
    "    else:\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Initialize the model with the fixed architecture and selected loss function\n",
    "    model = LaptimePredicionModel(lr=lr, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    # Setup the trainer\n",
    "    trainer = pl.Trainer(train_dataloader, max_epochs=10)\n",
    "\n",
    "    # Fit the model\n",
    "    trainer.fit(model)\n",
    "\n",
    "    # Calculate average test loss\n",
    "    avg_val_loss = sum(model.val_losses) / len(model.val_losses)\n",
    "    print(f\"Test Loss for Optimizer: {optimizer_name}, Loss function: {loss_fn_name}, \"\n",
    "          f\"Learning rate: {lr:.6f}: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return avg_val_loss  # We aim to minimize the test loss\n",
    "\n",
    "# Function to run the optimization process\n",
    "def optimize_hyperparameters():\n",
    "    # Create an Optuna study to minimize the test loss\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=10)  # Run 10 trials\n",
    "\n",
    "    # Print the best hyperparameters found by Optuna\n",
    "    print(\"Best Hyperparameters: \", study.best_params)\n",
    "    print(\"Best Test Loss: \", study.best_value)\n",
    "\n",
    "# Start the optimization process\n",
    "optimize_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Training with best configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LaptimePredicionModel()\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(logger=logger)\n",
    "trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=model, dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
