{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T14:59:53.505931Z",
     "start_time": "2024-11-30T14:59:49.411141Z"
    }
   },
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import custom modules\n",
    "from data_preparation import load_and_preprocess_data, prepare_sequence_data, split_data_by_race, save_data_splits\n",
    "from features import RaceFeatures\n",
    "from lstm import F1PredictionModel, F1Dataset, F1DataPreprocessor, train_model, save_model_with_preprocessor\n",
    "from evaluation import evaluate_model, plot_predictions\n",
    "\n",
    "def main():\n",
    "   # Load and preprocess data\n",
    "   print(\"Loading and preprocessing data...\")\n",
    "   df = load_and_preprocess_data()\n",
    "\n",
    "   # Split data by race to prevent data leakage\n",
    "   print(\"Splitting data...\")\n",
    "   train_df, test_df = split_data_by_race(df, test_size=0.2, random_state=42)\n",
    "   save_data_splits(train_df, test_df)\n",
    "\n",
    "   # Initialize preprocessor and features\n",
    "   preprocessor = F1DataPreprocessor()\n",
    "   race_features = RaceFeatures()\n",
    "\n",
    "   # Prepare sequence data\n",
    "   print(\"Preparing sequence data...\")\n",
    "   sequences_train, static_train, targets_train = prepare_sequence_data(train_df, race_features, window_size=3)\n",
    "   sequences_test, static_test, targets_test = prepare_sequence_data(test_df, race_features, window_size=3)\n",
    "\n",
    "   # Fit scalers on training data and transform all datasets\n",
    "   print(\"Scaling data...\")\n",
    "   preprocessor.fit_scalers(sequences_train, static_train, targets_train)\n",
    "   \n",
    "   sequences_train_scaled, static_train_scaled, targets_train_scaled = preprocessor.transform_data(\n",
    "       sequences_train, static_train, targets_train)\n",
    "   sequences_test_scaled, static_test_scaled, targets_test_scaled = preprocessor.transform_data(\n",
    "       sequences_test, static_test, targets_test)\n",
    "\n",
    "   # Create datasets\n",
    "   train_dataset = F1Dataset(sequences_train_scaled, static_train_scaled, targets_train_scaled)\n",
    "   test_dataset = F1Dataset(sequences_test_scaled, static_test_scaled, targets_test_scaled)\n",
    "\n",
    "   # Initialize the model with default parameters\n",
    "   model = F1PredictionModel(\n",
    "       sequence_dim=sequences_train_scaled.shape[2],\n",
    "       static_dim=static_train_scaled.shape[1],\n",
    "       hidden_dim=64,\n",
    "       num_layers=10,\n",
    "       dropout_prob=0.5\n",
    "   )\n",
    "\n",
    "   # Create data loaders\n",
    "   train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "   test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "   # Train model\n",
    "   print(\"Training model...\")\n",
    "   history = train_model(\n",
    "       model,\n",
    "       train_loader,\n",
    "       test_loader,  # Using test_loader as validation for now\n",
    "       epochs=10,\n",
    "       learning_rate=0.001\n",
    "   )\n",
    "\n",
    "   # Evaluate on test set\n",
    "   print(\"Evaluating model...\")\n",
    "   model.eval()\n",
    "   predictions = []\n",
    "   true_values = []\n",
    "\n",
    "   with torch.no_grad():\n",
    "       for batch in test_loader:\n",
    "           sequences = batch['sequence']\n",
    "           static = batch['static']\n",
    "           targets = batch['target']\n",
    "\n",
    "           outputs = model(sequences, static)\n",
    "           predictions.extend(outputs.numpy())\n",
    "           true_values.extend(targets.numpy())\n",
    "\n",
    "   # Inverse transform predictions and true values\n",
    "   predictions = preprocessor.lap_time_scaler.inverse_transform(\n",
    "       np.array(predictions).reshape(-1, 1)).flatten()\n",
    "   true_values = preprocessor.lap_time_scaler.inverse_transform(\n",
    "       np.array(true_values).reshape(-1, 1)).flatten()\n",
    "\n",
    "   # Calculate and display evaluation metrics\n",
    "   metrics = evaluate_model(true_values, predictions)\n",
    "   print(\"Test set metrics:\", metrics)\n",
    "\n",
    "   # Plot results\n",
    "   plot_predictions(true_values, predictions, model_name='LSTM Model')\n",
    "\n",
    "   # Save the model\n",
    "   save_model_with_preprocessor(\n",
    "       model,\n",
    "       preprocessor,\n",
    "       sequences_train_scaled.shape[2],\n",
    "       static_train_scaled.shape[1],\n",
    "       'lstm_model.pth'\n",
    "   )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "(586171, 15)\n",
      "(586171, 32)\n",
      "(586171, 40)\n",
      "(586171, 45)\n",
      "(586171, 46)\n",
      "(586171, 47)\n",
      "(586171, 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I551659/Documents/GitHub/IE650-RAMP/ie500-data-mining-group7/lap_simulation/data_preparation.py:79: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['pitstop_milliseconds'].fillna(0, inplace=True)  # Assuming 0 if no pit stop\n",
      "/Users/I551659/Documents/GitHub/IE650-RAMP/ie500-data-mining-group7/lap_simulation/data_preparation.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['constructor_points'].fillna(laps['constructor_points'].mean(), inplace=True)\n",
      "/Users/I551659/Documents/GitHub/IE650-RAMP/ie500-data-mining-group7/lap_simulation/data_preparation.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  laps['constructor_position'].fillna(laps['constructor_position'].max(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586171, 56)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Compound'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Compound'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 108\u001B[0m\n\u001B[1;32m     99\u001B[0m    save_model_with_preprocessor(\n\u001B[1;32m    100\u001B[0m        model,\n\u001B[1;32m    101\u001B[0m        preprocessor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    104\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlstm_model.pth\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    105\u001B[0m    )\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 108\u001B[0m    \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 17\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmain\u001B[39m():\n\u001B[1;32m     15\u001B[0m    \u001B[38;5;66;03m# Load and preprocess data\u001B[39;00m\n\u001B[1;32m     16\u001B[0m    \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoading and preprocessing data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m    df \u001B[38;5;241m=\u001B[39m \u001B[43mload_and_preprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m    \u001B[38;5;66;03m# Split data by race to prevent data leakage\u001B[39;00m\n\u001B[1;32m     20\u001B[0m    \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSplitting data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/GitHub/IE650-RAMP/ie500-data-mining-group7/lap_simulation/data_preparation.py:549\u001B[0m, in \u001B[0;36mload_and_preprocess_data\u001B[0;34m()\u001B[0m\n\u001B[1;32m    546\u001B[0m race_features \u001B[38;5;241m=\u001B[39m RaceFeatures()\n\u001B[1;32m    548\u001B[0m \u001B[38;5;66;03m# Preprocess data\u001B[39;00m\n\u001B[0;32m--> 549\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[38;5;66;03m# Validate that all required columns are present\u001B[39;00m\n\u001B[1;32m    552\u001B[0m required_columns \u001B[38;5;241m=\u001B[39m race_features\u001B[38;5;241m.\u001B[39mstatic_features \u001B[38;5;241m+\u001B[39m race_features\u001B[38;5;241m.\u001B[39mdynamic_features\n",
      "File \u001B[0;32m~/Documents/GitHub/IE650-RAMP/ie500-data-mining-group7/lap_simulation/data_preparation.py:148\u001B[0m, in \u001B[0;36mpreprocess_data\u001B[0;34m()\u001B[0m\n\u001B[1;32m    143\u001B[0m weather_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mseconds_from_start\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m weather_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    147\u001B[0m \u001B[38;5;66;03m# Standardize text data\u001B[39;00m\n\u001B[0;32m--> 148\u001B[0m tire_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCompound\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mtire_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mCompound\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mupper()\n\u001B[1;32m    149\u001B[0m tire_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEventName\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m tire_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEventName\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mstrip()\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mupper()\n\u001B[1;32m    150\u001B[0m races[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m races[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mstrip()\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mupper()\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/Library/Caches/pypoetry/virtualenvs/ie500-data-mining-group7-LKR-OXJO-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'Compound'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T14:56:14.659592Z",
     "start_time": "2024-11-30T14:56:14.656439Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ie500-data-mining-group7-LKR-OXJO-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
