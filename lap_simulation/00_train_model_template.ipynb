{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlstm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F1PredictionModel, F1Dataset, F1DataPreprocessor, train_model\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_model, plot_predictions\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_optimization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize_hyperparameters\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading and preprocessing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/IE650-RAMP/ie500-data-mining-group7/lap_simulation/model_optimization.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlstm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F1PredictionModel, train_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import custom modules\n",
    "from data_preparation import load_and_preprocess_data, prepare_sequence_data, split_data_by_race\n",
    "from features import RaceFeatures\n",
    "from lstm import F1PredictionModel, F1Dataset, F1DataPreprocessor, train_model\n",
    "from evaluation import evaluate_model, plot_predictions\n",
    "from model_optimization import *\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = load_and_preprocess_data()\n",
    "\n",
    "    # Split data by race to prevent data leakage\n",
    "    print(\"Splitting data...\")\n",
    "    train_val_df, test_df = split_data_by_race(df, test_size=0.2, random_state=42)\n",
    "    train_df, val_df = split_data_by_race(train_val_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize preprocessor and features\n",
    "    preprocessor = F1DataPreprocessor()\n",
    "    race_features = RaceFeatures()\n",
    "\n",
    "    # Prepare sequence data\n",
    "    print(\"Preparing sequence data...\")\n",
    "    sequences_train, static_train, targets_train = prepare_sequence_data(train_df, race_features, window_size=3)\n",
    "    sequences_val, static_val, targets_val = prepare_sequence_data(val_df, race_features, window_size=3)\n",
    "    sequences_test, static_test, targets_test = prepare_sequence_data(test_df, race_features, window_size=3)\n",
    "\n",
    "    # Fit scalers on training data and transform all datasets\n",
    "    print(\"Scaling data...\")\n",
    "    preprocessor.fit_scalers(sequences_train, static_train, targets_train)\n",
    "    \n",
    "    sequences_train_scaled, static_train_scaled, targets_train_scaled = preprocessor.transform_data(\n",
    "        sequences_train, static_train, targets_train)\n",
    "    sequences_val_scaled, static_val_scaled, targets_val_scaled = preprocessor.transform_data(\n",
    "        sequences_val, static_val, targets_val)\n",
    "    sequences_test_scaled, static_test_scaled, targets_test_scaled = preprocessor.transform_data(\n",
    "        sequences_test, static_test, targets_test)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = F1Dataset(sequences_train_scaled, static_train_scaled, targets_train_scaled)\n",
    "    val_dataset = F1Dataset(sequences_val_scaled, static_val_scaled, targets_val_scaled)\n",
    "    test_dataset = F1Dataset(sequences_test_scaled, static_test_scaled, targets_test_scaled)\n",
    "\n",
    "    # Get dimensions for model\n",
    "    sequence_dim = sequences_train_scaled.shape[2]\n",
    "    static_dim = static_train_scaled.shape[1]\n",
    "\n",
    "    # Optimize hyperparameters\n",
    "    print(\"Optimizing hyperparameters...\")\n",
    "    best_params = optimize_hyperparameters(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        sequence_dim=sequence_dim,\n",
    "        static_dim=static_dim,\n",
    "        n_trials=50\n",
    "    )\n",
    "    print(\"Best parameters:\", best_params)\n",
    "\n",
    "    # Create final model with optimized parameters\n",
    "    final_model = F1PredictionModel(\n",
    "        sequence_dim=sequence_dim,\n",
    "        static_dim=static_dim,\n",
    "        hidden_dim=best_params['hidden_dim'],\n",
    "        num_layers=best_params['num_layers'],\n",
    "        dropout_prob=best_params['dropout_prob']\n",
    "    )\n",
    "\n",
    "    # Create data loaders for final training\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Train final model\n",
    "    print(\"Training final model...\")\n",
    "    history = train_model(\n",
    "        final_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        epochs=20,\n",
    "        learning_rate=best_params['learning_rate']\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating model...\")\n",
    "    final_model.eval()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            sequences = batch['sequence']\n",
    "            static = batch['static']\n",
    "            targets = batch['target']\n",
    "\n",
    "            outputs = final_model(sequences, static)\n",
    "            predictions.extend(outputs.numpy())\n",
    "            true_values.extend(targets.numpy())\n",
    "\n",
    "    # Inverse transform predictions and true values\n",
    "    predictions = preprocessor.lap_time_scaler.inverse_transform(\n",
    "        np.array(predictions).reshape(-1, 1)).flatten()\n",
    "    true_values = preprocessor.lap_time_scaler.inverse_transform(\n",
    "        np.array(true_values).reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Calculate and display evaluation metrics\n",
    "    metrics = evaluate_model(true_values, predictions)\n",
    "    print(\"Test set metrics:\", metrics)\n",
    "\n",
    "    # Plot results\n",
    "    plot_predictions(true_values, predictions, model_name='LSTM Model')\n",
    "\n",
    "    # Save the model\n",
    "    save_model_with_preprocessor(\n",
    "        final_model,\n",
    "        preprocessor,\n",
    "        sequence_dim,\n",
    "        static_dim,\n",
    "        'lstm_model_optimized.pth'\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ie500-data-mining-group7-LKR-OXJO-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
